{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cad76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 8, 3, 1000]             512\n",
      "       BatchNorm2d-2           [-1, 8, 3, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]              48\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "          Dropout-15               [-1, 15, 16]               0\n",
      "PositioinalEncoding-16               [-1, 15, 16]               0\n",
      "           Linear-17               [-1, 15, 16]             272\n",
      "           Linear-18               [-1, 15, 16]             272\n",
      "           Linear-19               [-1, 15, 16]             272\n",
      "          Dropout-20            [-1, 2, 15, 15]               0\n",
      "           Linear-21               [-1, 15, 16]             272\n",
      "MultiHeadAttention-22               [-1, 15, 16]               0\n",
      "          Dropout-23               [-1, 15, 16]               0\n",
      "        LayerNorm-24               [-1, 15, 16]              32\n",
      "      ResidualAdd-25               [-1, 15, 16]               0\n",
      "           Linear-26                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-27                [-1, 15, 4]               0\n",
      "           Linear-28               [-1, 15, 64]           1,088\n",
      "             GELU-29               [-1, 15, 64]               0\n",
      "           Linear-30               [-1, 15, 32]           2,080\n",
      "           Linear-31               [-1, 15, 64]           1,088\n",
      "             GELU-32               [-1, 15, 64]               0\n",
      "           Linear-33               [-1, 15, 32]           2,080\n",
      "           Linear-34               [-1, 15, 64]           1,088\n",
      "             GELU-35               [-1, 15, 64]               0\n",
      "           Linear-36               [-1, 15, 32]           2,080\n",
      "           Linear-37               [-1, 15, 64]           1,088\n",
      "             GELU-38               [-1, 15, 64]               0\n",
      "           Linear-39               [-1, 15, 32]           2,080\n",
      "           Linear-40               [-1, 15, 16]             528\n",
      " MoE_Layerwrapper-41               [-1, 15, 16]               0\n",
      "           Linear-42               [-1, 15, 64]           1,088\n",
      "             GELU-43               [-1, 15, 64]               0\n",
      "          Dropout-44               [-1, 15, 64]               0\n",
      "           Linear-45               [-1, 15, 16]           1,040\n",
      "          Dropout-46               [-1, 15, 16]               0\n",
      "        LayerNorm-47               [-1, 15, 16]              32\n",
      "      ResidualAdd-48               [-1, 15, 16]               0\n",
      "           Linear-49               [-1, 15, 16]             272\n",
      "           Linear-50               [-1, 15, 16]             272\n",
      "           Linear-51               [-1, 15, 16]             272\n",
      "          Dropout-52            [-1, 2, 15, 15]               0\n",
      "           Linear-53               [-1, 15, 16]             272\n",
      "MultiHeadAttention-54               [-1, 15, 16]               0\n",
      "          Dropout-55               [-1, 15, 16]               0\n",
      "        LayerNorm-56               [-1, 15, 16]              32\n",
      "      ResidualAdd-57               [-1, 15, 16]               0\n",
      "           Linear-58                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-59                [-1, 15, 4]               0\n",
      "           Linear-60               [-1, 15, 64]           1,088\n",
      "             GELU-61               [-1, 15, 64]               0\n",
      "           Linear-62               [-1, 15, 32]           2,080\n",
      "           Linear-63               [-1, 15, 64]           1,088\n",
      "             GELU-64               [-1, 15, 64]               0\n",
      "           Linear-65               [-1, 15, 32]           2,080\n",
      "           Linear-66               [-1, 15, 64]           1,088\n",
      "             GELU-67               [-1, 15, 64]               0\n",
      "           Linear-68               [-1, 15, 32]           2,080\n",
      "           Linear-69               [-1, 15, 64]           1,088\n",
      "             GELU-70               [-1, 15, 64]               0\n",
      "           Linear-71               [-1, 15, 32]           2,080\n",
      "           Linear-72               [-1, 15, 16]             528\n",
      " MoE_Layerwrapper-73               [-1, 15, 16]               0\n",
      "           Linear-74               [-1, 15, 64]           1,088\n",
      "             GELU-75               [-1, 15, 64]               0\n",
      "          Dropout-76               [-1, 15, 64]               0\n",
      "           Linear-77               [-1, 15, 16]           1,040\n",
      "          Dropout-78               [-1, 15, 16]               0\n",
      "        LayerNorm-79               [-1, 15, 16]              32\n",
      "      ResidualAdd-80               [-1, 15, 16]               0\n",
      "           Linear-81               [-1, 15, 16]             272\n",
      "           Linear-82               [-1, 15, 16]             272\n",
      "           Linear-83               [-1, 15, 16]             272\n",
      "          Dropout-84            [-1, 2, 15, 15]               0\n",
      "           Linear-85               [-1, 15, 16]             272\n",
      "MultiHeadAttention-86               [-1, 15, 16]               0\n",
      "          Dropout-87               [-1, 15, 16]               0\n",
      "        LayerNorm-88               [-1, 15, 16]              32\n",
      "      ResidualAdd-89               [-1, 15, 16]               0\n",
      "           Linear-90                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-91                [-1, 15, 4]               0\n",
      "           Linear-92               [-1, 15, 64]           1,088\n",
      "             GELU-93               [-1, 15, 64]               0\n",
      "           Linear-94               [-1, 15, 32]           2,080\n",
      "           Linear-95               [-1, 15, 64]           1,088\n",
      "             GELU-96               [-1, 15, 64]               0\n",
      "           Linear-97               [-1, 15, 32]           2,080\n",
      "           Linear-98               [-1, 15, 64]           1,088\n",
      "             GELU-99               [-1, 15, 64]               0\n",
      "          Linear-100               [-1, 15, 32]           2,080\n",
      "          Linear-101               [-1, 15, 64]           1,088\n",
      "            GELU-102               [-1, 15, 64]               0\n",
      "          Linear-103               [-1, 15, 32]           2,080\n",
      "          Linear-104               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-105               [-1, 15, 16]               0\n",
      "          Linear-106               [-1, 15, 64]           1,088\n",
      "            GELU-107               [-1, 15, 64]               0\n",
      "         Dropout-108               [-1, 15, 64]               0\n",
      "          Linear-109               [-1, 15, 16]           1,040\n",
      "         Dropout-110               [-1, 15, 16]               0\n",
      "       LayerNorm-111               [-1, 15, 16]              32\n",
      "     ResidualAdd-112               [-1, 15, 16]               0\n",
      "          Linear-113               [-1, 15, 16]             272\n",
      "          Linear-114               [-1, 15, 16]             272\n",
      "          Linear-115               [-1, 15, 16]             272\n",
      "         Dropout-116            [-1, 2, 15, 15]               0\n",
      "          Linear-117               [-1, 15, 16]             272\n",
      "MultiHeadAttention-118               [-1, 15, 16]               0\n",
      "         Dropout-119               [-1, 15, 16]               0\n",
      "       LayerNorm-120               [-1, 15, 16]              32\n",
      "     ResidualAdd-121               [-1, 15, 16]               0\n",
      "          Linear-122                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-123                [-1, 15, 4]               0\n",
      "          Linear-124               [-1, 15, 64]           1,088\n",
      "            GELU-125               [-1, 15, 64]               0\n",
      "          Linear-126               [-1, 15, 32]           2,080\n",
      "          Linear-127               [-1, 15, 64]           1,088\n",
      "            GELU-128               [-1, 15, 64]               0\n",
      "          Linear-129               [-1, 15, 32]           2,080\n",
      "          Linear-130               [-1, 15, 64]           1,088\n",
      "            GELU-131               [-1, 15, 64]               0\n",
      "          Linear-132               [-1, 15, 32]           2,080\n",
      "          Linear-133               [-1, 15, 64]           1,088\n",
      "            GELU-134               [-1, 15, 64]               0\n",
      "          Linear-135               [-1, 15, 32]           2,080\n",
      "          Linear-136               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-137               [-1, 15, 16]               0\n",
      "          Linear-138               [-1, 15, 64]           1,088\n",
      "            GELU-139               [-1, 15, 64]               0\n",
      "         Dropout-140               [-1, 15, 64]               0\n",
      "          Linear-141               [-1, 15, 16]           1,040\n",
      "         Dropout-142               [-1, 15, 16]               0\n",
      "       LayerNorm-143               [-1, 15, 16]              32\n",
      "     ResidualAdd-144               [-1, 15, 16]               0\n",
      "          Linear-145               [-1, 15, 16]             272\n",
      "          Linear-146               [-1, 15, 16]             272\n",
      "          Linear-147               [-1, 15, 16]             272\n",
      "         Dropout-148            [-1, 2, 15, 15]               0\n",
      "          Linear-149               [-1, 15, 16]             272\n",
      "MultiHeadAttention-150               [-1, 15, 16]               0\n",
      "         Dropout-151               [-1, 15, 16]               0\n",
      "       LayerNorm-152               [-1, 15, 16]              32\n",
      "     ResidualAdd-153               [-1, 15, 16]               0\n",
      "          Linear-154                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-155                [-1, 15, 4]               0\n",
      "          Linear-156               [-1, 15, 64]           1,088\n",
      "            GELU-157               [-1, 15, 64]               0\n",
      "          Linear-158               [-1, 15, 32]           2,080\n",
      "          Linear-159               [-1, 15, 64]           1,088\n",
      "            GELU-160               [-1, 15, 64]               0\n",
      "          Linear-161               [-1, 15, 32]           2,080\n",
      "          Linear-162               [-1, 15, 64]           1,088\n",
      "            GELU-163               [-1, 15, 64]               0\n",
      "          Linear-164               [-1, 15, 32]           2,080\n",
      "          Linear-165               [-1, 15, 64]           1,088\n",
      "            GELU-166               [-1, 15, 64]               0\n",
      "          Linear-167               [-1, 15, 32]           2,080\n",
      "          Linear-168               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-169               [-1, 15, 16]               0\n",
      "          Linear-170               [-1, 15, 64]           1,088\n",
      "            GELU-171               [-1, 15, 64]               0\n",
      "         Dropout-172               [-1, 15, 64]               0\n",
      "          Linear-173               [-1, 15, 16]           1,040\n",
      "         Dropout-174               [-1, 15, 16]               0\n",
      "       LayerNorm-175               [-1, 15, 16]              32\n",
      "     ResidualAdd-176               [-1, 15, 16]               0\n",
      "          Linear-177               [-1, 15, 16]             272\n",
      "          Linear-178               [-1, 15, 16]             272\n",
      "          Linear-179               [-1, 15, 16]             272\n",
      "         Dropout-180            [-1, 2, 15, 15]               0\n",
      "          Linear-181               [-1, 15, 16]             272\n",
      "MultiHeadAttention-182               [-1, 15, 16]               0\n",
      "         Dropout-183               [-1, 15, 16]               0\n",
      "       LayerNorm-184               [-1, 15, 16]              32\n",
      "     ResidualAdd-185               [-1, 15, 16]               0\n",
      "          Linear-186                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-187                [-1, 15, 4]               0\n",
      "          Linear-188               [-1, 15, 64]           1,088\n",
      "            GELU-189               [-1, 15, 64]               0\n",
      "          Linear-190               [-1, 15, 32]           2,080\n",
      "          Linear-191               [-1, 15, 64]           1,088\n",
      "            GELU-192               [-1, 15, 64]               0\n",
      "          Linear-193               [-1, 15, 32]           2,080\n",
      "          Linear-194               [-1, 15, 64]           1,088\n",
      "            GELU-195               [-1, 15, 64]               0\n",
      "          Linear-196               [-1, 15, 32]           2,080\n",
      "          Linear-197               [-1, 15, 64]           1,088\n",
      "            GELU-198               [-1, 15, 64]               0\n",
      "          Linear-199               [-1, 15, 32]           2,080\n",
      "          Linear-200               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-201               [-1, 15, 16]               0\n",
      "          Linear-202               [-1, 15, 64]           1,088\n",
      "            GELU-203               [-1, 15, 64]               0\n",
      "         Dropout-204               [-1, 15, 64]               0\n",
      "          Linear-205               [-1, 15, 16]           1,040\n",
      "         Dropout-206               [-1, 15, 16]               0\n",
      "       LayerNorm-207               [-1, 15, 16]              32\n",
      "     ResidualAdd-208               [-1, 15, 16]               0\n",
      "         Flatten-209                  [-1, 240]               0\n",
      "         Dropout-210                  [-1, 240]               0\n",
      "          Linear-211                    [-1, 2]             482\n",
      "================================================================\n",
      "Total params: 104,506\n",
      "Trainable params: 104,506\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.57\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 1.98\n",
      "----------------------------------------------------------------\n",
      "Mon Feb 24 11:20:08 2025\n",
      "seed is 40\n",
      "Subject 1\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "1_0 train_acc: 0.5123 train_loss: 1.222221\tval_acc: 0.480565 val_loss: 0.7370583\n",
      "1_1 train_acc: 0.6107 train_loss: 0.856683\tval_acc: 0.597173 val_loss: 0.6623832\n",
      "1_2 train_acc: 0.5984 train_loss: 0.926452\tval_acc: 0.685512 val_loss: 0.6228372\n",
      "1_3 train_acc: 0.5861 train_loss: 0.826402\tval_acc: 0.724382 val_loss: 0.5700745\n",
      "1_4 train_acc: 0.5861 train_loss: 0.806338\tval_acc: 0.756184 val_loss: 0.5467055\n",
      "1_5 train_acc: 0.6557 train_loss: 0.652494\tval_acc: 0.749117 val_loss: 0.5294985\n",
      "1_7 train_acc: 0.6189 train_loss: 0.672334\tval_acc: 0.766784 val_loss: 0.5160091\n",
      "1_8 train_acc: 0.6393 train_loss: 0.656479\tval_acc: 0.784452 val_loss: 0.5045260\n",
      "1_10 train_acc: 0.7008 train_loss: 0.570794\tval_acc: 0.802120 val_loss: 0.4751745\n",
      "1_12 train_acc: 0.6557 train_loss: 0.610999\tval_acc: 0.816254 val_loss: 0.4589795\n",
      "1_13 train_acc: 0.6885 train_loss: 0.636078\tval_acc: 0.802120 val_loss: 0.4550273\n",
      "1_14 train_acc: 0.7295 train_loss: 0.576497\tval_acc: 0.851590 val_loss: 0.4332083\n",
      "1_21 train_acc: 0.7869 train_loss: 0.513263\tval_acc: 0.840989 val_loss: 0.4179651\n",
      "1_22 train_acc: 0.7541 train_loss: 0.495104\tval_acc: 0.858657 val_loss: 0.3891136\n",
      "1_24 train_acc: 0.7295 train_loss: 0.532617\tval_acc: 0.855124 val_loss: 0.3803792\n",
      "1_25 train_acc: 0.7090 train_loss: 0.540601\tval_acc: 0.848057 val_loss: 0.3751792\n",
      "1_26 train_acc: 0.7869 train_loss: 0.484519\tval_acc: 0.851590 val_loss: 0.3709354\n",
      "1_27 train_acc: 0.7377 train_loss: 0.565132\tval_acc: 0.840989 val_loss: 0.3659119\n",
      "1_28 train_acc: 0.7992 train_loss: 0.476103\tval_acc: 0.826855 val_loss: 0.3622361\n",
      "1_29 train_acc: 0.8484 train_loss: 0.406773\tval_acc: 0.865724 val_loss: 0.3297226\n",
      "1_33 train_acc: 0.8279 train_loss: 0.401042\tval_acc: 0.858657 val_loss: 0.3270113\n",
      "1_34 train_acc: 0.7582 train_loss: 0.478778\tval_acc: 0.865724 val_loss: 0.3015362\n",
      "1_37 train_acc: 0.7623 train_loss: 0.449490\tval_acc: 0.879859 val_loss: 0.2909017\n",
      "1_40 train_acc: 0.8279 train_loss: 0.380156\tval_acc: 0.886926 val_loss: 0.2843542\n",
      "1_42 train_acc: 0.8320 train_loss: 0.409414\tval_acc: 0.879859 val_loss: 0.2837629\n",
      "1_44 train_acc: 0.8197 train_loss: 0.367773\tval_acc: 0.883392 val_loss: 0.2680725\n",
      "1_47 train_acc: 0.8484 train_loss: 0.361939\tval_acc: 0.890459 val_loss: 0.2616273\n",
      "1_49 train_acc: 0.8361 train_loss: 0.375622\tval_acc: 0.922261 val_loss: 0.2230670\n",
      "1_60 train_acc: 0.8648 train_loss: 0.364576\tval_acc: 0.901060 val_loss: 0.2184826\n",
      "1_67 train_acc: 0.8648 train_loss: 0.307600\tval_acc: 0.918728 val_loss: 0.2148883\n",
      "1_68 train_acc: 0.8770 train_loss: 0.289591\tval_acc: 0.925795 val_loss: 0.1843156\n",
      "1_84 train_acc: 0.8033 train_loss: 0.366213\tval_acc: 0.939929 val_loss: 0.1808932\n",
      "1_88 train_acc: 0.8730 train_loss: 0.304760\tval_acc: 0.936396 val_loss: 0.1710999\n",
      "1_93 train_acc: 0.8975 train_loss: 0.255137\tval_acc: 0.943463 val_loss: 0.1679905\n",
      "1_99 train_acc: 0.8730 train_loss: 0.284043\tval_acc: 0.946996 val_loss: 0.1673578\n",
      "1_104 train_acc: 0.8893 train_loss: 0.264409\tval_acc: 0.943463 val_loss: 0.1571618\n",
      "1_109 train_acc: 0.8484 train_loss: 0.351356\tval_acc: 0.939929 val_loss: 0.1491260\n",
      "1_114 train_acc: 0.8852 train_loss: 0.291471\tval_acc: 0.943463 val_loss: 0.1471535\n",
      "1_117 train_acc: 0.8770 train_loss: 0.261067\tval_acc: 0.939929 val_loss: 0.1463545\n",
      "1_124 train_acc: 0.9139 train_loss: 0.210026\tval_acc: 0.936396 val_loss: 0.1454177\n",
      "1_126 train_acc: 0.8934 train_loss: 0.277636\tval_acc: 0.968198 val_loss: 0.1341678\n",
      "1_133 train_acc: 0.8770 train_loss: 0.258339\tval_acc: 0.975265 val_loss: 0.1180292\n",
      "1_142 train_acc: 0.8770 train_loss: 0.262650\tval_acc: 0.961131 val_loss: 0.1131183\n",
      "1_147 train_acc: 0.8893 train_loss: 0.241531\tval_acc: 0.985866 val_loss: 0.1023722\n",
      "1_184 train_acc: 0.9057 train_loss: 0.228669\tval_acc: 0.975265 val_loss: 0.0908982\n",
      "1_194 train_acc: 0.9221 train_loss: 0.229555\tval_acc: 0.964664 val_loss: 0.0849908\n",
      "1_229 train_acc: 0.8975 train_loss: 0.229797\tval_acc: 0.985866 val_loss: 0.0767607\n",
      "1_259 train_acc: 0.9057 train_loss: 0.193534\tval_acc: 0.978799 val_loss: 0.0764556\n",
      "1_262 train_acc: 0.9098 train_loss: 0.175017\tval_acc: 0.978799 val_loss: 0.0716768\n",
      "1_271 train_acc: 0.9262 train_loss: 0.193193\tval_acc: 0.992933 val_loss: 0.0632401\n",
      "1_276 train_acc: 0.9180 train_loss: 0.182266\tval_acc: 0.992933 val_loss: 0.0631763\n",
      "1_303 train_acc: 0.8975 train_loss: 0.239071\tval_acc: 0.985866 val_loss: 0.0581341\n",
      "1_323 train_acc: 0.9303 train_loss: 0.227010\tval_acc: 0.996466 val_loss: 0.0571350\n",
      "1_345 train_acc: 0.9426 train_loss: 0.153182\tval_acc: 0.989399 val_loss: 0.0561765\n",
      "1_348 train_acc: 0.9303 train_loss: 0.171930\tval_acc: 0.996466 val_loss: 0.0436058\n",
      "1_385 train_acc: 0.9344 train_loss: 0.171248\tval_acc: 0.989399 val_loss: 0.0426534\n",
      "1_386 train_acc: 0.9508 train_loss: 0.147579\tval_acc: 0.996466 val_loss: 0.0425743\n",
      "1_413 train_acc: 0.9180 train_loss: 0.160622\tval_acc: 0.996466 val_loss: 0.0415205\n",
      "1_429 train_acc: 0.8975 train_loss: 0.218763\tval_acc: 0.992933 val_loss: 0.0398951\n",
      "1_454 train_acc: 0.9057 train_loss: 0.216003\tval_acc: 0.996466 val_loss: 0.0396253\n",
      "1_464 train_acc: 0.9057 train_loss: 0.259188\tval_acc: 0.992933 val_loss: 0.0361275\n",
      "1_487 train_acc: 0.9590 train_loss: 0.118201\tval_acc: 0.996466 val_loss: 0.0314993\n",
      "1_492 train_acc: 0.9098 train_loss: 0.168970\tval_acc: 0.992933 val_loss: 0.0313651\n",
      "1_546 train_acc: 0.9221 train_loss: 0.163923\tval_acc: 0.996466 val_loss: 0.0311385\n",
      "1_568 train_acc: 0.9385 train_loss: 0.166518\tval_acc: 1.000000 val_loss: 0.0280018\n",
      "1_584 train_acc: 0.9303 train_loss: 0.157990\tval_acc: 1.000000 val_loss: 0.0275661\n",
      "1_596 train_acc: 0.9262 train_loss: 0.163928\tval_acc: 1.000000 val_loss: 0.0248989\n",
      "1_618 train_acc: 0.9303 train_loss: 0.183471\tval_acc: 0.996466 val_loss: 0.0226833\n",
      "1_651 train_acc: 0.9303 train_loss: 0.168260\tval_acc: 1.000000 val_loss: 0.0222816\n",
      "1_753 train_acc: 0.9467 train_loss: 0.153467\tval_acc: 1.000000 val_loss: 0.0221200\n",
      "1_761 train_acc: 0.9590 train_loss: 0.108644\tval_acc: 1.000000 val_loss: 0.0206189\n",
      "1_768 train_acc: 0.9303 train_loss: 0.154601\tval_acc: 1.000000 val_loss: 0.0193034\n",
      "1_809 train_acc: 0.9508 train_loss: 0.137356\tval_acc: 1.000000 val_loss: 0.0179866\n",
      "1_962 train_acc: 0.9426 train_loss: 0.135888\tval_acc: 1.000000 val_loss: 0.0179644\n",
      "1_964 train_acc: 0.9303 train_loss: 0.194042\tval_acc: 1.000000 val_loss: 0.0164663\n",
      "epoch:  964 \tThe test accuracy is: 0.853125\n",
      " THE BEST ACCURACY IS 0.853125\tkappa is 0.70625\n",
      "subject 1 duration: 0:09:01.530050\n",
      "seed is 853\n",
      "Subject 2\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (280, 3, 1000)\n",
      "2_0 train_acc: 0.5738 train_loss: 1.145286\tval_acc: 0.540636 val_loss: 0.7103137\n",
      "2_1 train_acc: 0.5574 train_loss: 1.159505\tval_acc: 0.597173 val_loss: 0.6655880\n",
      "2_2 train_acc: 0.5574 train_loss: 0.962073\tval_acc: 0.657244 val_loss: 0.6297681\n",
      "2_3 train_acc: 0.5820 train_loss: 0.840481\tval_acc: 0.699647 val_loss: 0.5897176\n",
      "2_4 train_acc: 0.6393 train_loss: 0.729840\tval_acc: 0.703180 val_loss: 0.5563147\n",
      "2_5 train_acc: 0.6434 train_loss: 0.693888\tval_acc: 0.713781 val_loss: 0.5516944\n",
      "2_6 train_acc: 0.6598 train_loss: 0.691799\tval_acc: 0.724382 val_loss: 0.5348639\n",
      "2_7 train_acc: 0.6475 train_loss: 0.664066\tval_acc: 0.766784 val_loss: 0.5040457\n",
      "2_8 train_acc: 0.6434 train_loss: 0.680899\tval_acc: 0.773852 val_loss: 0.4908088\n",
      "2_11 train_acc: 0.7049 train_loss: 0.586398\tval_acc: 0.780919 val_loss: 0.4891369\n",
      "2_13 train_acc: 0.7049 train_loss: 0.561775\tval_acc: 0.787986 val_loss: 0.4843122\n",
      "2_15 train_acc: 0.7336 train_loss: 0.519092\tval_acc: 0.787986 val_loss: 0.4783673\n",
      "2_16 train_acc: 0.7254 train_loss: 0.530756\tval_acc: 0.819788 val_loss: 0.4452741\n",
      "2_30 train_acc: 0.7172 train_loss: 0.507297\tval_acc: 0.823322 val_loss: 0.4436974\n",
      "2_32 train_acc: 0.7746 train_loss: 0.484514\tval_acc: 0.826855 val_loss: 0.4211625\n",
      "2_41 train_acc: 0.7459 train_loss: 0.522800\tval_acc: 0.840989 val_loss: 0.4043599\n",
      "2_51 train_acc: 0.7910 train_loss: 0.470149\tval_acc: 0.848057 val_loss: 0.3971317\n",
      "2_54 train_acc: 0.8074 train_loss: 0.453869\tval_acc: 0.844523 val_loss: 0.3852670\n",
      "2_58 train_acc: 0.7746 train_loss: 0.478202\tval_acc: 0.830389 val_loss: 0.3714787\n",
      "2_60 train_acc: 0.7623 train_loss: 0.449564\tval_acc: 0.848057 val_loss: 0.3611392\n",
      "2_62 train_acc: 0.7541 train_loss: 0.514783\tval_acc: 0.851590 val_loss: 0.3574737\n",
      "2_82 train_acc: 0.8279 train_loss: 0.390179\tval_acc: 0.816254 val_loss: 0.3516456\n",
      "2_89 train_acc: 0.7787 train_loss: 0.435421\tval_acc: 0.840989 val_loss: 0.3479720\n",
      "2_98 train_acc: 0.8074 train_loss: 0.411074\tval_acc: 0.833922 val_loss: 0.3468535\n",
      "2_108 train_acc: 0.8033 train_loss: 0.449695\tval_acc: 0.840989 val_loss: 0.3468121\n",
      "2_109 train_acc: 0.8074 train_loss: 0.428069\tval_acc: 0.858657 val_loss: 0.3315763\n",
      "2_120 train_acc: 0.8197 train_loss: 0.421795\tval_acc: 0.855124 val_loss: 0.3251642\n",
      "2_126 train_acc: 0.8197 train_loss: 0.410083\tval_acc: 0.890459 val_loss: 0.3025135\n",
      "2_150 train_acc: 0.7910 train_loss: 0.478534\tval_acc: 0.879859 val_loss: 0.3021603\n",
      "2_162 train_acc: 0.7705 train_loss: 0.451094\tval_acc: 0.883392 val_loss: 0.2958680\n",
      "2_165 train_acc: 0.8320 train_loss: 0.419832\tval_acc: 0.876325 val_loss: 0.2912452\n",
      "2_167 train_acc: 0.8361 train_loss: 0.406950\tval_acc: 0.897527 val_loss: 0.2897841\n",
      "2_168 train_acc: 0.8156 train_loss: 0.420322\tval_acc: 0.890459 val_loss: 0.2853902\n",
      "2_172 train_acc: 0.8607 train_loss: 0.346847\tval_acc: 0.883392 val_loss: 0.2745631\n",
      "2_192 train_acc: 0.8238 train_loss: 0.380306\tval_acc: 0.886926 val_loss: 0.2643490\n",
      "2_199 train_acc: 0.8238 train_loss: 0.374196\tval_acc: 0.908127 val_loss: 0.2633485\n",
      "2_214 train_acc: 0.8074 train_loss: 0.391532\tval_acc: 0.901060 val_loss: 0.2548819\n",
      "2_226 train_acc: 0.8525 train_loss: 0.354687\tval_acc: 0.890459 val_loss: 0.2502935\n",
      "2_232 train_acc: 0.8238 train_loss: 0.431134\tval_acc: 0.893993 val_loss: 0.2494283\n",
      "2_235 train_acc: 0.7869 train_loss: 0.385432\tval_acc: 0.904594 val_loss: 0.2427553\n",
      "2_247 train_acc: 0.8852 train_loss: 0.303605\tval_acc: 0.901060 val_loss: 0.2402883\n",
      "2_253 train_acc: 0.8730 train_loss: 0.312903\tval_acc: 0.908127 val_loss: 0.2377335\n",
      "2_262 train_acc: 0.8730 train_loss: 0.309995\tval_acc: 0.893993 val_loss: 0.2294897\n",
      "2_267 train_acc: 0.8689 train_loss: 0.310192\tval_acc: 0.915194 val_loss: 0.2152044\n",
      "2_272 train_acc: 0.8566 train_loss: 0.371521\tval_acc: 0.922261 val_loss: 0.2148419\n",
      "2_306 train_acc: 0.8238 train_loss: 0.395545\tval_acc: 0.918728 val_loss: 0.2097054\n",
      "2_323 train_acc: 0.8648 train_loss: 0.365324\tval_acc: 0.925795 val_loss: 0.2056085\n",
      "2_329 train_acc: 0.8320 train_loss: 0.374387\tval_acc: 0.936396 val_loss: 0.1910260\n",
      "2_338 train_acc: 0.8934 train_loss: 0.320422\tval_acc: 0.936396 val_loss: 0.1874088\n",
      "2_381 train_acc: 0.8730 train_loss: 0.340171\tval_acc: 0.932862 val_loss: 0.1691751\n",
      "2_389 train_acc: 0.8402 train_loss: 0.398941\tval_acc: 0.946996 val_loss: 0.1615345\n",
      "2_393 train_acc: 0.8607 train_loss: 0.351306\tval_acc: 0.957597 val_loss: 0.1533276\n",
      "2_429 train_acc: 0.8607 train_loss: 0.292855\tval_acc: 0.957597 val_loss: 0.1447676\n",
      "2_442 train_acc: 0.8934 train_loss: 0.268538\tval_acc: 0.954064 val_loss: 0.1252200\n",
      "2_512 train_acc: 0.8320 train_loss: 0.382215\tval_acc: 0.957597 val_loss: 0.1145542\n",
      "2_522 train_acc: 0.9221 train_loss: 0.204951\tval_acc: 0.982332 val_loss: 0.1068825\n",
      "2_565 train_acc: 0.8811 train_loss: 0.262684\tval_acc: 0.971731 val_loss: 0.0947124\n",
      "2_618 train_acc: 0.8730 train_loss: 0.261929\tval_acc: 0.978799 val_loss: 0.0877406\n",
      "2_640 train_acc: 0.8852 train_loss: 0.277281\tval_acc: 0.971731 val_loss: 0.0842523\n",
      "2_675 train_acc: 0.8934 train_loss: 0.273064\tval_acc: 0.964664 val_loss: 0.0799395\n",
      "2_692 train_acc: 0.9016 train_loss: 0.263688\tval_acc: 0.978799 val_loss: 0.0721260\n",
      "2_706 train_acc: 0.8648 train_loss: 0.289431\tval_acc: 0.982332 val_loss: 0.0719476\n",
      "2_728 train_acc: 0.9221 train_loss: 0.201947\tval_acc: 0.978799 val_loss: 0.0675956\n",
      "2_754 train_acc: 0.8730 train_loss: 0.321257\tval_acc: 0.985866 val_loss: 0.0675031\n",
      "2_755 train_acc: 0.8689 train_loss: 0.296890\tval_acc: 0.985866 val_loss: 0.0605863\n",
      "2_763 train_acc: 0.8648 train_loss: 0.294504\tval_acc: 0.978799 val_loss: 0.0587322\n",
      "2_812 train_acc: 0.9098 train_loss: 0.253318\tval_acc: 0.985866 val_loss: 0.0556783\n",
      "2_840 train_acc: 0.8689 train_loss: 0.308251\tval_acc: 0.985866 val_loss: 0.0516418\n",
      "2_880 train_acc: 0.8934 train_loss: 0.241269\tval_acc: 0.989399 val_loss: 0.0483059\n",
      "2_920 train_acc: 0.9057 train_loss: 0.228292\tval_acc: 0.985866 val_loss: 0.0452364\n",
      "epoch:  920 \tThe test accuracy is: 0.7428571428571429\n",
      " THE BEST ACCURACY IS 0.7428571428571429\tkappa is 0.48571428571428577\n",
      "subject 2 duration: 0:09:04.199844\n",
      "seed is 1418\n",
      "Subject 3\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "3_0 train_acc: 0.5287 train_loss: 1.190328\tval_acc: 0.512367 val_loss: 0.7532668\n",
      "3_1 train_acc: 0.6148 train_loss: 1.089339\tval_acc: 0.505300 val_loss: 0.7260625\n",
      "3_3 train_acc: 0.6066 train_loss: 0.777578\tval_acc: 0.713781 val_loss: 0.5927028\n",
      "3_4 train_acc: 0.6311 train_loss: 0.708885\tval_acc: 0.710247 val_loss: 0.5838671\n",
      "3_5 train_acc: 0.5943 train_loss: 0.690194\tval_acc: 0.742049 val_loss: 0.5493488\n",
      "3_7 train_acc: 0.6230 train_loss: 0.677576\tval_acc: 0.713781 val_loss: 0.5336655\n",
      "3_8 train_acc: 0.6803 train_loss: 0.600585\tval_acc: 0.742049 val_loss: 0.5125349\n",
      "3_14 train_acc: 0.7049 train_loss: 0.538609\tval_acc: 0.749117 val_loss: 0.5015478\n",
      "3_15 train_acc: 0.6967 train_loss: 0.553448\tval_acc: 0.738516 val_loss: 0.5004719\n",
      "3_19 train_acc: 0.7582 train_loss: 0.513324\tval_acc: 0.777385 val_loss: 0.4861206\n",
      "3_31 train_acc: 0.6885 train_loss: 0.603554\tval_acc: 0.777385 val_loss: 0.4627703\n",
      "3_39 train_acc: 0.7254 train_loss: 0.506971\tval_acc: 0.784452 val_loss: 0.4535903\n",
      "3_43 train_acc: 0.7541 train_loss: 0.478718\tval_acc: 0.784452 val_loss: 0.4477277\n",
      "3_47 train_acc: 0.7172 train_loss: 0.525245\tval_acc: 0.812721 val_loss: 0.4268047\n",
      "3_62 train_acc: 0.7254 train_loss: 0.516289\tval_acc: 0.819788 val_loss: 0.4258792\n",
      "3_73 train_acc: 0.7131 train_loss: 0.549275\tval_acc: 0.795053 val_loss: 0.4250778\n",
      "3_74 train_acc: 0.7582 train_loss: 0.492756\tval_acc: 0.809187 val_loss: 0.4200575\n",
      "3_76 train_acc: 0.7582 train_loss: 0.453483\tval_acc: 0.837456 val_loss: 0.3856378\n",
      "3_82 train_acc: 0.7828 train_loss: 0.493618\tval_acc: 0.830389 val_loss: 0.3840373\n",
      "3_88 train_acc: 0.7828 train_loss: 0.450847\tval_acc: 0.833922 val_loss: 0.3763393\n",
      "3_90 train_acc: 0.7828 train_loss: 0.450966\tval_acc: 0.830389 val_loss: 0.3687736\n",
      "3_93 train_acc: 0.7746 train_loss: 0.475386\tval_acc: 0.844523 val_loss: 0.3514332\n",
      "3_108 train_acc: 0.7705 train_loss: 0.451623\tval_acc: 0.855124 val_loss: 0.3485002\n",
      "3_109 train_acc: 0.8238 train_loss: 0.397882\tval_acc: 0.858657 val_loss: 0.3443025\n",
      "3_126 train_acc: 0.7746 train_loss: 0.475403\tval_acc: 0.865724 val_loss: 0.3336073\n",
      "3_132 train_acc: 0.8033 train_loss: 0.435677\tval_acc: 0.869258 val_loss: 0.3327822\n",
      "3_143 train_acc: 0.8074 train_loss: 0.406424\tval_acc: 0.840989 val_loss: 0.3322762\n",
      "3_148 train_acc: 0.8033 train_loss: 0.430236\tval_acc: 0.869258 val_loss: 0.3268878\n",
      "3_149 train_acc: 0.7910 train_loss: 0.443545\tval_acc: 0.869258 val_loss: 0.3214906\n",
      "3_151 train_acc: 0.7582 train_loss: 0.491733\tval_acc: 0.869258 val_loss: 0.3201968\n",
      "3_157 train_acc: 0.8566 train_loss: 0.359953\tval_acc: 0.890459 val_loss: 0.3071627\n",
      "3_170 train_acc: 0.7951 train_loss: 0.394471\tval_acc: 0.897527 val_loss: 0.2820254\n",
      "3_213 train_acc: 0.8525 train_loss: 0.331953\tval_acc: 0.890459 val_loss: 0.2588343\n",
      "3_223 train_acc: 0.8320 train_loss: 0.379738\tval_acc: 0.911661 val_loss: 0.2449232\n",
      "3_266 train_acc: 0.8484 train_loss: 0.355963\tval_acc: 0.911661 val_loss: 0.2371228\n",
      "3_274 train_acc: 0.8320 train_loss: 0.402737\tval_acc: 0.922261 val_loss: 0.2277747\n",
      "3_277 train_acc: 0.8443 train_loss: 0.351519\tval_acc: 0.911661 val_loss: 0.2193376\n",
      "3_301 train_acc: 0.8156 train_loss: 0.437707\tval_acc: 0.932862 val_loss: 0.1912656\n",
      "3_351 train_acc: 0.8607 train_loss: 0.324813\tval_acc: 0.911661 val_loss: 0.1890308\n",
      "3_354 train_acc: 0.8811 train_loss: 0.318791\tval_acc: 0.929329 val_loss: 0.1873978\n",
      "3_355 train_acc: 0.8156 train_loss: 0.407354\tval_acc: 0.929329 val_loss: 0.1813512\n",
      "3_380 train_acc: 0.8648 train_loss: 0.289998\tval_acc: 0.943463 val_loss: 0.1767821\n",
      "3_388 train_acc: 0.8443 train_loss: 0.319976\tval_acc: 0.922261 val_loss: 0.1676454\n",
      "3_396 train_acc: 0.8443 train_loss: 0.359564\tval_acc: 0.932862 val_loss: 0.1631871\n",
      "3_412 train_acc: 0.8770 train_loss: 0.308139\tval_acc: 0.929329 val_loss: 0.1600288\n",
      "3_423 train_acc: 0.8852 train_loss: 0.313280\tval_acc: 0.950530 val_loss: 0.1475633\n",
      "3_435 train_acc: 0.8443 train_loss: 0.323006\tval_acc: 0.939929 val_loss: 0.1441552\n",
      "3_468 train_acc: 0.8648 train_loss: 0.329728\tval_acc: 0.961131 val_loss: 0.1422306\n",
      "3_477 train_acc: 0.8689 train_loss: 0.291637\tval_acc: 0.954064 val_loss: 0.1295549\n",
      "3_498 train_acc: 0.8607 train_loss: 0.290336\tval_acc: 0.954064 val_loss: 0.1231968\n",
      "3_517 train_acc: 0.8689 train_loss: 0.294833\tval_acc: 0.968198 val_loss: 0.1140177\n",
      "3_552 train_acc: 0.8852 train_loss: 0.298304\tval_acc: 0.964664 val_loss: 0.1073634\n",
      "3_589 train_acc: 0.9385 train_loss: 0.196312\tval_acc: 0.964664 val_loss: 0.1026238\n",
      "3_594 train_acc: 0.8770 train_loss: 0.301027\tval_acc: 0.985866 val_loss: 0.0994748\n",
      "3_634 train_acc: 0.8934 train_loss: 0.269595\tval_acc: 0.985866 val_loss: 0.0843556\n",
      "3_691 train_acc: 0.9098 train_loss: 0.275995\tval_acc: 0.978799 val_loss: 0.0827838\n",
      "3_696 train_acc: 0.8811 train_loss: 0.292090\tval_acc: 0.975265 val_loss: 0.0809724\n",
      "3_701 train_acc: 0.8730 train_loss: 0.273317\tval_acc: 0.985866 val_loss: 0.0719995\n",
      "3_776 train_acc: 0.8852 train_loss: 0.280043\tval_acc: 0.989399 val_loss: 0.0719489\n",
      "3_781 train_acc: 0.8934 train_loss: 0.244694\tval_acc: 0.989399 val_loss: 0.0689808\n",
      "3_843 train_acc: 0.9098 train_loss: 0.214957\tval_acc: 0.985866 val_loss: 0.0640948\n",
      "3_925 train_acc: 0.9180 train_loss: 0.220233\tval_acc: 0.989399 val_loss: 0.0621948\n",
      "3_933 train_acc: 0.8852 train_loss: 0.251210\tval_acc: 0.996466 val_loss: 0.0531349\n",
      "3_956 train_acc: 0.9057 train_loss: 0.233897\tval_acc: 0.989399 val_loss: 0.0528579\n",
      "3_985 train_acc: 0.8893 train_loss: 0.283644\tval_acc: 0.985866 val_loss: 0.0528011\n",
      "epoch:  985 \tThe test accuracy is: 0.878125\n",
      " THE BEST ACCURACY IS 0.878125\tkappa is 0.75625\n",
      "subject 3 duration: 0:09:02.078874\n",
      "seed is 751\n",
      "Subject 4\n",
      "-------------------- train size： (420, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "4_0 train_acc: 0.4806 train_loss: 1.258747\tval_acc: 0.552189 val_loss: 0.7152209\n",
      "4_1 train_acc: 0.5310 train_loss: 0.982368\tval_acc: 0.707071 val_loss: 0.6519944\n",
      "4_2 train_acc: 0.5233 train_loss: 0.903935\tval_acc: 0.720539 val_loss: 0.6344300\n",
      "4_3 train_acc: 0.6008 train_loss: 0.815135\tval_acc: 0.760943 val_loss: 0.6010505\n",
      "4_4 train_acc: 0.6395 train_loss: 0.734465\tval_acc: 0.784512 val_loss: 0.5460415\n",
      "4_5 train_acc: 0.6202 train_loss: 0.674044\tval_acc: 0.835017 val_loss: 0.4625101\n",
      "4_6 train_acc: 0.7093 train_loss: 0.585511\tval_acc: 0.888889 val_loss: 0.3893375\n",
      "4_7 train_acc: 0.7868 train_loss: 0.476196\tval_acc: 0.858586 val_loss: 0.3096368\n",
      "4_8 train_acc: 0.8023 train_loss: 0.389183\tval_acc: 0.885522 val_loss: 0.2323101\n",
      "4_10 train_acc: 0.9147 train_loss: 0.256110\tval_acc: 0.925926 val_loss: 0.1837167\n",
      "4_11 train_acc: 0.8953 train_loss: 0.247422\tval_acc: 0.939394 val_loss: 0.1282192\n",
      "4_12 train_acc: 0.8837 train_loss: 0.295311\tval_acc: 0.956229 val_loss: 0.1177174\n",
      "4_14 train_acc: 0.8915 train_loss: 0.257045\tval_acc: 0.959596 val_loss: 0.1093967\n",
      "4_27 train_acc: 0.9225 train_loss: 0.224019\tval_acc: 0.962963 val_loss: 0.1072093\n",
      "4_28 train_acc: 0.9535 train_loss: 0.148423\tval_acc: 0.956229 val_loss: 0.0988332\n",
      "4_32 train_acc: 0.9535 train_loss: 0.127828\tval_acc: 0.956229 val_loss: 0.0915662\n",
      "4_50 train_acc: 0.9767 train_loss: 0.067252\tval_acc: 0.966330 val_loss: 0.0894976\n",
      "4_51 train_acc: 0.9535 train_loss: 0.115939\tval_acc: 0.962963 val_loss: 0.0876708\n",
      "4_52 train_acc: 0.9574 train_loss: 0.098802\tval_acc: 0.966330 val_loss: 0.0813519\n",
      "4_54 train_acc: 0.9612 train_loss: 0.137364\tval_acc: 0.973064 val_loss: 0.0809372\n",
      "4_85 train_acc: 0.9690 train_loss: 0.107040\tval_acc: 0.962963 val_loss: 0.0749188\n",
      "4_86 train_acc: 0.9729 train_loss: 0.070114\tval_acc: 0.979798 val_loss: 0.0641696\n",
      "4_102 train_acc: 0.9651 train_loss: 0.125901\tval_acc: 0.976431 val_loss: 0.0544658\n",
      "4_121 train_acc: 0.9729 train_loss: 0.083560\tval_acc: 0.976431 val_loss: 0.0507576\n",
      "4_122 train_acc: 0.9690 train_loss: 0.076418\tval_acc: 0.986532 val_loss: 0.0425105\n",
      "4_136 train_acc: 0.9845 train_loss: 0.059310\tval_acc: 0.993266 val_loss: 0.0402947\n",
      "4_144 train_acc: 0.9806 train_loss: 0.062275\tval_acc: 0.983165 val_loss: 0.0344661\n",
      "4_187 train_acc: 0.9690 train_loss: 0.100163\tval_acc: 0.989899 val_loss: 0.0320317\n",
      "4_195 train_acc: 0.9806 train_loss: 0.049915\tval_acc: 0.993266 val_loss: 0.0287527\n",
      "4_234 train_acc: 0.9845 train_loss: 0.063366\tval_acc: 0.996633 val_loss: 0.0247681\n",
      "4_292 train_acc: 0.9922 train_loss: 0.033829\tval_acc: 0.996633 val_loss: 0.0232801\n",
      "4_295 train_acc: 0.9767 train_loss: 0.051055\tval_acc: 0.993266 val_loss: 0.0217311\n",
      "4_310 train_acc: 0.9651 train_loss: 0.070170\tval_acc: 1.000000 val_loss: 0.0103354\n",
      "4_514 train_acc: 0.9845 train_loss: 0.052961\tval_acc: 1.000000 val_loss: 0.0103078\n",
      "4_525 train_acc: 0.9884 train_loss: 0.025133\tval_acc: 1.000000 val_loss: 0.0102983\n",
      "4_553 train_acc: 0.9922 train_loss: 0.032900\tval_acc: 1.000000 val_loss: 0.0100399\n",
      "4_561 train_acc: 0.9767 train_loss: 0.062177\tval_acc: 1.000000 val_loss: 0.0082536\n",
      "4_672 train_acc: 0.9922 train_loss: 0.018134\tval_acc: 1.000000 val_loss: 0.0072867\n",
      "4_721 train_acc: 0.9961 train_loss: 0.019096\tval_acc: 1.000000 val_loss: 0.0065545\n",
      "4_725 train_acc: 0.9845 train_loss: 0.027348\tval_acc: 1.000000 val_loss: 0.0063251\n",
      "4_795 train_acc: 0.9884 train_loss: 0.028275\tval_acc: 1.000000 val_loss: 0.0059143\n",
      "4_871 train_acc: 0.9884 train_loss: 0.020216\tval_acc: 1.000000 val_loss: 0.0056151\n",
      "4_895 train_acc: 0.9922 train_loss: 0.023550\tval_acc: 1.000000 val_loss: 0.0056007\n",
      "4_897 train_acc: 0.9845 train_loss: 0.030751\tval_acc: 1.000000 val_loss: 0.0046836\n",
      "4_944 train_acc: 0.9884 train_loss: 0.029920\tval_acc: 1.000000 val_loss: 0.0045229\n",
      "4_945 train_acc: 0.9961 train_loss: 0.013399\tval_acc: 1.000000 val_loss: 0.0035021\n",
      "4_954 train_acc: 0.9845 train_loss: 0.039648\tval_acc: 1.000000 val_loss: 0.0029196\n",
      "epoch:  954 \tThe test accuracy is: 0.984375\n",
      " THE BEST ACCURACY IS 0.984375\tkappa is 0.96875\n",
      "subject 4 duration: 0:09:16.842839\n",
      "seed is 268\n",
      "Subject 5\n",
      "-------------------- train size： (420, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "5_0 train_acc: 0.5310 train_loss: 1.296137\tval_acc: 0.501684 val_loss: 0.7826337\n",
      "5_1 train_acc: 0.6008 train_loss: 1.104671\tval_acc: 0.602694 val_loss: 0.6824673\n",
      "5_3 train_acc: 0.5659 train_loss: 0.972887\tval_acc: 0.653199 val_loss: 0.6350303\n",
      "5_4 train_acc: 0.5349 train_loss: 0.963572\tval_acc: 0.707071 val_loss: 0.5998344\n",
      "5_7 train_acc: 0.5814 train_loss: 0.700759\tval_acc: 0.730640 val_loss: 0.5559589\n",
      "5_12 train_acc: 0.6589 train_loss: 0.606188\tval_acc: 0.737374 val_loss: 0.5474965\n",
      "5_21 train_acc: 0.6512 train_loss: 0.630986\tval_acc: 0.740741 val_loss: 0.5470306\n",
      "5_27 train_acc: 0.6783 train_loss: 0.579888\tval_acc: 0.730640 val_loss: 0.5380307\n",
      "5_28 train_acc: 0.7016 train_loss: 0.590032\tval_acc: 0.747475 val_loss: 0.5343273\n",
      "5_34 train_acc: 0.7093 train_loss: 0.563422\tval_acc: 0.737374 val_loss: 0.5342995\n",
      "5_35 train_acc: 0.7442 train_loss: 0.563391\tval_acc: 0.727273 val_loss: 0.5236474\n",
      "5_36 train_acc: 0.6550 train_loss: 0.606127\tval_acc: 0.757576 val_loss: 0.5234911\n",
      "5_39 train_acc: 0.6744 train_loss: 0.592524\tval_acc: 0.771044 val_loss: 0.5190663\n",
      "5_43 train_acc: 0.6822 train_loss: 0.590639\tval_acc: 0.750842 val_loss: 0.5167301\n",
      "5_49 train_acc: 0.7209 train_loss: 0.587386\tval_acc: 0.774411 val_loss: 0.5033113\n",
      "5_54 train_acc: 0.7481 train_loss: 0.549313\tval_acc: 0.767677 val_loss: 0.4987861\n",
      "5_56 train_acc: 0.7054 train_loss: 0.555732\tval_acc: 0.784512 val_loss: 0.4963214\n",
      "5_63 train_acc: 0.7481 train_loss: 0.528411\tval_acc: 0.767677 val_loss: 0.4953136\n",
      "5_64 train_acc: 0.7558 train_loss: 0.554284\tval_acc: 0.777778 val_loss: 0.4866399\n",
      "5_76 train_acc: 0.7713 train_loss: 0.476913\tval_acc: 0.794613 val_loss: 0.4817030\n",
      "5_77 train_acc: 0.7442 train_loss: 0.503553\tval_acc: 0.797980 val_loss: 0.4781053\n",
      "5_79 train_acc: 0.7209 train_loss: 0.503109\tval_acc: 0.781145 val_loss: 0.4765218\n",
      "5_80 train_acc: 0.7636 train_loss: 0.507111\tval_acc: 0.791246 val_loss: 0.4755488\n",
      "5_83 train_acc: 0.7791 train_loss: 0.471923\tval_acc: 0.814815 val_loss: 0.4532126\n",
      "5_84 train_acc: 0.7558 train_loss: 0.489016\tval_acc: 0.831650 val_loss: 0.4431939\n",
      "5_87 train_acc: 0.7791 train_loss: 0.496290\tval_acc: 0.851852 val_loss: 0.4260305\n",
      "5_88 train_acc: 0.7326 train_loss: 0.536062\tval_acc: 0.835017 val_loss: 0.4242406\n",
      "5_92 train_acc: 0.7752 train_loss: 0.452856\tval_acc: 0.838384 val_loss: 0.4107184\n",
      "5_94 train_acc: 0.7868 train_loss: 0.458758\tval_acc: 0.845118 val_loss: 0.3929429\n",
      "5_95 train_acc: 0.7829 train_loss: 0.449612\tval_acc: 0.858586 val_loss: 0.3902511\n",
      "5_96 train_acc: 0.7442 train_loss: 0.519790\tval_acc: 0.848485 val_loss: 0.3719691\n",
      "5_98 train_acc: 0.7946 train_loss: 0.420351\tval_acc: 0.848485 val_loss: 0.3558672\n",
      "5_99 train_acc: 0.7558 train_loss: 0.536939\tval_acc: 0.868687 val_loss: 0.3535970\n",
      "5_100 train_acc: 0.8101 train_loss: 0.436443\tval_acc: 0.865320 val_loss: 0.3466096\n",
      "5_103 train_acc: 0.8217 train_loss: 0.399978\tval_acc: 0.878788 val_loss: 0.3053828\n",
      "5_107 train_acc: 0.8411 train_loss: 0.378244\tval_acc: 0.878788 val_loss: 0.2988961\n",
      "5_110 train_acc: 0.7984 train_loss: 0.421150\tval_acc: 0.888889 val_loss: 0.2813212\n",
      "5_113 train_acc: 0.8023 train_loss: 0.417588\tval_acc: 0.919192 val_loss: 0.2572541\n",
      "5_118 train_acc: 0.8760 train_loss: 0.329747\tval_acc: 0.905724 val_loss: 0.2404868\n",
      "5_122 train_acc: 0.8333 train_loss: 0.360730\tval_acc: 0.902357 val_loss: 0.2335458\n",
      "5_124 train_acc: 0.8333 train_loss: 0.396880\tval_acc: 0.905724 val_loss: 0.2303290\n",
      "5_125 train_acc: 0.8217 train_loss: 0.395891\tval_acc: 0.929293 val_loss: 0.1751235\n",
      "5_144 train_acc: 0.8876 train_loss: 0.248347\tval_acc: 0.925926 val_loss: 0.1647528\n",
      "5_152 train_acc: 0.9070 train_loss: 0.243072\tval_acc: 0.949495 val_loss: 0.1645329\n",
      "5_155 train_acc: 0.8798 train_loss: 0.259560\tval_acc: 0.949495 val_loss: 0.1632934\n",
      "5_165 train_acc: 0.8953 train_loss: 0.298296\tval_acc: 0.952862 val_loss: 0.1295675\n",
      "5_202 train_acc: 0.9070 train_loss: 0.244743\tval_acc: 0.942761 val_loss: 0.1264733\n",
      "5_228 train_acc: 0.9419 train_loss: 0.182430\tval_acc: 0.956229 val_loss: 0.1238611\n",
      "5_242 train_acc: 0.9496 train_loss: 0.151344\tval_acc: 0.956229 val_loss: 0.1170620\n",
      "5_250 train_acc: 0.9147 train_loss: 0.227405\tval_acc: 0.956229 val_loss: 0.1075641\n",
      "5_274 train_acc: 0.9264 train_loss: 0.174051\tval_acc: 0.969697 val_loss: 0.0883001\n",
      "5_332 train_acc: 0.8992 train_loss: 0.221424\tval_acc: 0.973064 val_loss: 0.0876717\n",
      "5_382 train_acc: 0.9264 train_loss: 0.152738\tval_acc: 0.959596 val_loss: 0.0860551\n",
      "5_385 train_acc: 0.9535 train_loss: 0.120681\tval_acc: 0.976431 val_loss: 0.0663875\n",
      "5_413 train_acc: 0.9186 train_loss: 0.171421\tval_acc: 0.979798 val_loss: 0.0655353\n",
      "5_446 train_acc: 0.9419 train_loss: 0.159223\tval_acc: 0.973064 val_loss: 0.0629380\n",
      "5_491 train_acc: 0.9612 train_loss: 0.087264\tval_acc: 0.986532 val_loss: 0.0554414\n",
      "5_543 train_acc: 0.9419 train_loss: 0.193271\tval_acc: 0.979798 val_loss: 0.0530859\n",
      "5_606 train_acc: 0.9690 train_loss: 0.071966\tval_acc: 0.983165 val_loss: 0.0499675\n",
      "5_662 train_acc: 0.9380 train_loss: 0.164796\tval_acc: 0.986532 val_loss: 0.0472317\n",
      "5_676 train_acc: 0.9457 train_loss: 0.121437\tval_acc: 0.993266 val_loss: 0.0468564\n",
      "5_715 train_acc: 0.9341 train_loss: 0.143499\tval_acc: 0.986532 val_loss: 0.0441197\n",
      "5_728 train_acc: 0.9341 train_loss: 0.169365\tval_acc: 0.979798 val_loss: 0.0433595\n",
      "5_733 train_acc: 0.9380 train_loss: 0.145611\tval_acc: 0.986532 val_loss: 0.0414476\n",
      "5_741 train_acc: 0.9574 train_loss: 0.115622\tval_acc: 0.996633 val_loss: 0.0403161\n",
      "5_762 train_acc: 0.9496 train_loss: 0.117481\tval_acc: 0.996633 val_loss: 0.0385144\n",
      "5_803 train_acc: 0.9729 train_loss: 0.078655\tval_acc: 0.993266 val_loss: 0.0354031\n",
      "5_879 train_acc: 0.9419 train_loss: 0.128563\tval_acc: 1.000000 val_loss: 0.0340352\n",
      "5_933 train_acc: 0.9380 train_loss: 0.163787\tval_acc: 0.996633 val_loss: 0.0300198\n",
      "5_944 train_acc: 0.9535 train_loss: 0.127148\tval_acc: 1.000000 val_loss: 0.0236953\n",
      "epoch:  944 \tThe test accuracy is: 0.98125\n",
      " THE BEST ACCURACY IS 0.98125\tkappa is 0.9625\n",
      "subject 5 duration: 0:09:18.486825\n",
      "seed is 431\n",
      "Subject 6\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "6_0 train_acc: 0.5656 train_loss: 1.223406\tval_acc: 0.530035 val_loss: 0.7247073\n",
      "6_1 train_acc: 0.5738 train_loss: 0.953152\tval_acc: 0.611307 val_loss: 0.6536782\n",
      "6_2 train_acc: 0.5861 train_loss: 0.849796\tval_acc: 0.607774 val_loss: 0.6437842\n",
      "6_3 train_acc: 0.5574 train_loss: 0.840745\tval_acc: 0.692580 val_loss: 0.5763626\n",
      "6_4 train_acc: 0.6230 train_loss: 0.775456\tval_acc: 0.759717 val_loss: 0.5304932\n",
      "6_5 train_acc: 0.6189 train_loss: 0.722250\tval_acc: 0.766784 val_loss: 0.4981493\n",
      "6_6 train_acc: 0.6352 train_loss: 0.647828\tval_acc: 0.770318 val_loss: 0.4832780\n",
      "6_7 train_acc: 0.7049 train_loss: 0.636677\tval_acc: 0.773852 val_loss: 0.4665882\n",
      "6_9 train_acc: 0.7008 train_loss: 0.595863\tval_acc: 0.787986 val_loss: 0.4565342\n",
      "6_14 train_acc: 0.7582 train_loss: 0.554190\tval_acc: 0.826855 val_loss: 0.4273755\n",
      "6_32 train_acc: 0.8033 train_loss: 0.433790\tval_acc: 0.826855 val_loss: 0.4185661\n",
      "6_34 train_acc: 0.7295 train_loss: 0.510838\tval_acc: 0.837456 val_loss: 0.3702133\n",
      "6_46 train_acc: 0.8320 train_loss: 0.416384\tval_acc: 0.848057 val_loss: 0.3555315\n",
      "6_49 train_acc: 0.7992 train_loss: 0.455373\tval_acc: 0.869258 val_loss: 0.3453094\n",
      "6_50 train_acc: 0.8115 train_loss: 0.418632\tval_acc: 0.851590 val_loss: 0.3451977\n",
      "6_58 train_acc: 0.7869 train_loss: 0.441850\tval_acc: 0.862191 val_loss: 0.3324090\n",
      "6_65 train_acc: 0.7787 train_loss: 0.474730\tval_acc: 0.851590 val_loss: 0.3320468\n",
      "6_66 train_acc: 0.8361 train_loss: 0.390837\tval_acc: 0.876325 val_loss: 0.3012274\n",
      "6_72 train_acc: 0.7541 train_loss: 0.423512\tval_acc: 0.862191 val_loss: 0.2977348\n",
      "6_80 train_acc: 0.8648 train_loss: 0.334309\tval_acc: 0.872792 val_loss: 0.2845954\n",
      "6_84 train_acc: 0.8484 train_loss: 0.364669\tval_acc: 0.901060 val_loss: 0.2731441\n",
      "6_89 train_acc: 0.8279 train_loss: 0.384652\tval_acc: 0.911661 val_loss: 0.2603763\n",
      "6_91 train_acc: 0.8320 train_loss: 0.357562\tval_acc: 0.904594 val_loss: 0.2501509\n",
      "6_92 train_acc: 0.8566 train_loss: 0.381312\tval_acc: 0.890459 val_loss: 0.2359610\n",
      "6_93 train_acc: 0.8525 train_loss: 0.347262\tval_acc: 0.901060 val_loss: 0.2252518\n",
      "6_97 train_acc: 0.8607 train_loss: 0.331405\tval_acc: 0.908127 val_loss: 0.2153104\n",
      "6_108 train_acc: 0.8648 train_loss: 0.278145\tval_acc: 0.922261 val_loss: 0.1995334\n",
      "6_121 train_acc: 0.8484 train_loss: 0.360123\tval_acc: 0.922261 val_loss: 0.1909205\n",
      "6_122 train_acc: 0.8484 train_loss: 0.340273\tval_acc: 0.932862 val_loss: 0.1873727\n",
      "6_131 train_acc: 0.8893 train_loss: 0.287692\tval_acc: 0.925795 val_loss: 0.1747675\n",
      "6_171 train_acc: 0.8770 train_loss: 0.284129\tval_acc: 0.943463 val_loss: 0.1607217\n",
      "6_190 train_acc: 0.8934 train_loss: 0.268231\tval_acc: 0.929329 val_loss: 0.1585100\n",
      "6_200 train_acc: 0.9098 train_loss: 0.218217\tval_acc: 0.950530 val_loss: 0.1378729\n",
      "6_205 train_acc: 0.8934 train_loss: 0.260605\tval_acc: 0.943463 val_loss: 0.1329055\n",
      "6_224 train_acc: 0.8730 train_loss: 0.306665\tval_acc: 0.954064 val_loss: 0.1325369\n",
      "6_252 train_acc: 0.8934 train_loss: 0.244019\tval_acc: 0.957597 val_loss: 0.1161934\n",
      "6_272 train_acc: 0.9631 train_loss: 0.147371\tval_acc: 0.964664 val_loss: 0.1067528\n",
      "6_326 train_acc: 0.9098 train_loss: 0.257118\tval_acc: 0.968198 val_loss: 0.1034722\n",
      "6_342 train_acc: 0.9180 train_loss: 0.205071\tval_acc: 0.968198 val_loss: 0.0955589\n",
      "6_380 train_acc: 0.9098 train_loss: 0.213624\tval_acc: 0.961131 val_loss: 0.0943218\n",
      "6_396 train_acc: 0.9221 train_loss: 0.202565\tval_acc: 0.968198 val_loss: 0.0939164\n",
      "6_401 train_acc: 0.8975 train_loss: 0.239482\tval_acc: 0.971731 val_loss: 0.0906087\n",
      "6_433 train_acc: 0.9016 train_loss: 0.247820\tval_acc: 0.968198 val_loss: 0.0812631\n",
      "6_506 train_acc: 0.8770 train_loss: 0.248429\tval_acc: 0.978799 val_loss: 0.0788921\n",
      "6_514 train_acc: 0.9057 train_loss: 0.249464\tval_acc: 0.978799 val_loss: 0.0766320\n",
      "6_517 train_acc: 0.9098 train_loss: 0.184737\tval_acc: 0.975265 val_loss: 0.0710230\n",
      "6_547 train_acc: 0.9262 train_loss: 0.170486\tval_acc: 0.985866 val_loss: 0.0654235\n",
      "6_566 train_acc: 0.9385 train_loss: 0.169122\tval_acc: 0.989399 val_loss: 0.0582895\n",
      "6_586 train_acc: 0.9303 train_loss: 0.170684\tval_acc: 0.989399 val_loss: 0.0557179\n",
      "6_611 train_acc: 0.9098 train_loss: 0.204141\tval_acc: 0.989399 val_loss: 0.0553354\n",
      "6_705 train_acc: 0.9180 train_loss: 0.233877\tval_acc: 0.985866 val_loss: 0.0547397\n",
      "6_715 train_acc: 0.9057 train_loss: 0.219661\tval_acc: 0.992933 val_loss: 0.0492769\n",
      "6_772 train_acc: 0.9057 train_loss: 0.203615\tval_acc: 0.985866 val_loss: 0.0477822\n",
      "6_794 train_acc: 0.9180 train_loss: 0.184826\tval_acc: 0.996466 val_loss: 0.0467698\n",
      "6_845 train_acc: 0.9303 train_loss: 0.188565\tval_acc: 0.996466 val_loss: 0.0414537\n",
      "6_933 train_acc: 0.9426 train_loss: 0.173550\tval_acc: 0.992933 val_loss: 0.0301455\n",
      "epoch:  933 \tThe test accuracy is: 0.890625\n",
      " THE BEST ACCURACY IS 0.890625\tkappa is 0.78125\n",
      "subject 6 duration: 0:09:04.941292\n",
      "seed is 296\n",
      "Subject 7\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "7_0 train_acc: 0.5287 train_loss: 1.200277\tval_acc: 0.583039 val_loss: 0.6638750\n",
      "7_1 train_acc: 0.5984 train_loss: 0.996106\tval_acc: 0.674912 val_loss: 0.5998871\n",
      "7_2 train_acc: 0.6885 train_loss: 0.803070\tval_acc: 0.734982 val_loss: 0.5431986\n",
      "7_3 train_acc: 0.6189 train_loss: 0.774027\tval_acc: 0.777385 val_loss: 0.5065148\n",
      "7_4 train_acc: 0.6434 train_loss: 0.819087\tval_acc: 0.816254 val_loss: 0.4513883\n",
      "7_5 train_acc: 0.6598 train_loss: 0.706518\tval_acc: 0.826855 val_loss: 0.4207420\n",
      "7_7 train_acc: 0.6967 train_loss: 0.630336\tval_acc: 0.833922 val_loss: 0.3960014\n",
      "7_18 train_acc: 0.7869 train_loss: 0.445381\tval_acc: 0.837456 val_loss: 0.3813329\n",
      "7_21 train_acc: 0.8402 train_loss: 0.404044\tval_acc: 0.851590 val_loss: 0.3798208\n",
      "7_27 train_acc: 0.8033 train_loss: 0.445545\tval_acc: 0.862191 val_loss: 0.3550479\n",
      "7_32 train_acc: 0.8689 train_loss: 0.344228\tval_acc: 0.855124 val_loss: 0.3460461\n",
      "7_36 train_acc: 0.7869 train_loss: 0.418554\tval_acc: 0.840989 val_loss: 0.3450128\n",
      "7_39 train_acc: 0.8115 train_loss: 0.408981\tval_acc: 0.848057 val_loss: 0.3397357\n",
      "7_40 train_acc: 0.8074 train_loss: 0.436716\tval_acc: 0.851590 val_loss: 0.3369052\n",
      "7_43 train_acc: 0.8607 train_loss: 0.352074\tval_acc: 0.865724 val_loss: 0.3174484\n",
      "7_44 train_acc: 0.8402 train_loss: 0.382616\tval_acc: 0.858657 val_loss: 0.3144240\n",
      "7_45 train_acc: 0.8279 train_loss: 0.392918\tval_acc: 0.897527 val_loss: 0.2662940\n",
      "7_50 train_acc: 0.8607 train_loss: 0.321874\tval_acc: 0.890459 val_loss: 0.2655577\n",
      "7_59 train_acc: 0.8484 train_loss: 0.346449\tval_acc: 0.904594 val_loss: 0.2555922\n",
      "7_65 train_acc: 0.8770 train_loss: 0.287327\tval_acc: 0.911661 val_loss: 0.2279234\n",
      "7_68 train_acc: 0.8770 train_loss: 0.271655\tval_acc: 0.911661 val_loss: 0.2223361\n",
      "7_74 train_acc: 0.8607 train_loss: 0.300408\tval_acc: 0.922261 val_loss: 0.2120878\n",
      "7_86 train_acc: 0.8811 train_loss: 0.227860\tval_acc: 0.918728 val_loss: 0.1989335\n",
      "7_109 train_acc: 0.8934 train_loss: 0.263638\tval_acc: 0.911661 val_loss: 0.1720843\n",
      "7_132 train_acc: 0.9139 train_loss: 0.258925\tval_acc: 0.929329 val_loss: 0.1570908\n",
      "7_145 train_acc: 0.9139 train_loss: 0.229970\tval_acc: 0.932862 val_loss: 0.1452478\n",
      "7_165 train_acc: 0.9098 train_loss: 0.214483\tval_acc: 0.954064 val_loss: 0.1451062\n",
      "7_182 train_acc: 0.8975 train_loss: 0.213417\tval_acc: 0.939929 val_loss: 0.1433834\n",
      "7_200 train_acc: 0.8852 train_loss: 0.297694\tval_acc: 0.943463 val_loss: 0.1423953\n",
      "7_211 train_acc: 0.9303 train_loss: 0.174270\tval_acc: 0.950530 val_loss: 0.1360861\n",
      "7_226 train_acc: 0.9098 train_loss: 0.208143\tval_acc: 0.954064 val_loss: 0.1167622\n",
      "7_264 train_acc: 0.9139 train_loss: 0.241339\tval_acc: 0.943463 val_loss: 0.1088662\n",
      "7_304 train_acc: 0.9344 train_loss: 0.225178\tval_acc: 0.954064 val_loss: 0.1039923\n",
      "7_311 train_acc: 0.9262 train_loss: 0.206350\tval_acc: 0.968198 val_loss: 0.0973002\n",
      "7_326 train_acc: 0.9508 train_loss: 0.149752\tval_acc: 0.961131 val_loss: 0.0955132\n",
      "7_337 train_acc: 0.9549 train_loss: 0.141276\tval_acc: 0.961131 val_loss: 0.0893659\n",
      "7_405 train_acc: 0.9590 train_loss: 0.135886\tval_acc: 0.971731 val_loss: 0.0872964\n",
      "7_407 train_acc: 0.9262 train_loss: 0.199042\tval_acc: 0.968198 val_loss: 0.0758280\n",
      "7_441 train_acc: 0.9262 train_loss: 0.173756\tval_acc: 0.978799 val_loss: 0.0751781\n",
      "7_446 train_acc: 0.9303 train_loss: 0.161341\tval_acc: 0.975265 val_loss: 0.0725362\n",
      "7_457 train_acc: 0.9344 train_loss: 0.186589\tval_acc: 0.982332 val_loss: 0.0713183\n",
      "7_471 train_acc: 0.9549 train_loss: 0.148864\tval_acc: 0.982332 val_loss: 0.0700439\n",
      "7_476 train_acc: 0.9344 train_loss: 0.137039\tval_acc: 0.985866 val_loss: 0.0626942\n",
      "7_490 train_acc: 0.9303 train_loss: 0.199774\tval_acc: 0.989399 val_loss: 0.0488610\n",
      "7_581 train_acc: 0.9549 train_loss: 0.106838\tval_acc: 0.989399 val_loss: 0.0483199\n",
      "7_667 train_acc: 0.9549 train_loss: 0.135587\tval_acc: 0.985866 val_loss: 0.0471977\n",
      "7_668 train_acc: 0.9549 train_loss: 0.138938\tval_acc: 0.989399 val_loss: 0.0416687\n",
      "7_673 train_acc: 0.9303 train_loss: 0.231666\tval_acc: 0.989399 val_loss: 0.0388758\n",
      "7_709 train_acc: 0.9139 train_loss: 0.178669\tval_acc: 0.989399 val_loss: 0.0379873\n",
      "7_725 train_acc: 0.9057 train_loss: 0.198817\tval_acc: 0.992933 val_loss: 0.0378696\n",
      "7_755 train_acc: 0.9754 train_loss: 0.091617\tval_acc: 0.996466 val_loss: 0.0325814\n",
      "7_815 train_acc: 0.9426 train_loss: 0.138394\tval_acc: 0.992933 val_loss: 0.0322253\n",
      "7_844 train_acc: 0.9508 train_loss: 0.135438\tval_acc: 0.996466 val_loss: 0.0300911\n",
      "7_867 train_acc: 0.9549 train_loss: 0.104710\tval_acc: 0.989399 val_loss: 0.0293760\n",
      "7_969 train_acc: 0.9549 train_loss: 0.108644\tval_acc: 0.989399 val_loss: 0.0281299\n",
      "7_975 train_acc: 0.9549 train_loss: 0.093633\tval_acc: 0.992933 val_loss: 0.0275450\n",
      "epoch:  975 \tThe test accuracy is: 0.95625\n",
      " THE BEST ACCURACY IS 0.95625\tkappa is 0.9125\n",
      "subject 7 duration: 0:09:05.676069\n",
      "seed is 2010\n",
      "Subject 8\n",
      "-------------------- train size： (440, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "8_0 train_acc: 0.5270 train_loss: 1.117754\tval_acc: 0.564103 val_loss: 0.7165981\n",
      "8_1 train_acc: 0.6126 train_loss: 0.817198\tval_acc: 0.666667 val_loss: 0.6127045\n",
      "8_3 train_acc: 0.7027 train_loss: 0.626929\tval_acc: 0.769231 val_loss: 0.5323970\n",
      "8_12 train_acc: 0.7432 train_loss: 0.505685\tval_acc: 0.772436 val_loss: 0.4853141\n",
      "8_15 train_acc: 0.8559 train_loss: 0.361344\tval_acc: 0.810897 val_loss: 0.4381600\n",
      "8_47 train_acc: 0.8694 train_loss: 0.298715\tval_acc: 0.826923 val_loss: 0.4207908\n",
      "8_51 train_acc: 0.8604 train_loss: 0.329477\tval_acc: 0.823718 val_loss: 0.3983183\n",
      "8_62 train_acc: 0.8468 train_loss: 0.309845\tval_acc: 0.833333 val_loss: 0.3880949\n",
      "8_68 train_acc: 0.8649 train_loss: 0.305388\tval_acc: 0.833333 val_loss: 0.3778861\n",
      "8_82 train_acc: 0.8964 train_loss: 0.280500\tval_acc: 0.833333 val_loss: 0.3764941\n",
      "8_93 train_acc: 0.9009 train_loss: 0.245096\tval_acc: 0.846154 val_loss: 0.3490140\n",
      "8_99 train_acc: 0.8784 train_loss: 0.269840\tval_acc: 0.849359 val_loss: 0.3467344\n",
      "8_106 train_acc: 0.8964 train_loss: 0.248923\tval_acc: 0.868590 val_loss: 0.2917847\n",
      "8_121 train_acc: 0.9099 train_loss: 0.241823\tval_acc: 0.910256 val_loss: 0.2878709\n",
      "8_169 train_acc: 0.8694 train_loss: 0.247122\tval_acc: 0.887821 val_loss: 0.2851930\n",
      "8_193 train_acc: 0.8874 train_loss: 0.232335\tval_acc: 0.891026 val_loss: 0.2669214\n",
      "8_208 train_acc: 0.8829 train_loss: 0.282962\tval_acc: 0.891026 val_loss: 0.2643156\n",
      "8_209 train_acc: 0.9279 train_loss: 0.204317\tval_acc: 0.894231 val_loss: 0.2517177\n",
      "8_221 train_acc: 0.9144 train_loss: 0.233743\tval_acc: 0.907051 val_loss: 0.2265806\n",
      "8_262 train_acc: 0.9189 train_loss: 0.180641\tval_acc: 0.919872 val_loss: 0.2104892\n",
      "8_327 train_acc: 0.9099 train_loss: 0.238369\tval_acc: 0.916667 val_loss: 0.1923725\n",
      "8_356 train_acc: 0.9279 train_loss: 0.214622\tval_acc: 0.910256 val_loss: 0.1910860\n",
      "8_374 train_acc: 0.9279 train_loss: 0.191134\tval_acc: 0.926282 val_loss: 0.1871476\n",
      "8_417 train_acc: 0.8874 train_loss: 0.294176\tval_acc: 0.935897 val_loss: 0.1775120\n",
      "8_443 train_acc: 0.8784 train_loss: 0.288024\tval_acc: 0.916667 val_loss: 0.1633811\n",
      "8_536 train_acc: 0.9234 train_loss: 0.208864\tval_acc: 0.929487 val_loss: 0.1569294\n",
      "8_553 train_acc: 0.9009 train_loss: 0.261076\tval_acc: 0.939103 val_loss: 0.1411271\n",
      "8_663 train_acc: 0.9279 train_loss: 0.181256\tval_acc: 0.942308 val_loss: 0.1407883\n",
      "8_676 train_acc: 0.9189 train_loss: 0.178520\tval_acc: 0.958333 val_loss: 0.1273842\n",
      "8_728 train_acc: 0.9279 train_loss: 0.180914\tval_acc: 0.951923 val_loss: 0.1240619\n",
      "8_741 train_acc: 0.9189 train_loss: 0.168219\tval_acc: 0.951923 val_loss: 0.1231737\n",
      "8_745 train_acc: 0.9234 train_loss: 0.177813\tval_acc: 0.955128 val_loss: 0.1225231\n",
      "8_761 train_acc: 0.9459 train_loss: 0.159113\tval_acc: 0.964744 val_loss: 0.1077672\n",
      "8_841 train_acc: 0.9324 train_loss: 0.176586\tval_acc: 0.977564 val_loss: 0.1076998\n",
      "8_873 train_acc: 0.9369 train_loss: 0.170933\tval_acc: 0.964744 val_loss: 0.1052965\n",
      "8_878 train_acc: 0.9189 train_loss: 0.190161\tval_acc: 0.974359 val_loss: 0.0967816\n",
      "8_892 train_acc: 0.9279 train_loss: 0.185860\tval_acc: 0.974359 val_loss: 0.0928389\n",
      "8_993 train_acc: 0.8829 train_loss: 0.240279\tval_acc: 0.974359 val_loss: 0.0904263\n",
      "epoch:  993 \tThe test accuracy is: 0.95625\n",
      " THE BEST ACCURACY IS 0.95625\tkappa is 0.9125\n",
      "subject 8 duration: 0:10:35.633426\n",
      "seed is 8\n",
      "Subject 9\n",
      "-------------------- train size： (400, 1, 3, 1000) test size： (320, 3, 1000)\n",
      "9_0 train_acc: 0.5574 train_loss: 1.111280\tval_acc: 0.515901 val_loss: 0.7429417\n",
      "9_1 train_acc: 0.5041 train_loss: 1.129499\tval_acc: 0.512367 val_loss: 0.7109777\n",
      "9_2 train_acc: 0.5287 train_loss: 0.936669\tval_acc: 0.572438 val_loss: 0.6705222\n",
      "9_4 train_acc: 0.5574 train_loss: 0.788990\tval_acc: 0.625442 val_loss: 0.6643164\n",
      "9_5 train_acc: 0.5697 train_loss: 0.739994\tval_acc: 0.639576 val_loss: 0.6598514\n",
      "9_7 train_acc: 0.5738 train_loss: 0.727261\tval_acc: 0.727915 val_loss: 0.6352016\n",
      "9_8 train_acc: 0.5574 train_loss: 0.732967\tval_acc: 0.710247 val_loss: 0.6351203\n",
      "9_10 train_acc: 0.6352 train_loss: 0.657865\tval_acc: 0.706714 val_loss: 0.6280303\n",
      "9_12 train_acc: 0.6434 train_loss: 0.664322\tval_acc: 0.731449 val_loss: 0.6223608\n",
      "9_13 train_acc: 0.6025 train_loss: 0.698585\tval_acc: 0.742049 val_loss: 0.6217632\n",
      "9_14 train_acc: 0.6475 train_loss: 0.612437\tval_acc: 0.696113 val_loss: 0.6190608\n",
      "9_17 train_acc: 0.6352 train_loss: 0.641117\tval_acc: 0.713781 val_loss: 0.6086448\n",
      "9_18 train_acc: 0.5902 train_loss: 0.643270\tval_acc: 0.717314 val_loss: 0.6007553\n",
      "9_20 train_acc: 0.7049 train_loss: 0.601741\tval_acc: 0.692580 val_loss: 0.5997657\n",
      "9_21 train_acc: 0.6680 train_loss: 0.608136\tval_acc: 0.745583 val_loss: 0.5768469\n",
      "9_22 train_acc: 0.6639 train_loss: 0.600533\tval_acc: 0.706714 val_loss: 0.5637017\n",
      "9_23 train_acc: 0.7213 train_loss: 0.576802\tval_acc: 0.756184 val_loss: 0.5583600\n",
      "9_25 train_acc: 0.6434 train_loss: 0.613151\tval_acc: 0.738516 val_loss: 0.5498852\n",
      "9_26 train_acc: 0.6803 train_loss: 0.591844\tval_acc: 0.749117 val_loss: 0.5472271\n",
      "9_27 train_acc: 0.7090 train_loss: 0.578446\tval_acc: 0.756184 val_loss: 0.5240152\n",
      "9_30 train_acc: 0.7582 train_loss: 0.533411\tval_acc: 0.745583 val_loss: 0.5182959\n",
      "9_31 train_acc: 0.7213 train_loss: 0.582386\tval_acc: 0.759717 val_loss: 0.5091503\n",
      "9_33 train_acc: 0.7172 train_loss: 0.586407\tval_acc: 0.780919 val_loss: 0.4632571\n",
      "9_37 train_acc: 0.7541 train_loss: 0.465591\tval_acc: 0.798587 val_loss: 0.4270096\n",
      "9_47 train_acc: 0.8033 train_loss: 0.449626\tval_acc: 0.795053 val_loss: 0.4141451\n",
      "9_49 train_acc: 0.7992 train_loss: 0.466557\tval_acc: 0.816254 val_loss: 0.3903607\n",
      "9_63 train_acc: 0.8074 train_loss: 0.423865\tval_acc: 0.823322 val_loss: 0.3747976\n",
      "9_73 train_acc: 0.8238 train_loss: 0.388325\tval_acc: 0.830389 val_loss: 0.3581336\n",
      "9_87 train_acc: 0.8689 train_loss: 0.328489\tval_acc: 0.816254 val_loss: 0.3565355\n",
      "9_89 train_acc: 0.8525 train_loss: 0.373827\tval_acc: 0.805654 val_loss: 0.3477755\n",
      "9_90 train_acc: 0.8279 train_loss: 0.393455\tval_acc: 0.844523 val_loss: 0.3233077\n",
      "9_109 train_acc: 0.8361 train_loss: 0.389445\tval_acc: 0.851590 val_loss: 0.3147034\n",
      "9_120 train_acc: 0.8770 train_loss: 0.319352\tval_acc: 0.869258 val_loss: 0.3104045\n",
      "9_130 train_acc: 0.8320 train_loss: 0.353124\tval_acc: 0.851590 val_loss: 0.2852969\n",
      "9_139 train_acc: 0.8115 train_loss: 0.385179\tval_acc: 0.851590 val_loss: 0.2811081\n",
      "9_148 train_acc: 0.8689 train_loss: 0.317384\tval_acc: 0.862191 val_loss: 0.2725301\n",
      "9_171 train_acc: 0.8525 train_loss: 0.272723\tval_acc: 0.883392 val_loss: 0.2631789\n",
      "9_172 train_acc: 0.8648 train_loss: 0.287288\tval_acc: 0.883392 val_loss: 0.2463478\n",
      "9_199 train_acc: 0.8934 train_loss: 0.257549\tval_acc: 0.897527 val_loss: 0.2427730\n",
      "9_200 train_acc: 0.8648 train_loss: 0.302322\tval_acc: 0.897527 val_loss: 0.2305119\n",
      "9_201 train_acc: 0.9139 train_loss: 0.247903\tval_acc: 0.911661 val_loss: 0.2226723\n",
      "9_232 train_acc: 0.9098 train_loss: 0.253573\tval_acc: 0.943463 val_loss: 0.1925076\n",
      "9_255 train_acc: 0.8525 train_loss: 0.332769\tval_acc: 0.922261 val_loss: 0.1876600\n",
      "9_280 train_acc: 0.8730 train_loss: 0.288757\tval_acc: 0.939929 val_loss: 0.1854098\n",
      "9_292 train_acc: 0.8893 train_loss: 0.293817\tval_acc: 0.939929 val_loss: 0.1716411\n",
      "9_313 train_acc: 0.9016 train_loss: 0.232876\tval_acc: 0.936396 val_loss: 0.1655558\n",
      "9_346 train_acc: 0.8975 train_loss: 0.234298\tval_acc: 0.943463 val_loss: 0.1642382\n",
      "9_372 train_acc: 0.9139 train_loss: 0.231559\tval_acc: 0.954064 val_loss: 0.1590932\n",
      "9_382 train_acc: 0.9057 train_loss: 0.254458\tval_acc: 0.936396 val_loss: 0.1562072\n",
      "9_383 train_acc: 0.9098 train_loss: 0.219548\tval_acc: 0.950530 val_loss: 0.1454595\n",
      "9_402 train_acc: 0.8852 train_loss: 0.255651\tval_acc: 0.961131 val_loss: 0.1449597\n",
      "9_436 train_acc: 0.9139 train_loss: 0.237163\tval_acc: 0.957597 val_loss: 0.1273630\n",
      "9_491 train_acc: 0.9303 train_loss: 0.183275\tval_acc: 0.975265 val_loss: 0.1133977\n",
      "9_529 train_acc: 0.8975 train_loss: 0.248170\tval_acc: 0.975265 val_loss: 0.1058247\n",
      "9_533 train_acc: 0.8852 train_loss: 0.232224\tval_acc: 0.982332 val_loss: 0.1016268\n",
      "9_573 train_acc: 0.8811 train_loss: 0.285524\tval_acc: 0.982332 val_loss: 0.0979867\n",
      "9_592 train_acc: 0.9057 train_loss: 0.218494\tval_acc: 0.978799 val_loss: 0.0924487\n",
      "9_617 train_acc: 0.8975 train_loss: 0.233166\tval_acc: 0.975265 val_loss: 0.0907337\n",
      "9_679 train_acc: 0.9221 train_loss: 0.168097\tval_acc: 0.975265 val_loss: 0.0903887\n",
      "9_683 train_acc: 0.9016 train_loss: 0.241755\tval_acc: 0.978799 val_loss: 0.0888790\n",
      "9_709 train_acc: 0.9098 train_loss: 0.190935\tval_acc: 0.975265 val_loss: 0.0871657\n",
      "9_710 train_acc: 0.9180 train_loss: 0.182843\tval_acc: 0.971731 val_loss: 0.0850690\n",
      "9_720 train_acc: 0.9303 train_loss: 0.144326\tval_acc: 0.989399 val_loss: 0.0826783\n",
      "9_755 train_acc: 0.9467 train_loss: 0.128954\tval_acc: 0.975265 val_loss: 0.0799230\n",
      "9_778 train_acc: 0.9180 train_loss: 0.195745\tval_acc: 0.985866 val_loss: 0.0794143\n",
      "9_813 train_acc: 0.9262 train_loss: 0.185401\tval_acc: 0.985866 val_loss: 0.0773006\n",
      "9_823 train_acc: 0.9221 train_loss: 0.190835\tval_acc: 0.985866 val_loss: 0.0715055\n",
      "9_879 train_acc: 0.9549 train_loss: 0.154202\tval_acc: 0.985866 val_loss: 0.0699131\n",
      "9_884 train_acc: 0.9344 train_loss: 0.174916\tval_acc: 0.996466 val_loss: 0.0561837\n",
      "epoch:  884 \tThe test accuracy is: 0.921875\n",
      " THE BEST ACCURACY IS 0.921875\tkappa is 0.84375\n",
      "subject 9 duration: 0:09:34.696848\n",
      "**The average Best accuracy is: 90.71924603174602kappa is: 81.43849206349206\n",
      "\n",
      "best epochs:  [964, 920, 985, 954, 944, 933, 975, 993, 884]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     85.312500  85.931476  85.312500  85.248973  70.625000\n",
      "1     74.285714  74.285714  74.285714  74.285714  48.571429\n",
      "2     87.812500  87.932521  87.812500  87.802852  75.625000\n",
      "3     98.437500  98.439392  98.437500  98.437485  96.875000\n",
      "4     98.125000  98.192771  98.125000  98.124341  96.250000\n",
      "5     89.062500  89.100684  89.062500  89.059829  78.125000\n",
      "6     95.625000  95.653533  95.625000  95.624316  91.250000\n",
      "7     95.625000  95.653533  95.625000  95.624316  91.250000\n",
      "8     92.187500  92.189148  92.187500  92.187424  84.375000\n",
      "mean  90.719246  90.819864  90.719246  90.710583  81.438492\n",
      "std    7.707084   7.661490   7.707084   7.713015  15.414167\n",
      "****************************************\n",
      "Mon Feb 24 12:44:13 2025\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# The code is based on this github project \n",
    "#    https://github.com/snailpt/CTNet/tree/main\n",
    "########################################################################################\n",
    "\n",
    "import os\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # temporal conv kernel size 64=0.25fs\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 22, 1000] \n",
    "            nn.BatchNorm2d(f1),\n",
    "            # channel depth-wise conv\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), # \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # average pooling 1\n",
    "            nn.AvgPool2d((1, pooling_size1)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # spatial conv\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "\n",
    "            # average pooling 2 to adjust the length of feature into transformer encoder\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),  \n",
    "                    \n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "########################################################################################\n",
    "# The Transformer code is based on this github project and has been fine-tuned: \n",
    "#    https://github.com/eeyhsong/EEG-Conformer\n",
    "########################################################################################\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        \n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, dim, cf: float = 1.0, num_experts: int = 1, top_k: int = 1, epsilon: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.epsilon = epsilon\n",
    "        self.top_k = top_k\n",
    "        self.w_gate = nn.Linear(dim, num_experts)\n",
    "        # print(\"GatingNetwork initialized with:\", cf, num_experts, top_k)\n",
    "\n",
    "    def forward(self, x: Tensor, use_aux_loss=False):\n",
    "        gate_scores = F.softmax(self.w_gate(x), dim=-1)  # [batch, num_experts]\n",
    "        capacity = int(self.cf * x.size(0))\n",
    "        # selecting top_k\n",
    "        top_k_scores, top_k_indices = gate_scores.topk(self.top_k, dim=-1)\n",
    "        mask = torch.zeros_like(gate_scores).scatter_(-1, top_k_indices, 1)\n",
    "        masked_gate_scores = gate_scores * mask\n",
    "        denominators = masked_gate_scores.sum(0, keepdim=True) + self.epsilon  # [1, num_experts]\n",
    "        gate_scores_norm = (masked_gate_scores / denominators) * capacity  # [batch, num_experts]\n",
    "\n",
    "        if use_aux_loss:\n",
    "            # caculate load per expert in a batch\n",
    "            load = gate_scores_norm.sum(0)  # [num_experts]\n",
    "            # load balancing loss: CV²\n",
    "            aux_loss = (torch.std(load) / (torch.mean(load) + self.epsilon)) ** 2\n",
    "            return gate_scores_norm, aux_loss, top_k_indices\n",
    "        return gate_scores_norm, top_k_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MoE_Layer(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, output_dim: int, cf: float = 1.0, num_experts: int = 1, top_k: int = 1, mult: int = 4, use_aux_loss: bool = False, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.mult = mult\n",
    "        self.use_aux_loss = use_aux_loss\n",
    "        self.top_k = top_k\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim * mult),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(dim * mult, hidden_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = GatingNetwork(dim, cf, num_experts, top_k)\n",
    "        self.projection = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if self.use_aux_loss:\n",
    "            gate_scores, aux_loss, top_k_indices = self.gate(x, use_aux_loss=True)\n",
    "        else:\n",
    "            gate_scores, top_k_indices = self.gate(x, use_aux_loss=False)\n",
    "            aux_loss = 0.0\n",
    "\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  \n",
    "        stacked_expert_outputs = torch.stack(expert_outputs, dim=-1) \n",
    "        moe_output = torch.sum(gate_scores.unsqueeze(-2) * stacked_expert_outputs, dim=-1)\n",
    "        moe_output = self.projection(moe_output)\n",
    "        return moe_output, aux_loss, top_k_indices\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads=4, cf=1.0, num_experts=4, top_k=1):\n",
    "        super().__init__()\n",
    "        self.drop_p = 0.5\n",
    "        self.forward_expansion = 4\n",
    "        self.forward_drop_p = 0.5\n",
    "        self.attention = ResidualAdd(nn.Sequential(\n",
    "            MultiHeadAttention(emb_size, num_heads, self.drop_p)\n",
    "        ), emb_size, self.drop_p)\n",
    "        self.moe = MoE_Layer(emb_size, emb_size * num_heads, emb_size, cf, num_experts, top_k, use_aux_loss=True)\n",
    "        self.ffn = ResidualAdd(nn.Sequential(\n",
    "            FeedForwardBlock(emb_size, expansion=self.forward_expansion, drop_p=self.forward_drop_p)\n",
    "        ), emb_size, self.drop_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        moe_out, aux_loss, top_k_indices = self.moe(x)\n",
    "        x = x + moe_out  # residual connection\n",
    "        x = self.ffn(x)\n",
    "        return x, aux_loss, top_k_indices\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, heads, depth, emb_size, cf, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([TransformerEncoderBlock(emb_size, heads, cf, num_experts, top_k) for _ in range(depth)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        total_aux_loss = 0.0\n",
    "        expert_indices_per_block = []\n",
    "        for block in self.blocks:\n",
    "            x, aux_loss, top_k_indices = block(x)\n",
    "            total_aux_loss += aux_loss\n",
    "            expert_indices_per_block.append(top_k_indices.detach().cpu().numpy())\n",
    "        return x, total_aux_loss, expert_indices_per_block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "# learnable positional embedding module        \n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)        \n",
    "        \n",
    "   \n",
    "        \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1=20,\n",
    "                 eeg1_kernel_size=64,\n",
    "                 eeg1_D=2,\n",
    "                 eeg1_pooling_size1=8,\n",
    "                 eeg1_pooling_size2=8,\n",
    "                 eeg1_dropout_rate=0.3,\n",
    "                 eeg1_number_channel=22,\n",
    "                 flatten_eeg1=600,  \n",
    "                 cf=1.0,\n",
    "                 num_experts=4,\n",
    "                 top_k=1,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.cf = cf\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1=eeg1_f1,\n",
    "                                              kernel_size=eeg1_kernel_size,\n",
    "                                              D=eeg1_D,\n",
    "                                              pooling_size1=eeg1_pooling_size1,\n",
    "                                              pooling_size2=eeg1_pooling_size2,\n",
    "                                              dropout_rate=eeg1_dropout_rate)\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(heads, depth, emb_size, cf, num_experts, top_k)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1, self.number_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        trans, aux_loss, expert_indices = self.trans(cnn)\n",
    "        features = cnn + trans \n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out, aux_loss, expert_indices\n",
    "\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads=heads\n",
    "        self.emb_size=emb_size\n",
    "        self.depth=depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "             heads=self.heads, \n",
    "             emb_size=self.emb_size,\n",
    "             depth=self.depth, \n",
    "            database_type=self.dataset_type, \n",
    "            eeg1_f1=eeg1_f1, \n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "            eeg1_number_channel = self.number_channel,\n",
    "            flatten_eeg1 = flatten_eeg1,  \n",
    "            cf = cf,\n",
    "            num_experts = num_experts,\n",
    "            top_k = top_k\n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "\n",
    "    # Segmentation and Reconstruction (S&R) data augmentation\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            \n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        aug_data = aug_data.float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label)  \n",
    "\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        # print(\"len(self.allData):\", len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]  # (288, 1, 22, 1000)\n",
    "        # print(\"shuffle_num\", shuffle_num)\n",
    "        # print(\"self.allLabel\", self.allLabel)\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "\n",
    "        print('-'*20, \"train size：\", self.train_data.shape, \"test size：\", self.test_data.shape)\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "\n",
    "\n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "\n",
    "        \n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        img, label, test_data, test_label = self.get_source_data()\n",
    "        # print(\"label size:\", label.shape)\n",
    "        # print(\"label size:\", label)\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the cnn model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            # 验证集\n",
    "            val_data_list = []\n",
    "            val_label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                number_validate = int(self.validate_ratio * number_sample)\n",
    "                \n",
    "                # split raw train dataset into real train dataset and validate dataset\n",
    "                train_data = img[:-number_validate]\n",
    "                train_label = label[:-number_validate]\n",
    "                \n",
    "                val_data_list.append(img[number_validate:])\n",
    "                val_label_list.append(label[number_validate:])\n",
    "                \n",
    "                # real train dataset\n",
    "                img = Variable(train_data.type(self.Tensor))\n",
    "                label = Variable(train_label.type(self.LongTensor))\n",
    "                \n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat real train dataset and generate aritifical train dataset\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "\n",
    "                # training model\n",
    "                features, outputs, aux_loss, expert_indices = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "                # print(features.size())\n",
    "                loss_cls = self.criterion_cls(outputs, label)\n",
    "                lambda_aux = 0.01  # load_balancing loss weight\n",
    "                loss = loss_cls + lambda_aux * aux_loss\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            # out_epoch = time.time()\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                # validate model\n",
    "                val_data = torch.cat(val_data_list).cuda()\n",
    "                val_label = torch.cat(val_label_list).cuda()\n",
    "                val_data = val_data.type(self.Tensor)\n",
    "                val_label = val_label.type(self.LongTensor)            \n",
    "                \n",
    "                val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "                self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "                outputs_list = []\n",
    "                with torch.no_grad():\n",
    "                    for i, (img, _) in enumerate(self.val_dataloader):\n",
    "                        # val model\n",
    "                        img = img.type(self.Tensor).cuda()\n",
    "                        _, Cls, aux_loss, _ = self.model(img)\n",
    "                        outputs_list.append(Cls)\n",
    "                        del img, Cls\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                Cls = torch.cat(outputs_list)\n",
    "                \n",
    "                val_loss = self.criterion_cls(Cls, val_label)\n",
    "                val_loss = val_loss+lambda_aux*aux_loss\n",
    "                val_pred = torch.max(Cls, 1)[1]\n",
    "                val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "                \n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "\n",
    "\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "                num = num + 1\n",
    "\n",
    "                # if min_loss>val_loss:                \n",
    "                if min_loss>val_loss:\n",
    "                    min_loss = val_loss\n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                        ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "\n",
    "        \n",
    "            del label, val_data, val_label\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename, weights_only=False).cuda()\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img.type(self.Tensor)).cuda()\n",
    "                # label_test = Variable(label.type(self.LongTensor))\n",
    "\n",
    "                # test model\n",
    "                features, outputs,_,_ = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        \n",
    "        \n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        \n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent', # \"LOSO\" or other\n",
    "         heads=8,             # heads of MHA\n",
    "         emb_size=48,         # token embding dim\n",
    "         depth=3,             # Transformer encoder depth\n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         eeg1_f1=20,          # features of temporal conv\n",
    "         eeg1_kernel_size=64, # kernel size of temporal conv\n",
    "         eeg1_D=2,            # depth-wise conv \n",
    "         eeg1_pooling_size1=8,# p1\n",
    "         eeg1_pooling_size2=8,# p2\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio = 0.2,\n",
    "         cf = 1,\n",
    "         num_experts=4,\n",
    "         top_k=1\n",
    "         ):\n",
    "\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    \n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "\n",
    "    for i in range(N_SUBJECT):      \n",
    "        \n",
    "        seed_n = np.random.randint(2024)\n",
    "\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus, \n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads, \n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth, \n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1 = eeg1_f1,\n",
    "                  eeg1_kernel_size = eeg1_kernel_size,\n",
    "                  eeg1_D = eeg1_D,\n",
    "                  eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "                  flatten_eeg1 = flatten_eeg1,  \n",
    "                  validate_ratio = validate_ratio\n",
    "                  )\n",
    "        starttime = datetime.datetime.now()\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {'accuray': accuracy*100,\n",
    "                          'precision': precison*100,\n",
    "                          'recall': recall*100,\n",
    "                          'f1': f1*100, \n",
    "                          'kappa': kappa*100\n",
    "                          }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "    \n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "   \n",
    "        df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GatingNetworkwrapper(nn.Module):\n",
    "    def __init__(self, dim, cf: float = 1.0, num_experts: int = 1,top_k:int = 1, epsilon: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.epsilon = epsilon\n",
    "        self.w_gate = nn.Linear(dim, num_experts)\n",
    "        # print(cf, num_experts, top_k)\n",
    "    def forward(self, x: Tensor, use_aux_loss=False):\n",
    "        gate_scores = F.softmax(self.w_gate(x), dim=-1)\n",
    "        capacity = int(self.cf * x.size(0))\n",
    "        top_k_scores, top_k_indices = gate_scores.topk(top_k, dim=-1)\n",
    "        mask = torch.zeros_like(gate_scores).scatter_(-1, top_k_indices, 1)\n",
    "        masked_gate_scores = gate_scores * mask\n",
    "        denominators = masked_gate_scores.sum(0, keepdim=True) + self.epsilon\n",
    "        gate_scores = (masked_gate_scores / denominators) * capacity\n",
    "\n",
    "        if use_aux_loss:\n",
    "            load = gate_scores.sum(0)\n",
    "            importance = gate_scores.sum(1)\n",
    "            loss = ((load - importance) ** 2).mean()\n",
    "            return gate_scores, loss\n",
    "\n",
    "        return gate_scores\n",
    "\n",
    "class MoE_Layerwrapper(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, output_dim: int, cf: float = 1.0, num_experts: int = 1, top_k:int = 1, mult: int = 4, use_aux_loss: bool = False, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.mult = mult\n",
    "        self.use_aux_loss = use_aux_loss\n",
    "        self.top_k = top_k\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim*mult),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(dim*mult, hidden_dim)\n",
    "\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = GatingNetworkwrapper(dim, cf, num_experts, top_k)\n",
    "        self.projection = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        gate_scores = self.gate(x, use_aux_loss=self.use_aux_loss)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "\n",
    "        if torch.isnan(gate_scores).any():\n",
    "            print(\"NaN in gate scores\")\n",
    "            gate_scores[torch.isnan(gate_scores)] = 0\n",
    "\n",
    "        stacked_expert_outputs = torch.stack(expert_outputs, dim=-1)\n",
    "        if torch.isnan(stacked_expert_outputs).any():\n",
    "            stacked_expert_outputs[torch.isnan(stacked_expert_outputs)] = 0\n",
    "\n",
    "        moe_output = torch.sum(gate_scores.unsqueeze(-2) * stacked_expert_outputs, dim=-1)\n",
    "        moe_output = self.projection(moe_output)\n",
    "        return moe_output\n",
    "\n",
    "class TransformerEncoderBlockwrapper(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 cf=1.0, num_experts=4, top_k=1):\n",
    "        self.drop_p=0.5\n",
    "        self.forward_expansion=4\n",
    "        self.forward_drop_p=0.5\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, self.drop_p),\n",
    "                ), emb_size, self.drop_p),\n",
    "            MoE_Layerwrapper(emb_size, emb_size * num_heads, emb_size, cf, num_experts, top_k),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=self.forward_expansion, drop_p=self.forward_drop_p),\n",
    "                ), emb_size, self.drop_p)\n",
    "            \n",
    "            )    \n",
    "        \n",
    "        \n",
    "class TransformerEncoderwrapper(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size, cf, num_experts, top_k):\n",
    "        super().__init__(*[TransformerEncoderBlockwrapper(emb_size, heads, cf, num_experts, top_k) for _ in range(depth)])\n",
    "      \n",
    "        \n",
    "   \n",
    "        \n",
    "# CTNet       \n",
    "class EEGTransformerwrapper(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 600,\n",
    "                 cf = 1.0,\n",
    "                 num_experts=4,\n",
    "                 top_k=1,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.cf = cf\n",
    "        self.num_experts=num_experts\n",
    "        self.top_k=top_k\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.flatten = nn.Flatten()\n",
    "        # print('self.number_channel', self.number_channel)\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1 = eeg1_f1,\n",
    "                                              kernel_size = eeg1_kernel_size,\n",
    "                                              D = eeg1_D,\n",
    "                                              pooling_size1 = eeg1_pooling_size1,\n",
    "                                              pooling_size2 = eeg1_pooling_size2,\n",
    "                                              dropout_rate = eeg1_dropout_rate,\n",
    "                                              )\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoderwrapper(heads, depth, emb_size, cf, num_experts, top_k)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class) # FLATTEN_EEGNet + FLATTEN_cnn_module\n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "\n",
    "        #  positional embedding\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        \n",
    "        trans = self.trans(cnn)\n",
    "        # residual connect\n",
    "        features = cnn+trans\n",
    "        \n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    N_SUBJECT = 9       # BCI \n",
    "    N_AUG = 3           # data augmentation times for benerating artificial training data set\n",
    "    N_SEG = 8           # segmentation times for S&R\n",
    "\n",
    "    EPOCHS = 1000\n",
    "    EMB_DIM = 16\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "    TYPE = 'B'\n",
    "    validate_ratio = 0.3 # split raw train dataset into real train dataset and validate dataset\n",
    "\n",
    "    EEGNet1_F1 = 8\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "    FLATTEN_EEGNet1 = 240\n",
    "\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25    \n",
    "\n",
    "    # for cf in [1, 1.2, 1.5]:\n",
    "    for cf in [1.2]: # best cf\n",
    "        # for num_experts in [2, 4, 8]:\n",
    "        for num_experts in [4]: # best num_experts\n",
    "            if num_experts==2:\n",
    "                k = [1, 2]\n",
    "            elif num_experts==4:\n",
    "                # k = [1,2,4]\n",
    "                k = [2] # best top_k\n",
    "            else:\n",
    "                k = [1,2,4,8]\n",
    "            \n",
    "            for top_k in k:\n",
    "            \n",
    "                number_class, number_channel = numberClassChannel(TYPE)\n",
    "                RESULT_NAME = \"{}_heads_{}_depth_{}_cf_{}_num_exerts_{}_top_k_{}\".format(TYPE, HEADS, DEPTH, cf, num_experts, top_k)\n",
    "\n",
    "                sModel = EEGTransformer(\n",
    "                    heads=HEADS, \n",
    "                    emb_size=EMB_DIM,\n",
    "                    depth=DEPTH, \n",
    "                    database_type=TYPE,\n",
    "                    eeg1_f1=EEGNet1_F1, \n",
    "                    eeg1_D=EEGNet1_D,\n",
    "                    eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "                    eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                    eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                    eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                    eeg1_number_channel = number_channel,\n",
    "                    flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "                    cf = cf,\n",
    "                    num_experts=num_experts,\n",
    "                    top_k=top_k\n",
    "                    ).cuda()\n",
    "                summaryModel = EEGTransformerwrapper(\n",
    "                    heads=HEADS, \n",
    "                    emb_size=EMB_DIM,\n",
    "                    depth=DEPTH, \n",
    "                    database_type=TYPE,\n",
    "                    eeg1_f1=EEGNet1_F1, \n",
    "                    eeg1_D=EEGNet1_D,\n",
    "                    eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "                    eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                    eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                    eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                    eeg1_number_channel = number_channel,\n",
    "                    flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "                    cf = cf,\n",
    "                    num_experts=num_experts,\n",
    "                    top_k=top_k\n",
    "                    ).cuda()\n",
    "                # wrapped_model = EEGTransformerWrapper(sModel)\n",
    "                summary(summaryModel, (1, number_channel, 1000))\n",
    "\n",
    "                print(time.asctime(time.localtime(time.time())))\n",
    "\n",
    "\n",
    "                result = main(RESULT_NAME,\n",
    "                                evaluate_mode = EVALUATE_MODE,\n",
    "                                heads=HEADS, \n",
    "                                emb_size=EMB_DIM,\n",
    "                                depth=DEPTH, \n",
    "                                dataset_type=TYPE,\n",
    "                                eeg1_f1 = EEGNet1_F1,\n",
    "                                eeg1_kernel_size = EEGNet1_KERNEL_SIZE,\n",
    "                                eeg1_D = EEGNet1_D,\n",
    "                                eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                                eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                                eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                                flatten_eeg1 = FLATTEN_EEGNet1,\n",
    "                                validate_ratio = validate_ratio,\n",
    "                                cf = cf,\n",
    "                                num_experts=num_experts,\n",
    "                                top_k=top_k\n",
    "                              )\n",
    "                print(time.asctime(time.localtime(time.time())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdba101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bc0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
