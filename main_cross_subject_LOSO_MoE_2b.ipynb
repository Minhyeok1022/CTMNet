{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cad76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 8, 3, 1000]             512\n",
      "       BatchNorm2d-2           [-1, 8, 3, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]              48\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "          Dropout-15               [-1, 15, 16]               0\n",
      "PositioinalEncoding-16               [-1, 15, 16]               0\n",
      "           Linear-17               [-1, 15, 16]             272\n",
      "           Linear-18               [-1, 15, 16]             272\n",
      "           Linear-19               [-1, 15, 16]             272\n",
      "          Dropout-20            [-1, 2, 15, 15]               0\n",
      "           Linear-21               [-1, 15, 16]             272\n",
      "MultiHeadAttention-22               [-1, 15, 16]               0\n",
      "          Dropout-23               [-1, 15, 16]               0\n",
      "        LayerNorm-24               [-1, 15, 16]              32\n",
      "      ResidualAdd-25               [-1, 15, 16]               0\n",
      "           Linear-26                [-1, 15, 4]              68\n",
      "       SwitchGate-27                [-1, 15, 4]               0\n",
      "           Linear-28               [-1, 15, 64]           1,088\n",
      "             GELU-29               [-1, 15, 64]               0\n",
      "           Linear-30               [-1, 15, 32]           2,080\n",
      "           Linear-31               [-1, 15, 64]           1,088\n",
      "             GELU-32               [-1, 15, 64]               0\n",
      "           Linear-33               [-1, 15, 32]           2,080\n",
      "           Linear-34               [-1, 15, 64]           1,088\n",
      "             GELU-35               [-1, 15, 64]               0\n",
      "           Linear-36               [-1, 15, 32]           2,080\n",
      "           Linear-37               [-1, 15, 64]           1,088\n",
      "             GELU-38               [-1, 15, 64]               0\n",
      "           Linear-39               [-1, 15, 32]           2,080\n",
      "           Linear-40               [-1, 15, 16]             528\n",
      "        SwitchMoE-41               [-1, 15, 16]               0\n",
      "           Linear-42               [-1, 15, 64]           1,088\n",
      "             GELU-43               [-1, 15, 64]               0\n",
      "          Dropout-44               [-1, 15, 64]               0\n",
      "           Linear-45               [-1, 15, 16]           1,040\n",
      "          Dropout-46               [-1, 15, 16]               0\n",
      "        LayerNorm-47               [-1, 15, 16]              32\n",
      "      ResidualAdd-48               [-1, 15, 16]               0\n",
      "           Linear-49               [-1, 15, 16]             272\n",
      "           Linear-50               [-1, 15, 16]             272\n",
      "           Linear-51               [-1, 15, 16]             272\n",
      "          Dropout-52            [-1, 2, 15, 15]               0\n",
      "           Linear-53               [-1, 15, 16]             272\n",
      "MultiHeadAttention-54               [-1, 15, 16]               0\n",
      "          Dropout-55               [-1, 15, 16]               0\n",
      "        LayerNorm-56               [-1, 15, 16]              32\n",
      "      ResidualAdd-57               [-1, 15, 16]               0\n",
      "           Linear-58                [-1, 15, 4]              68\n",
      "       SwitchGate-59                [-1, 15, 4]               0\n",
      "           Linear-60               [-1, 15, 64]           1,088\n",
      "             GELU-61               [-1, 15, 64]               0\n",
      "           Linear-62               [-1, 15, 32]           2,080\n",
      "           Linear-63               [-1, 15, 64]           1,088\n",
      "             GELU-64               [-1, 15, 64]               0\n",
      "           Linear-65               [-1, 15, 32]           2,080\n",
      "           Linear-66               [-1, 15, 64]           1,088\n",
      "             GELU-67               [-1, 15, 64]               0\n",
      "           Linear-68               [-1, 15, 32]           2,080\n",
      "           Linear-69               [-1, 15, 64]           1,088\n",
      "             GELU-70               [-1, 15, 64]               0\n",
      "           Linear-71               [-1, 15, 32]           2,080\n",
      "           Linear-72               [-1, 15, 16]             528\n",
      "        SwitchMoE-73               [-1, 15, 16]               0\n",
      "           Linear-74               [-1, 15, 64]           1,088\n",
      "             GELU-75               [-1, 15, 64]               0\n",
      "          Dropout-76               [-1, 15, 64]               0\n",
      "           Linear-77               [-1, 15, 16]           1,040\n",
      "          Dropout-78               [-1, 15, 16]               0\n",
      "        LayerNorm-79               [-1, 15, 16]              32\n",
      "      ResidualAdd-80               [-1, 15, 16]               0\n",
      "           Linear-81               [-1, 15, 16]             272\n",
      "           Linear-82               [-1, 15, 16]             272\n",
      "           Linear-83               [-1, 15, 16]             272\n",
      "          Dropout-84            [-1, 2, 15, 15]               0\n",
      "           Linear-85               [-1, 15, 16]             272\n",
      "MultiHeadAttention-86               [-1, 15, 16]               0\n",
      "          Dropout-87               [-1, 15, 16]               0\n",
      "        LayerNorm-88               [-1, 15, 16]              32\n",
      "      ResidualAdd-89               [-1, 15, 16]               0\n",
      "           Linear-90                [-1, 15, 4]              68\n",
      "       SwitchGate-91                [-1, 15, 4]               0\n",
      "           Linear-92               [-1, 15, 64]           1,088\n",
      "             GELU-93               [-1, 15, 64]               0\n",
      "           Linear-94               [-1, 15, 32]           2,080\n",
      "           Linear-95               [-1, 15, 64]           1,088\n",
      "             GELU-96               [-1, 15, 64]               0\n",
      "           Linear-97               [-1, 15, 32]           2,080\n",
      "           Linear-98               [-1, 15, 64]           1,088\n",
      "             GELU-99               [-1, 15, 64]               0\n",
      "          Linear-100               [-1, 15, 32]           2,080\n",
      "          Linear-101               [-1, 15, 64]           1,088\n",
      "            GELU-102               [-1, 15, 64]               0\n",
      "          Linear-103               [-1, 15, 32]           2,080\n",
      "          Linear-104               [-1, 15, 16]             528\n",
      "       SwitchMoE-105               [-1, 15, 16]               0\n",
      "          Linear-106               [-1, 15, 64]           1,088\n",
      "            GELU-107               [-1, 15, 64]               0\n",
      "         Dropout-108               [-1, 15, 64]               0\n",
      "          Linear-109               [-1, 15, 16]           1,040\n",
      "         Dropout-110               [-1, 15, 16]               0\n",
      "       LayerNorm-111               [-1, 15, 16]              32\n",
      "     ResidualAdd-112               [-1, 15, 16]               0\n",
      "          Linear-113               [-1, 15, 16]             272\n",
      "          Linear-114               [-1, 15, 16]             272\n",
      "          Linear-115               [-1, 15, 16]             272\n",
      "         Dropout-116            [-1, 2, 15, 15]               0\n",
      "          Linear-117               [-1, 15, 16]             272\n",
      "MultiHeadAttention-118               [-1, 15, 16]               0\n",
      "         Dropout-119               [-1, 15, 16]               0\n",
      "       LayerNorm-120               [-1, 15, 16]              32\n",
      "     ResidualAdd-121               [-1, 15, 16]               0\n",
      "          Linear-122                [-1, 15, 4]              68\n",
      "      SwitchGate-123                [-1, 15, 4]               0\n",
      "          Linear-124               [-1, 15, 64]           1,088\n",
      "            GELU-125               [-1, 15, 64]               0\n",
      "          Linear-126               [-1, 15, 32]           2,080\n",
      "          Linear-127               [-1, 15, 64]           1,088\n",
      "            GELU-128               [-1, 15, 64]               0\n",
      "          Linear-129               [-1, 15, 32]           2,080\n",
      "          Linear-130               [-1, 15, 64]           1,088\n",
      "            GELU-131               [-1, 15, 64]               0\n",
      "          Linear-132               [-1, 15, 32]           2,080\n",
      "          Linear-133               [-1, 15, 64]           1,088\n",
      "            GELU-134               [-1, 15, 64]               0\n",
      "          Linear-135               [-1, 15, 32]           2,080\n",
      "          Linear-136               [-1, 15, 16]             528\n",
      "       SwitchMoE-137               [-1, 15, 16]               0\n",
      "          Linear-138               [-1, 15, 64]           1,088\n",
      "            GELU-139               [-1, 15, 64]               0\n",
      "         Dropout-140               [-1, 15, 64]               0\n",
      "          Linear-141               [-1, 15, 16]           1,040\n",
      "         Dropout-142               [-1, 15, 16]               0\n",
      "       LayerNorm-143               [-1, 15, 16]              32\n",
      "     ResidualAdd-144               [-1, 15, 16]               0\n",
      "          Linear-145               [-1, 15, 16]             272\n",
      "          Linear-146               [-1, 15, 16]             272\n",
      "          Linear-147               [-1, 15, 16]             272\n",
      "         Dropout-148            [-1, 2, 15, 15]               0\n",
      "          Linear-149               [-1, 15, 16]             272\n",
      "MultiHeadAttention-150               [-1, 15, 16]               0\n",
      "         Dropout-151               [-1, 15, 16]               0\n",
      "       LayerNorm-152               [-1, 15, 16]              32\n",
      "     ResidualAdd-153               [-1, 15, 16]               0\n",
      "          Linear-154                [-1, 15, 4]              68\n",
      "      SwitchGate-155                [-1, 15, 4]               0\n",
      "          Linear-156               [-1, 15, 64]           1,088\n",
      "            GELU-157               [-1, 15, 64]               0\n",
      "          Linear-158               [-1, 15, 32]           2,080\n",
      "          Linear-159               [-1, 15, 64]           1,088\n",
      "            GELU-160               [-1, 15, 64]               0\n",
      "          Linear-161               [-1, 15, 32]           2,080\n",
      "          Linear-162               [-1, 15, 64]           1,088\n",
      "            GELU-163               [-1, 15, 64]               0\n",
      "          Linear-164               [-1, 15, 32]           2,080\n",
      "          Linear-165               [-1, 15, 64]           1,088\n",
      "            GELU-166               [-1, 15, 64]               0\n",
      "          Linear-167               [-1, 15, 32]           2,080\n",
      "          Linear-168               [-1, 15, 16]             528\n",
      "       SwitchMoE-169               [-1, 15, 16]               0\n",
      "          Linear-170               [-1, 15, 64]           1,088\n",
      "            GELU-171               [-1, 15, 64]               0\n",
      "         Dropout-172               [-1, 15, 64]               0\n",
      "          Linear-173               [-1, 15, 16]           1,040\n",
      "         Dropout-174               [-1, 15, 16]               0\n",
      "       LayerNorm-175               [-1, 15, 16]              32\n",
      "     ResidualAdd-176               [-1, 15, 16]               0\n",
      "          Linear-177               [-1, 15, 16]             272\n",
      "          Linear-178               [-1, 15, 16]             272\n",
      "          Linear-179               [-1, 15, 16]             272\n",
      "         Dropout-180            [-1, 2, 15, 15]               0\n",
      "          Linear-181               [-1, 15, 16]             272\n",
      "MultiHeadAttention-182               [-1, 15, 16]               0\n",
      "         Dropout-183               [-1, 15, 16]               0\n",
      "       LayerNorm-184               [-1, 15, 16]              32\n",
      "     ResidualAdd-185               [-1, 15, 16]               0\n",
      "          Linear-186                [-1, 15, 4]              68\n",
      "      SwitchGate-187                [-1, 15, 4]               0\n",
      "          Linear-188               [-1, 15, 64]           1,088\n",
      "            GELU-189               [-1, 15, 64]               0\n",
      "          Linear-190               [-1, 15, 32]           2,080\n",
      "          Linear-191               [-1, 15, 64]           1,088\n",
      "            GELU-192               [-1, 15, 64]               0\n",
      "          Linear-193               [-1, 15, 32]           2,080\n",
      "          Linear-194               [-1, 15, 64]           1,088\n",
      "            GELU-195               [-1, 15, 64]               0\n",
      "          Linear-196               [-1, 15, 32]           2,080\n",
      "          Linear-197               [-1, 15, 64]           1,088\n",
      "            GELU-198               [-1, 15, 64]               0\n",
      "          Linear-199               [-1, 15, 32]           2,080\n",
      "          Linear-200               [-1, 15, 16]             528\n",
      "       SwitchMoE-201               [-1, 15, 16]               0\n",
      "          Linear-202               [-1, 15, 64]           1,088\n",
      "            GELU-203               [-1, 15, 64]               0\n",
      "         Dropout-204               [-1, 15, 64]               0\n",
      "          Linear-205               [-1, 15, 16]           1,040\n",
      "         Dropout-206               [-1, 15, 16]               0\n",
      "       LayerNorm-207               [-1, 15, 16]              32\n",
      "     ResidualAdd-208               [-1, 15, 16]               0\n",
      "         Flatten-209                  [-1, 240]               0\n",
      "         Dropout-210                  [-1, 240]               0\n",
      "          Linear-211                    [-1, 2]             482\n",
      "================================================================\n",
      "Total params: 104,506\n",
      "Trainable params: 104,506\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.57\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 1.98\n",
      "----------------------------------------------------------------\n",
      "Mon Feb 17 16:38:30 2025\n",
      "seed is 332\n",
      "Subject 1\n",
      "(21460, 1, 3, 1000) (21460,) (1740, 1, 3, 1000) (1740,) (720, 1, 3, 1000) (720,)\n",
      "1_0 train_acc: 0.5690 train_loss: 0.001698\tval_acc: 0.716092 val_loss: 0.002243395, acc:0.566667\n",
      "1_1 train_acc: 0.6842 train_loss: 0.001176\tval_acc: 0.738506 val_loss: 0.002131869, acc:0.623611\n",
      "1_2 train_acc: 0.7727 train_loss: 0.000928\tval_acc: 0.798276 val_loss: 0.001818115, acc:0.673611\n",
      "1_3 train_acc: 0.8123 train_loss: 0.000812\tval_acc: 0.806897 val_loss: 0.001751997, acc:0.691667\n",
      "1_4 train_acc: 0.8283 train_loss: 0.000765\tval_acc: 0.801149 val_loss: 0.001724591, acc:0.712500\n",
      "1_5 train_acc: 0.8337 train_loss: 0.000734\tval_acc: 0.800575 val_loss: 0.001683084, acc:0.702778\n",
      "1_6 train_acc: 0.8381 train_loss: 0.000734\tval_acc: 0.805172 val_loss: 0.001676562, acc:0.727778\n",
      "1_7 train_acc: 0.8389 train_loss: 0.000718\tval_acc: 0.813793 val_loss: 0.001624398, acc:0.718056\n",
      "1_11 train_acc: 0.8519 train_loss: 0.000682\tval_acc: 0.812644 val_loss: 0.001612297, acc:0.741667\n",
      "1_13 train_acc: 0.8516 train_loss: 0.000673\tval_acc: 0.820690 val_loss: 0.001582312, acc:0.747222\n",
      "1_14 train_acc: 0.8542 train_loss: 0.000666\tval_acc: 0.816667 val_loss: 0.001549686, acc:0.744444\n",
      "1_16 train_acc: 0.8589 train_loss: 0.000652\tval_acc: 0.818966 val_loss: 0.001523269, acc:0.738889\n",
      "1_21 train_acc: 0.8624 train_loss: 0.000640\tval_acc: 0.820690 val_loss: 0.001494393, acc:0.763889\n",
      "1_25 train_acc: 0.8657 train_loss: 0.000625\tval_acc: 0.823563 val_loss: 0.001472968, acc:0.763889\n",
      "1_59 train_acc: 0.8745 train_loss: 0.000588\tval_acc: 0.828736 val_loss: 0.001472570, acc:0.759722\n",
      "1_64 train_acc: 0.8750 train_loss: 0.000579\tval_acc: 0.827011 val_loss: 0.001467938, acc:0.759722\n",
      "1_66 train_acc: 0.8752 train_loss: 0.000577\tval_acc: 0.827586 val_loss: 0.001459829, acc:0.754167\n",
      "1_71 train_acc: 0.8726 train_loss: 0.000581\tval_acc: 0.825287 val_loss: 0.001455123, acc:0.765278\n",
      "1_72 train_acc: 0.8746 train_loss: 0.000574\tval_acc: 0.827011 val_loss: 0.001443944, acc:0.762500\n",
      "1_77 train_acc: 0.8754 train_loss: 0.000579\tval_acc: 0.833333 val_loss: 0.001441167, acc:0.765278\n",
      "1_101 train_acc: 0.8770 train_loss: 0.000577\tval_acc: 0.826437 val_loss: 0.001421860, acc:0.762500\n",
      "1_104 train_acc: 0.8775 train_loss: 0.000564\tval_acc: 0.829310 val_loss: 0.001421439, acc:0.754167\n",
      "1_118 train_acc: 0.8794 train_loss: 0.000565\tval_acc: 0.822414 val_loss: 0.001415541, acc:0.761111\n",
      "1_131 train_acc: 0.8804 train_loss: 0.000562\tval_acc: 0.822989 val_loss: 0.001408558, acc:0.766667\n",
      "1_144 train_acc: 0.8808 train_loss: 0.000552\tval_acc: 0.828736 val_loss: 0.001407692, acc:0.755556\n",
      "1_156 train_acc: 0.8838 train_loss: 0.000550\tval_acc: 0.829310 val_loss: 0.001399205, acc:0.762500\n",
      "1_170 train_acc: 0.8829 train_loss: 0.000543\tval_acc: 0.827011 val_loss: 0.001395560, acc:0.770833\n",
      "1_184 train_acc: 0.8796 train_loss: 0.000542\tval_acc: 0.828736 val_loss: 0.001381290, acc:0.766667\n",
      "1_186 train_acc: 0.8841 train_loss: 0.000547\tval_acc: 0.832759 val_loss: 0.001371634, acc:0.766667\n",
      "1_222 train_acc: 0.8849 train_loss: 0.000543\tval_acc: 0.829885 val_loss: 0.001345724, acc:0.759722\n",
      "1_303 train_acc: 0.8884 train_loss: 0.000525\tval_acc: 0.839080 val_loss: 0.001338489, acc:0.777778\n",
      "1_372 train_acc: 0.8927 train_loss: 0.000512\tval_acc: 0.834483 val_loss: 0.001334745, acc:0.781944\n",
      "1_426 train_acc: 0.8922 train_loss: 0.000511\tval_acc: 0.837356 val_loss: 0.001334419, acc:0.783333\n",
      "1_454 train_acc: 0.8922 train_loss: 0.000502\tval_acc: 0.838506 val_loss: 0.001330675, acc:0.769444\n",
      "1_471 train_acc: 0.8919 train_loss: 0.000515\tval_acc: 0.837356 val_loss: 0.001314018, acc:0.791667\n",
      "epoch:  471 \tThe test accuracy is: 0.7916666666666666\n",
      " THE BEST ACCURACY IS 0.7916666666666666\tkappa is 0.5833333333333333\n",
      "subject 1 duration: 0:24:48.407371\n",
      "seed is 59\n",
      "Subject 2\n",
      "(21608, 1, 3, 1000) (21608,) (1752, 1, 3, 1000) (1752,) (680, 1, 3, 1000) (680,)\n",
      "2_0 train_acc: 0.6039 train_loss: 0.001691\tval_acc: 0.717466 val_loss: 0.002245709, acc:0.630882\n",
      "2_1 train_acc: 0.7282 train_loss: 0.001094\tval_acc: 0.762557 val_loss: 0.002024317, acc:0.644118\n",
      "2_2 train_acc: 0.7883 train_loss: 0.000902\tval_acc: 0.793950 val_loss: 0.001843489, acc:0.685294\n",
      "2_5 train_acc: 0.8352 train_loss: 0.000755\tval_acc: 0.800228 val_loss: 0.001808844, acc:0.697059\n",
      "2_7 train_acc: 0.8438 train_loss: 0.000723\tval_acc: 0.796233 val_loss: 0.001717934, acc:0.691176\n",
      "2_10 train_acc: 0.8509 train_loss: 0.000687\tval_acc: 0.805365 val_loss: 0.001604371, acc:0.711765\n",
      "2_13 train_acc: 0.8590 train_loss: 0.000665\tval_acc: 0.801941 val_loss: 0.001600952, acc:0.722059\n",
      "2_17 train_acc: 0.8641 train_loss: 0.000647\tval_acc: 0.810502 val_loss: 0.001577567, acc:0.705882\n",
      "2_20 train_acc: 0.8668 train_loss: 0.000628\tval_acc: 0.815639 val_loss: 0.001528672, acc:0.713235\n",
      "2_24 train_acc: 0.8694 train_loss: 0.000617\tval_acc: 0.817352 val_loss: 0.001517874, acc:0.705882\n",
      "2_30 train_acc: 0.8747 train_loss: 0.000596\tval_acc: 0.819064 val_loss: 0.001478564, acc:0.685294\n",
      "2_35 train_acc: 0.8713 train_loss: 0.000596\tval_acc: 0.828196 val_loss: 0.001458692, acc:0.701471\n",
      "2_45 train_acc: 0.8766 train_loss: 0.000576\tval_acc: 0.827626 val_loss: 0.001439920, acc:0.719118\n",
      "2_63 train_acc: 0.8814 train_loss: 0.000551\tval_acc: 0.834475 val_loss: 0.001427872, acc:0.708824\n",
      "2_68 train_acc: 0.8840 train_loss: 0.000555\tval_acc: 0.836758 val_loss: 0.001410001, acc:0.700000\n",
      "2_83 train_acc: 0.8878 train_loss: 0.000538\tval_acc: 0.840183 val_loss: 0.001406844, acc:0.713235\n",
      "2_86 train_acc: 0.8864 train_loss: 0.000537\tval_acc: 0.835046 val_loss: 0.001371682, acc:0.697059\n",
      "2_230 train_acc: 0.8920 train_loss: 0.000526\tval_acc: 0.844178 val_loss: 0.001368230, acc:0.704412\n",
      "2_274 train_acc: 0.8939 train_loss: 0.000516\tval_acc: 0.847603 val_loss: 0.001359561, acc:0.705882\n",
      "2_276 train_acc: 0.8916 train_loss: 0.000522\tval_acc: 0.849886 val_loss: 0.001359273, acc:0.713235\n",
      "2_411 train_acc: 0.9045 train_loss: 0.000466\tval_acc: 0.840753 val_loss: 0.001358334, acc:0.713235\n",
      "2_425 train_acc: 0.9047 train_loss: 0.000469\tval_acc: 0.847603 val_loss: 0.001349743, acc:0.701471\n",
      "2_481 train_acc: 0.9067 train_loss: 0.000465\tval_acc: 0.844178 val_loss: 0.001334114, acc:0.735294\n",
      "2_496 train_acc: 0.9069 train_loss: 0.000467\tval_acc: 0.841324 val_loss: 0.001328667, acc:0.716176\n",
      "2_499 train_acc: 0.9077 train_loss: 0.000456\tval_acc: 0.840753 val_loss: 0.001301243, acc:0.722059\n",
      "2_534 train_acc: 0.9092 train_loss: 0.000457\tval_acc: 0.841324 val_loss: 0.001286245, acc:0.711765\n",
      "2_545 train_acc: 0.9082 train_loss: 0.000455\tval_acc: 0.839612 val_loss: 0.001268600, acc:0.745588\n",
      "2_574 train_acc: 0.9114 train_loss: 0.000443\tval_acc: 0.857306 val_loss: 0.001220366, acc:0.713235\n",
      "2_575 train_acc: 0.9075 train_loss: 0.000450\tval_acc: 0.853311 val_loss: 0.001220216, acc:0.727941\n",
      "2_576 train_acc: 0.9074 train_loss: 0.000456\tval_acc: 0.853881 val_loss: 0.001197741, acc:0.714706\n",
      "epoch:  576 \tThe test accuracy is: 0.7147058823529412\n",
      " THE BEST ACCURACY IS 0.7147058823529412\tkappa is 0.4294117647058824\n",
      "subject 2 duration: 0:25:11.352009\n",
      "seed is 1161\n",
      "Subject 3\n",
      "(21460, 1, 3, 1000) (21460,) (1740, 1, 3, 1000) (1740,) (720, 1, 3, 1000) (720,)\n",
      "3_0 train_acc: 0.6140 train_loss: 0.001532\tval_acc: 0.697126 val_loss: 0.002210985, acc:0.612500\n",
      "3_1 train_acc: 0.7568 train_loss: 0.000992\tval_acc: 0.750575 val_loss: 0.001977064, acc:0.593056\n",
      "3_2 train_acc: 0.8066 train_loss: 0.000832\tval_acc: 0.794253 val_loss: 0.001791377, acc:0.594444\n",
      "3_3 train_acc: 0.8232 train_loss: 0.000776\tval_acc: 0.806897 val_loss: 0.001741536, acc:0.605556\n",
      "3_4 train_acc: 0.8286 train_loss: 0.000751\tval_acc: 0.815517 val_loss: 0.001634793, acc:0.630556\n",
      "3_6 train_acc: 0.8405 train_loss: 0.000715\tval_acc: 0.817816 val_loss: 0.001562967, acc:0.625000\n",
      "3_9 train_acc: 0.8543 train_loss: 0.000666\tval_acc: 0.829310 val_loss: 0.001550066, acc:0.616667\n",
      "3_10 train_acc: 0.8583 train_loss: 0.000655\tval_acc: 0.832759 val_loss: 0.001493660, acc:0.625000\n",
      "3_15 train_acc: 0.8650 train_loss: 0.000621\tval_acc: 0.833908 val_loss: 0.001488699, acc:0.630556\n",
      "3_16 train_acc: 0.8651 train_loss: 0.000626\tval_acc: 0.829310 val_loss: 0.001453416, acc:0.634722\n",
      "3_21 train_acc: 0.8705 train_loss: 0.000598\tval_acc: 0.839080 val_loss: 0.001436746, acc:0.640278\n",
      "3_25 train_acc: 0.8722 train_loss: 0.000594\tval_acc: 0.839080 val_loss: 0.001422186, acc:0.641667\n",
      "3_28 train_acc: 0.8750 train_loss: 0.000579\tval_acc: 0.841379 val_loss: 0.001417855, acc:0.647222\n",
      "3_36 train_acc: 0.8780 train_loss: 0.000563\tval_acc: 0.841379 val_loss: 0.001403738, acc:0.640278\n",
      "3_42 train_acc: 0.8780 train_loss: 0.000563\tval_acc: 0.836782 val_loss: 0.001400772, acc:0.663889\n",
      "3_60 train_acc: 0.8820 train_loss: 0.000543\tval_acc: 0.843678 val_loss: 0.001395000, acc:0.663889\n",
      "3_70 train_acc: 0.8822 train_loss: 0.000546\tval_acc: 0.839080 val_loss: 0.001376239, acc:0.663889\n",
      "3_72 train_acc: 0.8838 train_loss: 0.000541\tval_acc: 0.840230 val_loss: 0.001369994, acc:0.668056\n",
      "3_75 train_acc: 0.8866 train_loss: 0.000538\tval_acc: 0.849425 val_loss: 0.001362261, acc:0.662500\n",
      "3_81 train_acc: 0.8854 train_loss: 0.000540\tval_acc: 0.839655 val_loss: 0.001359497, acc:0.656944\n",
      "3_88 train_acc: 0.8851 train_loss: 0.000537\tval_acc: 0.847701 val_loss: 0.001355116, acc:0.670833\n",
      "3_96 train_acc: 0.8871 train_loss: 0.000529\tval_acc: 0.845402 val_loss: 0.001352829, acc:0.661111\n",
      "3_108 train_acc: 0.8865 train_loss: 0.000528\tval_acc: 0.845402 val_loss: 0.001347873, acc:0.656944\n",
      "3_110 train_acc: 0.8878 train_loss: 0.000527\tval_acc: 0.842529 val_loss: 0.001332828, acc:0.672222\n",
      "3_120 train_acc: 0.8909 train_loss: 0.000522\tval_acc: 0.841379 val_loss: 0.001327883, acc:0.662500\n",
      "3_126 train_acc: 0.8918 train_loss: 0.000512\tval_acc: 0.844828 val_loss: 0.001324995, acc:0.670833\n",
      "3_133 train_acc: 0.8907 train_loss: 0.000515\tval_acc: 0.842529 val_loss: 0.001320761, acc:0.666667\n",
      "3_150 train_acc: 0.8925 train_loss: 0.000511\tval_acc: 0.849425 val_loss: 0.001304698, acc:0.659722\n",
      "3_161 train_acc: 0.8926 train_loss: 0.000506\tval_acc: 0.844253 val_loss: 0.001304155, acc:0.669444\n",
      "3_162 train_acc: 0.8928 train_loss: 0.000505\tval_acc: 0.845977 val_loss: 0.001300659, acc:0.651389\n",
      "3_188 train_acc: 0.8907 train_loss: 0.000510\tval_acc: 0.851724 val_loss: 0.001293465, acc:0.666667\n",
      "3_205 train_acc: 0.8975 train_loss: 0.000494\tval_acc: 0.852299 val_loss: 0.001287190, acc:0.663889\n",
      "3_212 train_acc: 0.8965 train_loss: 0.000487\tval_acc: 0.847701 val_loss: 0.001279565, acc:0.656944\n",
      "3_213 train_acc: 0.8966 train_loss: 0.000497\tval_acc: 0.856322 val_loss: 0.001277575, acc:0.665278\n",
      "3_217 train_acc: 0.8920 train_loss: 0.000500\tval_acc: 0.852299 val_loss: 0.001258880, acc:0.659722\n",
      "3_306 train_acc: 0.8969 train_loss: 0.000486\tval_acc: 0.858621 val_loss: 0.001253366, acc:0.669444\n",
      "3_388 train_acc: 0.9021 train_loss: 0.000472\tval_acc: 0.854023 val_loss: 0.001250343, acc:0.665278\n",
      "3_394 train_acc: 0.9017 train_loss: 0.000467\tval_acc: 0.856897 val_loss: 0.001249798, acc:0.659722\n",
      "3_416 train_acc: 0.9027 train_loss: 0.000465\tval_acc: 0.860345 val_loss: 0.001247485, acc:0.663889\n",
      "3_417 train_acc: 0.9045 train_loss: 0.000457\tval_acc: 0.854598 val_loss: 0.001244002, acc:0.663889\n",
      "3_478 train_acc: 0.9040 train_loss: 0.000462\tval_acc: 0.860345 val_loss: 0.001240396, acc:0.652778\n",
      "3_513 train_acc: 0.9042 train_loss: 0.000463\tval_acc: 0.864943 val_loss: 0.001233129, acc:0.665278\n",
      "epoch:  513 \tThe test accuracy is: 0.6652777777777777\n",
      " THE BEST ACCURACY IS 0.6652777777777777\tkappa is 0.3305555555555556\n",
      "subject 3 duration: 0:24:50.001847\n",
      "seed is 594\n",
      "Subject 4\n",
      "(21386, 1, 3, 1000) (21386,) (1734, 1, 3, 1000) (1734,) (740, 1, 3, 1000) (740,)\n",
      "4_0 train_acc: 0.6115 train_loss: 0.001596\tval_acc: 0.709919 val_loss: 0.002257020, acc:0.608108\n",
      "4_1 train_acc: 0.6927 train_loss: 0.001155\tval_acc: 0.724337 val_loss: 0.002136895, acc:0.720270\n",
      "4_2 train_acc: 0.7342 train_loss: 0.001036\tval_acc: 0.754902 val_loss: 0.002042256, acc:0.812162\n",
      "4_3 train_acc: 0.7603 train_loss: 0.000962\tval_acc: 0.772780 val_loss: 0.001920767, acc:0.816216\n",
      "4_4 train_acc: 0.7771 train_loss: 0.000913\tval_acc: 0.782007 val_loss: 0.001916159, acc:0.785135\n",
      "4_7 train_acc: 0.8079 train_loss: 0.000814\tval_acc: 0.788351 val_loss: 0.001874160, acc:0.813514\n",
      "4_8 train_acc: 0.8094 train_loss: 0.000812\tval_acc: 0.793541 val_loss: 0.001841574, acc:0.789189\n",
      "4_9 train_acc: 0.8186 train_loss: 0.000785\tval_acc: 0.793541 val_loss: 0.001825133, acc:0.790541\n",
      "4_10 train_acc: 0.8212 train_loss: 0.000774\tval_acc: 0.795848 val_loss: 0.001803877, acc:0.798649\n",
      "4_12 train_acc: 0.8269 train_loss: 0.000757\tval_acc: 0.794694 val_loss: 0.001783927, acc:0.785135\n",
      "4_15 train_acc: 0.8291 train_loss: 0.000754\tval_acc: 0.797578 val_loss: 0.001751652, acc:0.789189\n",
      "4_17 train_acc: 0.8368 train_loss: 0.000727\tval_acc: 0.802768 val_loss: 0.001721418, acc:0.790541\n",
      "4_22 train_acc: 0.8396 train_loss: 0.000710\tval_acc: 0.803345 val_loss: 0.001642830, acc:0.801351\n",
      "4_31 train_acc: 0.8442 train_loss: 0.000696\tval_acc: 0.808535 val_loss: 0.001623560, acc:0.802703\n",
      "4_35 train_acc: 0.8474 train_loss: 0.000684\tval_acc: 0.806228 val_loss: 0.001620955, acc:0.795946\n",
      "4_40 train_acc: 0.8457 train_loss: 0.000681\tval_acc: 0.805652 val_loss: 0.001617555, acc:0.810811\n",
      "4_43 train_acc: 0.8508 train_loss: 0.000670\tval_acc: 0.809112 val_loss: 0.001611310, acc:0.790541\n",
      "4_47 train_acc: 0.8508 train_loss: 0.000664\tval_acc: 0.814879 val_loss: 0.001562398, acc:0.801351\n",
      "4_58 train_acc: 0.8333 train_loss: 0.000736\tval_acc: 0.813725 val_loss: 0.001557587, acc:0.783784\n",
      "4_61 train_acc: 0.8545 train_loss: 0.000656\tval_acc: 0.816609 val_loss: 0.001552046, acc:0.806757\n",
      "4_68 train_acc: 0.8596 train_loss: 0.000639\tval_acc: 0.821799 val_loss: 0.001517566, acc:0.804054\n",
      "4_89 train_acc: 0.8608 train_loss: 0.000638\tval_acc: 0.818916 val_loss: 0.001516560, acc:0.818919\n",
      "4_95 train_acc: 0.8612 train_loss: 0.000636\tval_acc: 0.814879 val_loss: 0.001512013, acc:0.798649\n",
      "4_99 train_acc: 0.8593 train_loss: 0.000636\tval_acc: 0.822953 val_loss: 0.001506016, acc:0.809459\n",
      "4_101 train_acc: 0.8619 train_loss: 0.000634\tval_acc: 0.820069 val_loss: 0.001494373, acc:0.805405\n",
      "4_111 train_acc: 0.8641 train_loss: 0.000628\tval_acc: 0.820069 val_loss: 0.001480391, acc:0.809459\n",
      "4_120 train_acc: 0.8599 train_loss: 0.000643\tval_acc: 0.824683 val_loss: 0.001478759, acc:0.808108\n",
      "4_128 train_acc: 0.8567 train_loss: 0.000648\tval_acc: 0.825260 val_loss: 0.001470523, acc:0.798649\n",
      "4_172 train_acc: 0.8589 train_loss: 0.000638\tval_acc: 0.820646 val_loss: 0.001470206, acc:0.817568\n",
      "4_176 train_acc: 0.8590 train_loss: 0.000637\tval_acc: 0.823529 val_loss: 0.001461919, acc:0.818919\n",
      "4_177 train_acc: 0.8557 train_loss: 0.000641\tval_acc: 0.820646 val_loss: 0.001459213, acc:0.812162\n",
      "4_201 train_acc: 0.8593 train_loss: 0.000630\tval_acc: 0.827566 val_loss: 0.001454542, acc:0.818919\n",
      "4_212 train_acc: 0.8638 train_loss: 0.000623\tval_acc: 0.833333 val_loss: 0.001379996, acc:0.813514\n",
      "4_513 train_acc: 0.8744 train_loss: 0.000580\tval_acc: 0.829873 val_loss: 0.001377503, acc:0.821622\n",
      "epoch:  513 \tThe test accuracy is: 0.8216216216216217\n",
      " THE BEST ACCURACY IS 0.8216216216216217\tkappa is 0.6432432432432432\n",
      "subject 4 duration: 0:24:49.301135\n",
      "seed is 1334\n",
      "Subject 5\n",
      "(21386, 1, 3, 1000) (21386,) (1734, 1, 3, 1000) (1734,) (740, 1, 3, 1000) (740,)\n",
      "5_0 train_acc: 0.6062 train_loss: 0.001549\tval_acc: 0.722607 val_loss: 0.002234811, acc:0.775676\n",
      "5_1 train_acc: 0.7511 train_loss: 0.001008\tval_acc: 0.758362 val_loss: 0.002211418, acc:0.814865\n",
      "5_2 train_acc: 0.7905 train_loss: 0.000887\tval_acc: 0.778547 val_loss: 0.002148458, acc:0.809459\n",
      "5_5 train_acc: 0.8289 train_loss: 0.000756\tval_acc: 0.790657 val_loss: 0.002108365, acc:0.825676\n",
      "5_7 train_acc: 0.8369 train_loss: 0.000724\tval_acc: 0.794118 val_loss: 0.002042992, acc:0.817568\n",
      "5_8 train_acc: 0.8411 train_loss: 0.000718\tval_acc: 0.803922 val_loss: 0.002025379, acc:0.824324\n",
      "5_10 train_acc: 0.8456 train_loss: 0.000695\tval_acc: 0.806805 val_loss: 0.001938611, acc:0.833784\n",
      "5_13 train_acc: 0.8475 train_loss: 0.000682\tval_acc: 0.810842 val_loss: 0.001926063, acc:0.820270\n",
      "5_15 train_acc: 0.8495 train_loss: 0.000671\tval_acc: 0.805652 val_loss: 0.001888101, acc:0.845946\n",
      "5_16 train_acc: 0.8474 train_loss: 0.000677\tval_acc: 0.803922 val_loss: 0.001828381, acc:0.833784\n",
      "5_24 train_acc: 0.8540 train_loss: 0.000657\tval_acc: 0.814879 val_loss: 0.001807350, acc:0.804054\n",
      "5_25 train_acc: 0.8514 train_loss: 0.000660\tval_acc: 0.813149 val_loss: 0.001797441, acc:0.822973\n",
      "5_30 train_acc: 0.8607 train_loss: 0.000645\tval_acc: 0.811995 val_loss: 0.001783402, acc:0.825676\n",
      "5_34 train_acc: 0.8596 train_loss: 0.000641\tval_acc: 0.818339 val_loss: 0.001748704, acc:0.827027\n",
      "5_37 train_acc: 0.8635 train_loss: 0.000629\tval_acc: 0.821799 val_loss: 0.001740796, acc:0.813514\n",
      "5_46 train_acc: 0.8624 train_loss: 0.000628\tval_acc: 0.813725 val_loss: 0.001724609, acc:0.839189\n",
      "5_54 train_acc: 0.8617 train_loss: 0.000627\tval_acc: 0.818916 val_loss: 0.001720461, acc:0.824324\n",
      "5_76 train_acc: 0.8691 train_loss: 0.000606\tval_acc: 0.820069 val_loss: 0.001695834, acc:0.829730\n",
      "5_86 train_acc: 0.8695 train_loss: 0.000602\tval_acc: 0.819493 val_loss: 0.001649668, acc:0.816216\n",
      "5_100 train_acc: 0.8663 train_loss: 0.000605\tval_acc: 0.822376 val_loss: 0.001637809, acc:0.839189\n",
      "5_106 train_acc: 0.8706 train_loss: 0.000595\tval_acc: 0.825836 val_loss: 0.001622782, acc:0.824324\n",
      "5_168 train_acc: 0.8713 train_loss: 0.000587\tval_acc: 0.830450 val_loss: 0.001607801, acc:0.836486\n",
      "5_218 train_acc: 0.8722 train_loss: 0.000589\tval_acc: 0.839100 val_loss: 0.001595808, acc:0.839189\n",
      "5_222 train_acc: 0.8771 train_loss: 0.000575\tval_acc: 0.831027 val_loss: 0.001590646, acc:0.829730\n",
      "5_240 train_acc: 0.8754 train_loss: 0.000576\tval_acc: 0.827566 val_loss: 0.001583025, acc:0.845946\n",
      "5_249 train_acc: 0.8767 train_loss: 0.000568\tval_acc: 0.832757 val_loss: 0.001564941, acc:0.837838\n",
      "5_292 train_acc: 0.8769 train_loss: 0.000565\tval_acc: 0.840254 val_loss: 0.001555777, acc:0.833784\n",
      "5_319 train_acc: 0.8811 train_loss: 0.000558\tval_acc: 0.833333 val_loss: 0.001547520, acc:0.841892\n",
      "5_358 train_acc: 0.8795 train_loss: 0.000550\tval_acc: 0.837947 val_loss: 0.001542640, acc:0.845946\n",
      "5_470 train_acc: 0.8796 train_loss: 0.000547\tval_acc: 0.841984 val_loss: 0.001538544, acc:0.837838\n",
      "5_471 train_acc: 0.8808 train_loss: 0.000546\tval_acc: 0.841407 val_loss: 0.001516872, acc:0.829730\n",
      "5_482 train_acc: 0.8838 train_loss: 0.000539\tval_acc: 0.840254 val_loss: 0.001493000, acc:0.845946\n",
      "epoch:  482 \tThe test accuracy is: 0.845945945945946\n",
      " THE BEST ACCURACY IS 0.845945945945946\tkappa is 0.6918918918918919\n",
      "subject 5 duration: 0:24:47.337160\n",
      "seed is 951\n",
      "Subject 6\n",
      "(21460, 1, 3, 1000) (21460,) (1740, 1, 3, 1000) (1740,) (720, 1, 3, 1000) (720,)\n",
      "6_0 train_acc: 0.6004 train_loss: 0.001596\tval_acc: 0.710345 val_loss: 0.002239985, acc:0.683333\n",
      "6_1 train_acc: 0.7288 train_loss: 0.001058\tval_acc: 0.758621 val_loss: 0.001974821, acc:0.708333\n",
      "6_3 train_acc: 0.8025 train_loss: 0.000845\tval_acc: 0.779310 val_loss: 0.001851330, acc:0.744444\n",
      "6_4 train_acc: 0.8159 train_loss: 0.000805\tval_acc: 0.790805 val_loss: 0.001809495, acc:0.780556\n",
      "6_5 train_acc: 0.8208 train_loss: 0.000782\tval_acc: 0.801149 val_loss: 0.001789702, acc:0.787500\n",
      "6_8 train_acc: 0.8341 train_loss: 0.000726\tval_acc: 0.807471 val_loss: 0.001695790, acc:0.787500\n",
      "6_10 train_acc: 0.8421 train_loss: 0.000707\tval_acc: 0.801149 val_loss: 0.001685592, acc:0.815278\n",
      "6_14 train_acc: 0.8493 train_loss: 0.000690\tval_acc: 0.812069 val_loss: 0.001674557, acc:0.750000\n",
      "6_15 train_acc: 0.8484 train_loss: 0.000679\tval_acc: 0.805747 val_loss: 0.001669355, acc:0.784722\n",
      "6_16 train_acc: 0.8518 train_loss: 0.000669\tval_acc: 0.808621 val_loss: 0.001652024, acc:0.808333\n",
      "6_17 train_acc: 0.8525 train_loss: 0.000663\tval_acc: 0.810920 val_loss: 0.001643943, acc:0.823611\n",
      "6_18 train_acc: 0.8555 train_loss: 0.000651\tval_acc: 0.813218 val_loss: 0.001636543, acc:0.793056\n",
      "6_25 train_acc: 0.8595 train_loss: 0.000635\tval_acc: 0.816092 val_loss: 0.001620094, acc:0.823611\n",
      "6_30 train_acc: 0.8619 train_loss: 0.000624\tval_acc: 0.816092 val_loss: 0.001600935, acc:0.831944\n",
      "6_54 train_acc: 0.8639 train_loss: 0.000617\tval_acc: 0.817816 val_loss: 0.001600348, acc:0.836111\n",
      "6_55 train_acc: 0.8655 train_loss: 0.000612\tval_acc: 0.821839 val_loss: 0.001590248, acc:0.833333\n",
      "6_65 train_acc: 0.8682 train_loss: 0.000608\tval_acc: 0.824713 val_loss: 0.001581832, acc:0.833333\n",
      "6_85 train_acc: 0.8661 train_loss: 0.000611\tval_acc: 0.824713 val_loss: 0.001557004, acc:0.830556\n",
      "6_256 train_acc: 0.8757 train_loss: 0.000576\tval_acc: 0.828736 val_loss: 0.001543583, acc:0.851389\n",
      "6_281 train_acc: 0.8786 train_loss: 0.000556\tval_acc: 0.832759 val_loss: 0.001523712, acc:0.806944\n",
      "6_474 train_acc: 0.8868 train_loss: 0.000539\tval_acc: 0.840805 val_loss: 0.001486539, acc:0.813889\n",
      "6_588 train_acc: 0.8873 train_loss: 0.000524\tval_acc: 0.844828 val_loss: 0.001470395, acc:0.816667\n",
      "6_591 train_acc: 0.8886 train_loss: 0.000525\tval_acc: 0.840230 val_loss: 0.001464781, acc:0.837500\n",
      "epoch:  591 \tThe test accuracy is: 0.8375\n",
      " THE BEST ACCURACY IS 0.8375\tkappa is 0.675\n",
      "subject 6 duration: 0:24:50.853742\n",
      "seed is 344\n",
      "Subject 7\n",
      "(21460, 1, 3, 1000) (21460,) (1740, 1, 3, 1000) (1740,) (720, 1, 3, 1000) (720,)\n",
      "7_0 train_acc: 0.5732 train_loss: 0.001634\tval_acc: 0.677586 val_loss: 0.002325132, acc:0.787500\n",
      "7_1 train_acc: 0.6971 train_loss: 0.001129\tval_acc: 0.740805 val_loss: 0.002038984, acc:0.766667\n",
      "7_2 train_acc: 0.7701 train_loss: 0.000934\tval_acc: 0.768391 val_loss: 0.001942944, acc:0.770833\n",
      "7_3 train_acc: 0.7992 train_loss: 0.000847\tval_acc: 0.762069 val_loss: 0.001881274, acc:0.822222\n",
      "7_4 train_acc: 0.8083 train_loss: 0.000815\tval_acc: 0.777586 val_loss: 0.001833926, acc:0.840278\n",
      "7_5 train_acc: 0.8185 train_loss: 0.000787\tval_acc: 0.785057 val_loss: 0.001820083, acc:0.830556\n",
      "7_6 train_acc: 0.8256 train_loss: 0.000768\tval_acc: 0.782759 val_loss: 0.001801125, acc:0.854167\n",
      "7_7 train_acc: 0.8249 train_loss: 0.000766\tval_acc: 0.790805 val_loss: 0.001793831, acc:0.845833\n",
      "7_9 train_acc: 0.8295 train_loss: 0.000753\tval_acc: 0.789080 val_loss: 0.001772712, acc:0.827778\n",
      "7_13 train_acc: 0.8412 train_loss: 0.000710\tval_acc: 0.801149 val_loss: 0.001700304, acc:0.831944\n",
      "7_15 train_acc: 0.8404 train_loss: 0.000699\tval_acc: 0.811494 val_loss: 0.001684234, acc:0.844444\n",
      "7_16 train_acc: 0.8434 train_loss: 0.000694\tval_acc: 0.814943 val_loss: 0.001675751, acc:0.830556\n",
      "7_18 train_acc: 0.8438 train_loss: 0.000684\tval_acc: 0.804023 val_loss: 0.001656818, acc:0.831944\n",
      "7_22 train_acc: 0.8480 train_loss: 0.000674\tval_acc: 0.814943 val_loss: 0.001646677, acc:0.826389\n",
      "7_23 train_acc: 0.8524 train_loss: 0.000675\tval_acc: 0.801724 val_loss: 0.001645437, acc:0.826389\n",
      "7_26 train_acc: 0.8534 train_loss: 0.000660\tval_acc: 0.807471 val_loss: 0.001624780, acc:0.820833\n",
      "7_28 train_acc: 0.8573 train_loss: 0.000656\tval_acc: 0.815517 val_loss: 0.001618459, acc:0.819444\n",
      "7_29 train_acc: 0.8547 train_loss: 0.000653\tval_acc: 0.816667 val_loss: 0.001616332, acc:0.820833\n",
      "7_31 train_acc: 0.8514 train_loss: 0.000664\tval_acc: 0.816667 val_loss: 0.001577025, acc:0.822222\n",
      "7_36 train_acc: 0.8592 train_loss: 0.000639\tval_acc: 0.808621 val_loss: 0.001566868, acc:0.820833\n",
      "7_38 train_acc: 0.8594 train_loss: 0.000638\tval_acc: 0.813793 val_loss: 0.001561828, acc:0.816667\n",
      "7_43 train_acc: 0.8600 train_loss: 0.000635\tval_acc: 0.819540 val_loss: 0.001525917, acc:0.815278\n",
      "7_62 train_acc: 0.8634 train_loss: 0.000613\tval_acc: 0.825862 val_loss: 0.001511927, acc:0.816667\n",
      "7_70 train_acc: 0.8629 train_loss: 0.000617\tval_acc: 0.827011 val_loss: 0.001494362, acc:0.812500\n",
      "7_80 train_acc: 0.8681 train_loss: 0.000603\tval_acc: 0.824138 val_loss: 0.001463923, acc:0.815278\n",
      "7_117 train_acc: 0.8729 train_loss: 0.000586\tval_acc: 0.828161 val_loss: 0.001455027, acc:0.813889\n",
      "7_129 train_acc: 0.8729 train_loss: 0.000588\tval_acc: 0.829885 val_loss: 0.001454421, acc:0.809722\n",
      "7_131 train_acc: 0.8722 train_loss: 0.000583\tval_acc: 0.827586 val_loss: 0.001446036, acc:0.815278\n",
      "7_135 train_acc: 0.8722 train_loss: 0.000585\tval_acc: 0.833908 val_loss: 0.001430682, acc:0.806944\n",
      "7_166 train_acc: 0.8758 train_loss: 0.000569\tval_acc: 0.836782 val_loss: 0.001430606, acc:0.826389\n",
      "7_174 train_acc: 0.8743 train_loss: 0.000574\tval_acc: 0.831609 val_loss: 0.001419845, acc:0.819444\n",
      "7_197 train_acc: 0.8766 train_loss: 0.000569\tval_acc: 0.836782 val_loss: 0.001414401, acc:0.825000\n",
      "7_222 train_acc: 0.8795 train_loss: 0.000560\tval_acc: 0.832759 val_loss: 0.001411791, acc:0.809722\n",
      "7_233 train_acc: 0.8769 train_loss: 0.000565\tval_acc: 0.834483 val_loss: 0.001411428, acc:0.818056\n",
      "7_237 train_acc: 0.8739 train_loss: 0.000567\tval_acc: 0.835057 val_loss: 0.001406188, acc:0.818056\n",
      "7_296 train_acc: 0.8800 train_loss: 0.000548\tval_acc: 0.837356 val_loss: 0.001399489, acc:0.813889\n",
      "7_299 train_acc: 0.8815 train_loss: 0.000556\tval_acc: 0.839655 val_loss: 0.001394248, acc:0.820833\n",
      "7_326 train_acc: 0.8823 train_loss: 0.000546\tval_acc: 0.840805 val_loss: 0.001389854, acc:0.827778\n",
      "7_348 train_acc: 0.8826 train_loss: 0.000550\tval_acc: 0.844253 val_loss: 0.001386800, acc:0.822222\n",
      "7_351 train_acc: 0.8822 train_loss: 0.000545\tval_acc: 0.846552 val_loss: 0.001384374, acc:0.826389\n",
      "7_368 train_acc: 0.8821 train_loss: 0.000549\tval_acc: 0.843678 val_loss: 0.001383105, acc:0.826389\n",
      "7_413 train_acc: 0.8807 train_loss: 0.000546\tval_acc: 0.842529 val_loss: 0.001381491, acc:0.819444\n",
      "7_445 train_acc: 0.8826 train_loss: 0.000536\tval_acc: 0.841379 val_loss: 0.001375886, acc:0.830556\n",
      "7_590 train_acc: 0.8864 train_loss: 0.000529\tval_acc: 0.837931 val_loss: 0.001372884, acc:0.830556\n",
      "epoch:  590 \tThe test accuracy is: 0.8305555555555556\n",
      " THE BEST ACCURACY IS 0.8305555555555556\tkappa is 0.6611111111111111\n",
      "subject 7 duration: 0:24:56.426442\n",
      "seed is 246\n",
      "Subject 8\n",
      "(21312, 1, 3, 1000) (21312,) (1728, 1, 3, 1000) (1728,) (760, 1, 3, 1000) (760,)\n",
      "8_0 train_acc: 0.5789 train_loss: 0.001742\tval_acc: 0.679977 val_loss: 0.002070819, acc:0.788158\n",
      "8_1 train_acc: 0.6760 train_loss: 0.001219\tval_acc: 0.726273 val_loss: 0.001860139, acc:0.810526\n",
      "8_2 train_acc: 0.7406 train_loss: 0.001033\tval_acc: 0.766204 val_loss: 0.001675900, acc:0.784211\n",
      "8_3 train_acc: 0.7868 train_loss: 0.000894\tval_acc: 0.767940 val_loss: 0.001639886, acc:0.764474\n",
      "8_4 train_acc: 0.8018 train_loss: 0.000845\tval_acc: 0.773727 val_loss: 0.001606775, acc:0.763158\n",
      "8_5 train_acc: 0.8096 train_loss: 0.000815\tval_acc: 0.772569 val_loss: 0.001591519, acc:0.768421\n",
      "8_7 train_acc: 0.8189 train_loss: 0.000789\tval_acc: 0.778935 val_loss: 0.001564329, acc:0.753947\n",
      "8_11 train_acc: 0.8345 train_loss: 0.000743\tval_acc: 0.791088 val_loss: 0.001525656, acc:0.751316\n",
      "8_38 train_acc: 0.8576 train_loss: 0.000645\tval_acc: 0.803241 val_loss: 0.001522208, acc:0.736842\n",
      "8_47 train_acc: 0.8666 train_loss: 0.000619\tval_acc: 0.797454 val_loss: 0.001479628, acc:0.752632\n",
      "8_48 train_acc: 0.8640 train_loss: 0.000624\tval_acc: 0.802662 val_loss: 0.001466970, acc:0.747368\n",
      "8_55 train_acc: 0.8636 train_loss: 0.000620\tval_acc: 0.809606 val_loss: 0.001458665, acc:0.756579\n",
      "8_64 train_acc: 0.8612 train_loss: 0.000624\tval_acc: 0.805556 val_loss: 0.001422325, acc:0.751316\n",
      "8_149 train_acc: 0.8676 train_loss: 0.000606\tval_acc: 0.814815 val_loss: 0.001419975, acc:0.752632\n",
      "8_166 train_acc: 0.8737 train_loss: 0.000585\tval_acc: 0.810185 val_loss: 0.001419263, acc:0.753947\n",
      "8_168 train_acc: 0.8719 train_loss: 0.000587\tval_acc: 0.818287 val_loss: 0.001419119, acc:0.760526\n",
      "8_176 train_acc: 0.8746 train_loss: 0.000582\tval_acc: 0.815972 val_loss: 0.001406355, acc:0.760526\n",
      "8_181 train_acc: 0.8752 train_loss: 0.000577\tval_acc: 0.814815 val_loss: 0.001400694, acc:0.760526\n",
      "8_188 train_acc: 0.8722 train_loss: 0.000583\tval_acc: 0.818287 val_loss: 0.001367660, acc:0.760526\n",
      "8_253 train_acc: 0.8803 train_loss: 0.000565\tval_acc: 0.817708 val_loss: 0.001354216, acc:0.753947\n",
      "8_256 train_acc: 0.8769 train_loss: 0.000570\tval_acc: 0.824074 val_loss: 0.001346289, acc:0.760526\n",
      "8_392 train_acc: 0.8839 train_loss: 0.000544\tval_acc: 0.824074 val_loss: 0.001346257, acc:0.752632\n",
      "8_423 train_acc: 0.8838 train_loss: 0.000552\tval_acc: 0.830440 val_loss: 0.001344878, acc:0.757895\n",
      "8_437 train_acc: 0.8819 train_loss: 0.000549\tval_acc: 0.824653 val_loss: 0.001335430, acc:0.759211\n",
      "8_456 train_acc: 0.8839 train_loss: 0.000536\tval_acc: 0.822917 val_loss: 0.001334497, acc:0.757895\n",
      "8_471 train_acc: 0.8818 train_loss: 0.000547\tval_acc: 0.821759 val_loss: 0.001334174, acc:0.757895\n",
      "8_484 train_acc: 0.8825 train_loss: 0.000543\tval_acc: 0.823495 val_loss: 0.001332556, acc:0.765789\n",
      "8_487 train_acc: 0.8833 train_loss: 0.000551\tval_acc: 0.821759 val_loss: 0.001332058, acc:0.757895\n",
      "8_488 train_acc: 0.8872 train_loss: 0.000536\tval_acc: 0.822338 val_loss: 0.001322639, acc:0.765789\n",
      "8_573 train_acc: 0.8865 train_loss: 0.000537\tval_acc: 0.825231 val_loss: 0.001314804, acc:0.760526\n",
      "epoch:  573 \tThe test accuracy is: 0.7605263157894737\n",
      " THE BEST ACCURACY IS 0.7605263157894737\tkappa is 0.5210526315789474\n",
      "subject 8 duration: 0:24:57.931429\n",
      "seed is 810\n",
      "Subject 9\n",
      "(21460, 1, 3, 1000) (21460,) (1740, 1, 3, 1000) (1740,) (720, 1, 3, 1000) (720,)\n",
      "9_0 train_acc: 0.6217 train_loss: 0.001492\tval_acc: 0.736207 val_loss: 0.002114273, acc:0.548611\n",
      "9_2 train_acc: 0.8033 train_loss: 0.000849\tval_acc: 0.786207 val_loss: 0.001947446, acc:0.740278\n",
      "9_4 train_acc: 0.8320 train_loss: 0.000757\tval_acc: 0.793678 val_loss: 0.001775774, acc:0.747222\n",
      "9_9 train_acc: 0.8479 train_loss: 0.000693\tval_acc: 0.793103 val_loss: 0.001773090, acc:0.759722\n",
      "9_13 train_acc: 0.8519 train_loss: 0.000679\tval_acc: 0.806322 val_loss: 0.001716935, acc:0.777778\n",
      "9_14 train_acc: 0.8504 train_loss: 0.000679\tval_acc: 0.805172 val_loss: 0.001683221, acc:0.775000\n",
      "9_20 train_acc: 0.8538 train_loss: 0.000660\tval_acc: 0.806897 val_loss: 0.001680786, acc:0.773611\n",
      "9_28 train_acc: 0.8519 train_loss: 0.000662\tval_acc: 0.797126 val_loss: 0.001676758, acc:0.777778\n",
      "9_29 train_acc: 0.8535 train_loss: 0.000663\tval_acc: 0.805172 val_loss: 0.001659871, acc:0.770833\n",
      "9_30 train_acc: 0.8520 train_loss: 0.000664\tval_acc: 0.800575 val_loss: 0.001618807, acc:0.775000\n",
      "9_34 train_acc: 0.8562 train_loss: 0.000657\tval_acc: 0.811494 val_loss: 0.001611784, acc:0.775000\n",
      "9_72 train_acc: 0.8613 train_loss: 0.000625\tval_acc: 0.807471 val_loss: 0.001610613, acc:0.794444\n",
      "9_77 train_acc: 0.8608 train_loss: 0.000636\tval_acc: 0.809770 val_loss: 0.001602560, acc:0.779167\n",
      "9_78 train_acc: 0.8615 train_loss: 0.000632\tval_acc: 0.812644 val_loss: 0.001584100, acc:0.780556\n",
      "9_79 train_acc: 0.8637 train_loss: 0.000621\tval_acc: 0.810920 val_loss: 0.001567849, acc:0.786111\n",
      "9_94 train_acc: 0.8665 train_loss: 0.000617\tval_acc: 0.814943 val_loss: 0.001541130, acc:0.793056\n",
      "9_103 train_acc: 0.8675 train_loss: 0.000609\tval_acc: 0.821839 val_loss: 0.001505938, acc:0.797222\n",
      "9_144 train_acc: 0.8724 train_loss: 0.000589\tval_acc: 0.822989 val_loss: 0.001481808, acc:0.791667\n",
      "9_151 train_acc: 0.8700 train_loss: 0.000586\tval_acc: 0.818966 val_loss: 0.001464381, acc:0.783333\n",
      "9_171 train_acc: 0.8721 train_loss: 0.000583\tval_acc: 0.821264 val_loss: 0.001446782, acc:0.786111\n",
      "9_195 train_acc: 0.8764 train_loss: 0.000568\tval_acc: 0.825862 val_loss: 0.001428302, acc:0.781944\n",
      "9_282 train_acc: 0.8803 train_loss: 0.000557\tval_acc: 0.829885 val_loss: 0.001410694, acc:0.791667\n",
      "9_393 train_acc: 0.8870 train_loss: 0.000526\tval_acc: 0.828736 val_loss: 0.001409714, acc:0.805556\n",
      "9_398 train_acc: 0.8877 train_loss: 0.000528\tval_acc: 0.832759 val_loss: 0.001405390, acc:0.802778\n",
      "9_438 train_acc: 0.8897 train_loss: 0.000521\tval_acc: 0.830460 val_loss: 0.001393338, acc:0.801389\n",
      "9_451 train_acc: 0.8889 train_loss: 0.000521\tval_acc: 0.836782 val_loss: 0.001387534, acc:0.795833\n",
      "9_525 train_acc: 0.8890 train_loss: 0.000516\tval_acc: 0.836207 val_loss: 0.001380261, acc:0.798611\n",
      "9_529 train_acc: 0.8897 train_loss: 0.000513\tval_acc: 0.836207 val_loss: 0.001378392, acc:0.798611\n",
      "epoch:  529 \tThe test accuracy is: 0.7986111111111112\n",
      " THE BEST ACCURACY IS 0.7986111111111112\tkappa is 0.5972222222222222\n",
      "subject 9 duration: 0:24:51.724960\n",
      "**The average Best accuracy is: 78.51567640912326kappa is: 57.03135281824652\n",
      "\n",
      "best epochs:  [471, 576, 513, 513, 482, 591, 590, 573, 529]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     79.166667  79.788173  79.166667  79.057429  58.333333\n",
      "1     71.470588  74.764527  71.470588  70.489280  42.941176\n",
      "2     66.527778  67.409114  66.527778  66.098715  33.055556\n",
      "3     82.162162  86.381214  82.162162  81.629567  64.324324\n",
      "4     84.594595  86.668835  84.594595  84.373611  69.189189\n",
      "5     83.750000  84.816248  83.750000  83.624626  67.500000\n",
      "6     83.055556  83.105623  83.055556  83.049147  66.111111\n",
      "7     76.052632  81.666667  76.052632  74.942029  52.105263\n",
      "8     79.861111  80.056151  79.861111  79.828387  59.722222\n",
      "mean  78.515676  80.517395  78.515676  78.121421  57.031353\n",
      "std    6.122727   6.168382   6.122727   6.350769  12.245454\n",
      "****************************************\n",
      "Mon Feb 17 20:22:34 2025\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# The code is based on this github project \n",
    "#    https://github.com/snailpt/CTNet/tree/main\n",
    "########################################################################################\n",
    "\n",
    "import os\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score  \n",
    "from sklearn.metrics import f1_score  \n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # temporal conv kernel size 64=0.25fs\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 22, 1000] \n",
    "            nn.BatchNorm2d(f1),\n",
    "            # channel depth-wise conv\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), # \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # average pooling 1\n",
    "            nn.AvgPool2d((1, pooling_size1)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # spatial conv\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "\n",
    "            # average pooling 2 to adjust the length of feature into transformer encoder\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),  \n",
    "                    \n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        \n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, dim, num_experts: int, capacity_factor: float = 1.0, epsilon: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.w_gate = nn.Linear(dim, num_experts)\n",
    "\n",
    "    def forward(self, x: Tensor, use_aux_loss=False):\n",
    "        gate_scores = F.softmax(self.w_gate(x), dim=-1)\n",
    "        capacity = int(self.capacity_factor * x.size(0))\n",
    "        top_k_scores, top_k_indices = gate_scores.topk(2, dim=-1)\n",
    "        mask = torch.zeros_like(gate_scores).scatter_(1, top_k_indices, 1)\n",
    "        masked_gate_scores = gate_scores * mask\n",
    "        denominators = masked_gate_scores.sum(0, keepdim=True) + self.epsilon\n",
    "        gate_scores = (masked_gate_scores / denominators) * capacity\n",
    "\n",
    "        if use_aux_loss:\n",
    "            load = gate_scores.sum(0)\n",
    "            importance = gate_scores.sum(1)\n",
    "            loss = ((load - importance) ** 2).mean()\n",
    "            return gate_scores, loss\n",
    "\n",
    "        return gate_scores\n",
    "\n",
    "class SwitchMoE(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, output_dim: int, num_experts: int, capacity_factor: float = 1.0, mult: int = 4, use_aux_loss: bool = False, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.mult = mult\n",
    "        self.use_aux_loss = use_aux_loss\n",
    "\n",
    "        # self.experts = nn.ModuleList([FeedForward(dim, hidden_dim, mult, *args, **kwargs) for _ in range(num_experts)])\n",
    "        \n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim*mult),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(dim*mult, hidden_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = SwitchGate(dim, num_experts, capacity_factor)\n",
    "        self.projection = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # gate_scores, loss = self.gate(x, use_aux_loss=self.use_aux_loss)\n",
    "        gate_scores = self.gate(x, use_aux_loss=self.use_aux_loss)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "\n",
    "        if torch.isnan(gate_scores).any():\n",
    "            print(\"NaN in gate scores\")\n",
    "            gate_scores[torch.isnan(gate_scores)] = 0\n",
    "\n",
    "        stacked_expert_outputs = torch.stack(expert_outputs, dim=-1)\n",
    "        if torch.isnan(stacked_expert_outputs).any():\n",
    "            stacked_expert_outputs[torch.isnan(stacked_expert_outputs)] = 0\n",
    "\n",
    "        moe_output = torch.sum(gate_scores.unsqueeze(-2) * stacked_expert_outputs, dim=-1)\n",
    "        moe_output = self.projection(moe_output)\n",
    "        return moe_output\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5, num_experts=4):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "            SwitchMoE(emb_size, emb_size * num_heads, emb_size, num_experts),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                ), emb_size, drop_p)\n",
    "            \n",
    "            )     \n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)      \n",
    "        \n",
    "   \n",
    "    \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 600,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.flatten = nn.Flatten()\n",
    "        # print('self.number_channel', self.number_channel)\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1 = eeg1_f1,\n",
    "                                              kernel_size = eeg1_kernel_size,\n",
    "                                              D = eeg1_D,\n",
    "                                              pooling_size1 = eeg1_pooling_size1,\n",
    "                                              pooling_size2 = eeg1_pooling_size2,\n",
    "                                              dropout_rate = eeg1_dropout_rate,\n",
    "                                              )\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(heads, depth, emb_size)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class) # FLATTEN_EEGNet + FLATTEN_cnn_module\n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "\n",
    "        #  positional embedding\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        \n",
    "        trans = self.trans(cnn)\n",
    "        # residual connect\n",
    "        features = cnn+trans\n",
    "        \n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads=heads\n",
    "        self.emb_size=emb_size\n",
    "        self.depth=depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "             heads=self.heads, \n",
    "             emb_size=self.emb_size,\n",
    "             depth=self.depth, \n",
    "            database_type=self.dataset_type, \n",
    "            eeg1_f1=eeg1_f1, \n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "            eeg1_number_channel = self.number_channel,\n",
    "            flatten_eeg1 = flatten_eeg1,  \n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "        \n",
    "\n",
    "\n",
    "    # Segmentation and Reconstruction (S&R) data augmentation\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        \n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            number_records_by_augmentation = self.number_augmentation * tmp_data.shape[0]\n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append([clsAug + 1]*number_records_by_augmentation)\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        # aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        # aug_data = aug_data[aug_shuffle, :, :]\n",
    "        # aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        # aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        # aug_data = aug_data.float()\n",
    "        # aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        # aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label) \n",
    "        \n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "        # split original allData into training and validate datasets\n",
    "\n",
    "        train_data_list = []\n",
    "        train_label_list = []\n",
    "        validate_data_list = []\n",
    "        validate_label_list = []\n",
    "        for cls in range(self.number_class):\n",
    "            # filter by class \n",
    "            cls_idx = np.where(self.allLabel == cls + 1)\n",
    "            cat_data = self.allData[cls_idx]\n",
    "            cat_label = self.allLabel[cls_idx]\n",
    "\n",
    "            \n",
    "            # each category split\n",
    "            number_sample = cat_data.shape[0]\n",
    "            number_validate = int(self.validate_ratio * number_sample)\n",
    "            # shuffle index\n",
    "            index_shuffle = np.random.permutation(len(cat_data))\n",
    "            index_train = index_shuffle[:-number_validate]\n",
    "            index_validate = index_shuffle[-number_validate:]\n",
    "            \n",
    "            train_data_list.append(cat_data[index_train])\n",
    "            train_label_list.append(cat_label[index_train])\n",
    "            \n",
    "            validate_data_list.append(cat_data[index_validate])\n",
    "            validate_label_list.append(cat_label[index_validate])\n",
    "        \n",
    "        # data augmentation\n",
    "        aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "            \n",
    "        train_data_list.append(aug_data)\n",
    "        train_label_list.append(aug_label)\n",
    "            \n",
    "        self.trainData = np.concatenate(train_data_list)\n",
    "        self.trainLabel = np.concatenate(train_label_list)\n",
    "        self.validateData = np.concatenate(validate_data_list)\n",
    "        self.validateLabel = np.concatenate(validate_label_list)\n",
    "        \n",
    "        # shuffle in all category\n",
    "        shuffle_num = np.random.permutation(len(self.trainData))\n",
    "        self.trainData = self.trainData[shuffle_num, :, :, :]  # (number of training sample, 1, 22, 1000)\n",
    "        self.trainLabel = self.trainLabel[shuffle_num]\n",
    "\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "\n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        # self.allData = (self.allData - target_mean) / target_std\n",
    "        self.trainData = (self.trainData - target_mean) / target_std\n",
    "        self.validateData = (self.validateData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "        print(self.trainData.shape, self.trainLabel.shape, self.validateData.shape, self.validateLabel.shape, self.testData.shape, self.testLabel.shape)\n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.trainData, self.trainLabel, self.validateData, self.validateLabel, self.testData, self.testLabel\n",
    "\n",
    "\n",
    "\n",
    "    def fit_test(self, model, loss_fn, testloader):\n",
    "        y_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_running_loss = 0\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            for x, y in testloader:\n",
    "                x = Variable(x.type(self.Tensor))\n",
    "                y = Variable(y.type(self.LongTensor))\n",
    "                \n",
    "                features, y_pred = model(x)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                y_pred = torch.argmax(y_pred, dim=1)\n",
    "                test_correct += (y_pred == y).sum().item()\n",
    "                test_total += y.size(0)\n",
    "                test_running_loss += loss.item()\n",
    "                y_pred = y_pred.cpu().numpy()\n",
    "                y = y.cpu().numpy()\n",
    "                y_list.extend(y)  \n",
    "                y_pred_list.extend(y_pred)  \n",
    "\n",
    "        acc_score = accuracy_score(y_list, y_pred_list)\n",
    "        epoch_test_loss = test_running_loss / len(testloader.dataset)\n",
    "\n",
    "        return epoch_test_loss, acc_score, y_list, y_pred_list\n",
    "    \n",
    "    \n",
    "    def fit_train(self, model, loss_fn, dataloader, optimizer, trainData, trainLabel):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in dataloader:\n",
    "            # real train dataset\n",
    "            img = Variable(train_data.type(self.Tensor))\n",
    "            label = Variable(train_label.type(self.LongTensor))\n",
    "\n",
    "\n",
    "            # training model\n",
    "            features, y_pred = model(img)\n",
    "            # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "            # print(features.size())\n",
    "            loss = loss_fn(y_pred, label) \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                y_pred = torch.argmax(y_pred, dim=1)\n",
    "                correct += (y_pred == label).sum().item()\n",
    "                total += label.size(0)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "\n",
    "        return epoch_train_loss, epoch_train_acc\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        img, label, validate_data, validate_label, test_data, test_label = self.get_source_data()\n",
    "        # train dataset\n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        # validate dataset\n",
    "        validate_data = torch.from_numpy(validate_data)\n",
    "        validate_label = torch.from_numpy(validate_label - 1)\n",
    "        validate_dataset = torch.utils.data.TensorDataset(validate_data, validate_label)\n",
    "\n",
    "        self.validate_dataloader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=288, shuffle=False)\n",
    "        # test dataset\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=288, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the CTNet model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # train model\n",
    "            self.model.train()\n",
    "            train_loss, train_acc = self.fit_train(self.model, self.criterion_cls, self.dataloader, self.optimizer, self.allData, self.allLabel)\n",
    "            epoch_process['train_acc'] = train_acc\n",
    "            epoch_process['train_loss'] = train_loss\n",
    "            \n",
    "            # validate model\n",
    "            (validate_loss, \n",
    "             validate_acc, \n",
    "             y_list, \n",
    "             y_pred_list) = self.fit_test(self.model, self.criterion_cls, self.validate_dataloader)\n",
    "            epoch_process['val_acc'] = validate_acc                \n",
    "            epoch_process['val_loss'] = validate_loss\n",
    "\n",
    "#             train_pred = torch.max(outputs, 1)[1]\n",
    "#             train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "            num = num + 1\n",
    "\n",
    "            if min_loss>validate_loss:\n",
    "                min_loss = validate_loss\n",
    "                best_epoch = e\n",
    "                epoch_process['epoch'] = e\n",
    "                torch.save(self.model, self.model_filename)\n",
    "\n",
    "                (test_loss, \n",
    "                 test_acc, \n",
    "                 y_list, \n",
    "                 y_pred_list) = self.fit_test(self.model, self.criterion_cls, self.test_dataloader)\n",
    "                epoch_process['test_acc'] = test_acc                \n",
    "                epoch_process['test_loss'] = test_loss\n",
    "                print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.9f}, acc:{:.6f}\".format(self.nSub,\n",
    "                                                                                       epoch_process['epoch'],\n",
    "                                                                                       epoch_process['train_acc'],\n",
    "                                                                                       epoch_process['train_loss'],\n",
    "                                                                                       epoch_process['val_acc'],\n",
    "                                                                                       epoch_process['val_loss'],\n",
    "                                                                                       epoch_process['test_acc']\n",
    "                                                                                    ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename, weights_only=False).cuda()\n",
    "        # test model\n",
    "        (test_loss, \n",
    "         test_acc, \n",
    "         y_list, \n",
    "         y_pred_list) = self.fit_test(self.model, self.criterion_cls, self.test_dataloader)\n",
    "\n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, torch.tensor(y_list), torch.tensor(y_pred_list), df_process, best_epoch\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent', # LOSO or not\n",
    "         heads=8,             # heads of MHA\n",
    "         emb_size=48,         # token embding dim\n",
    "         depth=3,             # Transformer encoder depth\n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         eeg1_f1=20,          # features of temporal conv\n",
    "         eeg1_kernel_size=64, # kernel size of temporal conv\n",
    "         eeg1_D=2,            # depth-wise conv \n",
    "         eeg1_pooling_size1=8,# p1\n",
    "         eeg1_pooling_size2=8,# p2\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio = 0.2,\n",
    "         batch_size = 72\n",
    "         ):\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    \n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "    \n",
    "\n",
    "    for i in range(N_SUBJECT):     \n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus, \n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads, \n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth, \n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1 = eeg1_f1,\n",
    "                  eeg1_kernel_size = eeg1_kernel_size,\n",
    "                  eeg1_D = eeg1_D,\n",
    "                  eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "                  flatten_eeg1 = flatten_eeg1,  \n",
    "                  validate_ratio = validate_ratio,\n",
    "                  batch_size = batch_size \n",
    "                  )\n",
    "\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {'accuray': accuracy*100,\n",
    "                          'precision': precison*100,\n",
    "                          'recall': recall*100,\n",
    "                          'f1': f1*100, \n",
    "                          'kappa': kappa*100\n",
    "                          }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "    \n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "                \n",
    "        df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    N_SUBJECT = 9       # BCI \n",
    "    N_AUG = 3           # data augmentation times for benerating artificial training data set\n",
    "    N_SEG = 8           # segmentation times for S&R\n",
    "\n",
    "    EPOCHS = 600\n",
    "    EMB_DIM = 16\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "    TYPE = 'B'\n",
    "    validate_ratio = 0.3 # split raw train dataset into real train dataset and validate dataset\n",
    "    BATCH_SIZE = 512\n",
    "    EEGNet1_F1 = 8\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "    FLATTEN_EEGNet1 = 240\n",
    "\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25    \n",
    "\n",
    "    parameters_list = [0]\n",
    "    for i in parameters_list:\n",
    "        number_class, number_channel = numberClassChannel(TYPE)\n",
    "        RESULT_NAME = \"Loso_{}_heads_{}_depth_{}_{}\".format(TYPE, HEADS, DEPTH, i)\n",
    "    \n",
    "        sModel = EEGTransformer(\n",
    "            heads=HEADS, \n",
    "            emb_size=EMB_DIM,\n",
    "            depth=DEPTH, \n",
    "            database_type=TYPE,\n",
    "            eeg1_f1=EEGNet1_F1, \n",
    "            eeg1_D=EEGNet1_D,\n",
    "            eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "            eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "            eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "            eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "            eeg1_number_channel = number_channel,\n",
    "            flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "            ).cuda()\n",
    "        summary(sModel, (1, number_channel, 1000)) \n",
    "    \n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        \n",
    "        result = main(RESULT_NAME,\n",
    "                        evaluate_mode = EVALUATE_MODE,\n",
    "                        heads=HEADS, \n",
    "                        emb_size=EMB_DIM,\n",
    "                        depth=DEPTH, \n",
    "                        dataset_type=TYPE,\n",
    "                        eeg1_f1 = EEGNet1_F1,\n",
    "                        eeg1_kernel_size = EEGNet1_KERNEL_SIZE,\n",
    "                        eeg1_D = EEGNet1_D,\n",
    "                        eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                        eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                        eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                        flatten_eeg1 = FLATTEN_EEGNet1,\n",
    "                        validate_ratio = validate_ratio,\n",
    "                        batch_size = BATCH_SIZE\n",
    "                      )\n",
    "        print(time.asctime(time.localtime(time.time())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
