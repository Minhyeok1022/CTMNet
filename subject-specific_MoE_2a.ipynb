{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 22, 1000]             512\n",
      "       BatchNorm2d-2          [-1, 8, 22, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]             352\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "          Dropout-15               [-1, 15, 16]               0\n",
      "PositioinalEncoding-16               [-1, 15, 16]               0\n",
      "           Linear-17               [-1, 15, 16]             272\n",
      "           Linear-18               [-1, 15, 16]             272\n",
      "           Linear-19               [-1, 15, 16]             272\n",
      "          Dropout-20            [-1, 2, 15, 15]               0\n",
      "           Linear-21               [-1, 15, 16]             272\n",
      "MultiHeadAttention-22               [-1, 15, 16]               0\n",
      "          Dropout-23               [-1, 15, 16]               0\n",
      "        LayerNorm-24               [-1, 15, 16]              32\n",
      "      ResidualAdd-25               [-1, 15, 16]               0\n",
      "           Linear-26                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-27                [-1, 15, 4]               0\n",
      "           Linear-28               [-1, 15, 64]           1,088\n",
      "             GELU-29               [-1, 15, 64]               0\n",
      "           Linear-30               [-1, 15, 32]           2,080\n",
      "           Linear-31               [-1, 15, 64]           1,088\n",
      "             GELU-32               [-1, 15, 64]               0\n",
      "           Linear-33               [-1, 15, 32]           2,080\n",
      "           Linear-34               [-1, 15, 64]           1,088\n",
      "             GELU-35               [-1, 15, 64]               0\n",
      "           Linear-36               [-1, 15, 32]           2,080\n",
      "           Linear-37               [-1, 15, 64]           1,088\n",
      "             GELU-38               [-1, 15, 64]               0\n",
      "           Linear-39               [-1, 15, 32]           2,080\n",
      "           Linear-40               [-1, 15, 16]             528\n",
      " MoE_Layerwrapper-41               [-1, 15, 16]               0\n",
      "           Linear-42               [-1, 15, 64]           1,088\n",
      "             GELU-43               [-1, 15, 64]               0\n",
      "          Dropout-44               [-1, 15, 64]               0\n",
      "           Linear-45               [-1, 15, 16]           1,040\n",
      "          Dropout-46               [-1, 15, 16]               0\n",
      "        LayerNorm-47               [-1, 15, 16]              32\n",
      "      ResidualAdd-48               [-1, 15, 16]               0\n",
      "           Linear-49               [-1, 15, 16]             272\n",
      "           Linear-50               [-1, 15, 16]             272\n",
      "           Linear-51               [-1, 15, 16]             272\n",
      "          Dropout-52            [-1, 2, 15, 15]               0\n",
      "           Linear-53               [-1, 15, 16]             272\n",
      "MultiHeadAttention-54               [-1, 15, 16]               0\n",
      "          Dropout-55               [-1, 15, 16]               0\n",
      "        LayerNorm-56               [-1, 15, 16]              32\n",
      "      ResidualAdd-57               [-1, 15, 16]               0\n",
      "           Linear-58                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-59                [-1, 15, 4]               0\n",
      "           Linear-60               [-1, 15, 64]           1,088\n",
      "             GELU-61               [-1, 15, 64]               0\n",
      "           Linear-62               [-1, 15, 32]           2,080\n",
      "           Linear-63               [-1, 15, 64]           1,088\n",
      "             GELU-64               [-1, 15, 64]               0\n",
      "           Linear-65               [-1, 15, 32]           2,080\n",
      "           Linear-66               [-1, 15, 64]           1,088\n",
      "             GELU-67               [-1, 15, 64]               0\n",
      "           Linear-68               [-1, 15, 32]           2,080\n",
      "           Linear-69               [-1, 15, 64]           1,088\n",
      "             GELU-70               [-1, 15, 64]               0\n",
      "           Linear-71               [-1, 15, 32]           2,080\n",
      "           Linear-72               [-1, 15, 16]             528\n",
      " MoE_Layerwrapper-73               [-1, 15, 16]               0\n",
      "           Linear-74               [-1, 15, 64]           1,088\n",
      "             GELU-75               [-1, 15, 64]               0\n",
      "          Dropout-76               [-1, 15, 64]               0\n",
      "           Linear-77               [-1, 15, 16]           1,040\n",
      "          Dropout-78               [-1, 15, 16]               0\n",
      "        LayerNorm-79               [-1, 15, 16]              32\n",
      "      ResidualAdd-80               [-1, 15, 16]               0\n",
      "           Linear-81               [-1, 15, 16]             272\n",
      "           Linear-82               [-1, 15, 16]             272\n",
      "           Linear-83               [-1, 15, 16]             272\n",
      "          Dropout-84            [-1, 2, 15, 15]               0\n",
      "           Linear-85               [-1, 15, 16]             272\n",
      "MultiHeadAttention-86               [-1, 15, 16]               0\n",
      "          Dropout-87               [-1, 15, 16]               0\n",
      "        LayerNorm-88               [-1, 15, 16]              32\n",
      "      ResidualAdd-89               [-1, 15, 16]               0\n",
      "           Linear-90                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-91                [-1, 15, 4]               0\n",
      "           Linear-92               [-1, 15, 64]           1,088\n",
      "             GELU-93               [-1, 15, 64]               0\n",
      "           Linear-94               [-1, 15, 32]           2,080\n",
      "           Linear-95               [-1, 15, 64]           1,088\n",
      "             GELU-96               [-1, 15, 64]               0\n",
      "           Linear-97               [-1, 15, 32]           2,080\n",
      "           Linear-98               [-1, 15, 64]           1,088\n",
      "             GELU-99               [-1, 15, 64]               0\n",
      "          Linear-100               [-1, 15, 32]           2,080\n",
      "          Linear-101               [-1, 15, 64]           1,088\n",
      "            GELU-102               [-1, 15, 64]               0\n",
      "          Linear-103               [-1, 15, 32]           2,080\n",
      "          Linear-104               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-105               [-1, 15, 16]               0\n",
      "          Linear-106               [-1, 15, 64]           1,088\n",
      "            GELU-107               [-1, 15, 64]               0\n",
      "         Dropout-108               [-1, 15, 64]               0\n",
      "          Linear-109               [-1, 15, 16]           1,040\n",
      "         Dropout-110               [-1, 15, 16]               0\n",
      "       LayerNorm-111               [-1, 15, 16]              32\n",
      "     ResidualAdd-112               [-1, 15, 16]               0\n",
      "          Linear-113               [-1, 15, 16]             272\n",
      "          Linear-114               [-1, 15, 16]             272\n",
      "          Linear-115               [-1, 15, 16]             272\n",
      "         Dropout-116            [-1, 2, 15, 15]               0\n",
      "          Linear-117               [-1, 15, 16]             272\n",
      "MultiHeadAttention-118               [-1, 15, 16]               0\n",
      "         Dropout-119               [-1, 15, 16]               0\n",
      "       LayerNorm-120               [-1, 15, 16]              32\n",
      "     ResidualAdd-121               [-1, 15, 16]               0\n",
      "          Linear-122                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-123                [-1, 15, 4]               0\n",
      "          Linear-124               [-1, 15, 64]           1,088\n",
      "            GELU-125               [-1, 15, 64]               0\n",
      "          Linear-126               [-1, 15, 32]           2,080\n",
      "          Linear-127               [-1, 15, 64]           1,088\n",
      "            GELU-128               [-1, 15, 64]               0\n",
      "          Linear-129               [-1, 15, 32]           2,080\n",
      "          Linear-130               [-1, 15, 64]           1,088\n",
      "            GELU-131               [-1, 15, 64]               0\n",
      "          Linear-132               [-1, 15, 32]           2,080\n",
      "          Linear-133               [-1, 15, 64]           1,088\n",
      "            GELU-134               [-1, 15, 64]               0\n",
      "          Linear-135               [-1, 15, 32]           2,080\n",
      "          Linear-136               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-137               [-1, 15, 16]               0\n",
      "          Linear-138               [-1, 15, 64]           1,088\n",
      "            GELU-139               [-1, 15, 64]               0\n",
      "         Dropout-140               [-1, 15, 64]               0\n",
      "          Linear-141               [-1, 15, 16]           1,040\n",
      "         Dropout-142               [-1, 15, 16]               0\n",
      "       LayerNorm-143               [-1, 15, 16]              32\n",
      "     ResidualAdd-144               [-1, 15, 16]               0\n",
      "          Linear-145               [-1, 15, 16]             272\n",
      "          Linear-146               [-1, 15, 16]             272\n",
      "          Linear-147               [-1, 15, 16]             272\n",
      "         Dropout-148            [-1, 2, 15, 15]               0\n",
      "          Linear-149               [-1, 15, 16]             272\n",
      "MultiHeadAttention-150               [-1, 15, 16]               0\n",
      "         Dropout-151               [-1, 15, 16]               0\n",
      "       LayerNorm-152               [-1, 15, 16]              32\n",
      "     ResidualAdd-153               [-1, 15, 16]               0\n",
      "          Linear-154                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-155                [-1, 15, 4]               0\n",
      "          Linear-156               [-1, 15, 64]           1,088\n",
      "            GELU-157               [-1, 15, 64]               0\n",
      "          Linear-158               [-1, 15, 32]           2,080\n",
      "          Linear-159               [-1, 15, 64]           1,088\n",
      "            GELU-160               [-1, 15, 64]               0\n",
      "          Linear-161               [-1, 15, 32]           2,080\n",
      "          Linear-162               [-1, 15, 64]           1,088\n",
      "            GELU-163               [-1, 15, 64]               0\n",
      "          Linear-164               [-1, 15, 32]           2,080\n",
      "          Linear-165               [-1, 15, 64]           1,088\n",
      "            GELU-166               [-1, 15, 64]               0\n",
      "          Linear-167               [-1, 15, 32]           2,080\n",
      "          Linear-168               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-169               [-1, 15, 16]               0\n",
      "          Linear-170               [-1, 15, 64]           1,088\n",
      "            GELU-171               [-1, 15, 64]               0\n",
      "         Dropout-172               [-1, 15, 64]               0\n",
      "          Linear-173               [-1, 15, 16]           1,040\n",
      "         Dropout-174               [-1, 15, 16]               0\n",
      "       LayerNorm-175               [-1, 15, 16]              32\n",
      "     ResidualAdd-176               [-1, 15, 16]               0\n",
      "          Linear-177               [-1, 15, 16]             272\n",
      "          Linear-178               [-1, 15, 16]             272\n",
      "          Linear-179               [-1, 15, 16]             272\n",
      "         Dropout-180            [-1, 2, 15, 15]               0\n",
      "          Linear-181               [-1, 15, 16]             272\n",
      "MultiHeadAttention-182               [-1, 15, 16]               0\n",
      "         Dropout-183               [-1, 15, 16]               0\n",
      "       LayerNorm-184               [-1, 15, 16]              32\n",
      "     ResidualAdd-185               [-1, 15, 16]               0\n",
      "          Linear-186                [-1, 15, 4]              68\n",
      "GatingNetworkwrapper-187                [-1, 15, 4]               0\n",
      "          Linear-188               [-1, 15, 64]           1,088\n",
      "            GELU-189               [-1, 15, 64]               0\n",
      "          Linear-190               [-1, 15, 32]           2,080\n",
      "          Linear-191               [-1, 15, 64]           1,088\n",
      "            GELU-192               [-1, 15, 64]               0\n",
      "          Linear-193               [-1, 15, 32]           2,080\n",
      "          Linear-194               [-1, 15, 64]           1,088\n",
      "            GELU-195               [-1, 15, 64]               0\n",
      "          Linear-196               [-1, 15, 32]           2,080\n",
      "          Linear-197               [-1, 15, 64]           1,088\n",
      "            GELU-198               [-1, 15, 64]               0\n",
      "          Linear-199               [-1, 15, 32]           2,080\n",
      "          Linear-200               [-1, 15, 16]             528\n",
      "MoE_Layerwrapper-201               [-1, 15, 16]               0\n",
      "          Linear-202               [-1, 15, 64]           1,088\n",
      "            GELU-203               [-1, 15, 64]               0\n",
      "         Dropout-204               [-1, 15, 64]               0\n",
      "          Linear-205               [-1, 15, 16]           1,040\n",
      "         Dropout-206               [-1, 15, 16]               0\n",
      "       LayerNorm-207               [-1, 15, 16]              32\n",
      "     ResidualAdd-208               [-1, 15, 16]               0\n",
      "         Flatten-209                  [-1, 240]               0\n",
      "         Dropout-210                  [-1, 240]               0\n",
      "          Linear-211                    [-1, 4]             964\n",
      "================================================================\n",
      "Total params: 105,292\n",
      "Trainable params: 105,292\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 3.89\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 4.38\n",
      "----------------------------------------------------------------\n",
      "Mon Feb 24 11:22:27 2025\n",
      "seed is 1580\n",
      "Subject 1\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "1_0 train_acc: 0.2285 train_loss: 2.387371\tval_acc: 0.254902 val_loss: 1.4106436\n",
      "1_2 train_acc: 0.3408 train_loss: 1.806187\tval_acc: 0.323529 val_loss: 1.3780465\n",
      "1_5 train_acc: 0.2996 train_loss: 1.720644\tval_acc: 0.387255 val_loss: 1.3216670\n",
      "1_6 train_acc: 0.3446 train_loss: 1.506626\tval_acc: 0.460784 val_loss: 1.2376835\n",
      "1_8 train_acc: 0.3820 train_loss: 1.411101\tval_acc: 0.593137 val_loss: 1.1080186\n",
      "1_9 train_acc: 0.4120 train_loss: 1.418453\tval_acc: 0.563725 val_loss: 1.0765667\n",
      "1_10 train_acc: 0.3670 train_loss: 1.436291\tval_acc: 0.563725 val_loss: 1.0488902\n",
      "1_11 train_acc: 0.4195 train_loss: 1.339979\tval_acc: 0.681373 val_loss: 0.9333075\n",
      "1_12 train_acc: 0.4906 train_loss: 1.134151\tval_acc: 0.676471 val_loss: 0.9114804\n",
      "1_13 train_acc: 0.5318 train_loss: 1.152021\tval_acc: 0.730392 val_loss: 0.8001475\n",
      "1_14 train_acc: 0.5019 train_loss: 1.036341\tval_acc: 0.769608 val_loss: 0.7434426\n",
      "1_15 train_acc: 0.5955 train_loss: 0.939143\tval_acc: 0.799020 val_loss: 0.7109174\n",
      "1_16 train_acc: 0.5543 train_loss: 0.944110\tval_acc: 0.779412 val_loss: 0.7027964\n",
      "1_17 train_acc: 0.5730 train_loss: 0.945210\tval_acc: 0.789216 val_loss: 0.6481395\n",
      "1_18 train_acc: 0.5730 train_loss: 0.889209\tval_acc: 0.833333 val_loss: 0.6048524\n",
      "1_19 train_acc: 0.6255 train_loss: 0.838354\tval_acc: 0.838235 val_loss: 0.5885988\n",
      "1_20 train_acc: 0.6255 train_loss: 0.826952\tval_acc: 0.803922 val_loss: 0.5798166\n",
      "1_21 train_acc: 0.6330 train_loss: 0.815827\tval_acc: 0.818627 val_loss: 0.5460728\n",
      "1_22 train_acc: 0.6742 train_loss: 0.727093\tval_acc: 0.803922 val_loss: 0.5425251\n",
      "1_23 train_acc: 0.6854 train_loss: 0.703975\tval_acc: 0.828431 val_loss: 0.5165319\n",
      "1_24 train_acc: 0.6517 train_loss: 0.695750\tval_acc: 0.823529 val_loss: 0.5027455\n",
      "1_28 train_acc: 0.7116 train_loss: 0.716260\tval_acc: 0.813725 val_loss: 0.4991239\n",
      "1_29 train_acc: 0.7191 train_loss: 0.689222\tval_acc: 0.779412 val_loss: 0.4964585\n",
      "1_34 train_acc: 0.7640 train_loss: 0.621268\tval_acc: 0.803922 val_loss: 0.4631725\n",
      "1_37 train_acc: 0.7640 train_loss: 0.595047\tval_acc: 0.789216 val_loss: 0.4596869\n",
      "1_41 train_acc: 0.7603 train_loss: 0.592070\tval_acc: 0.794118 val_loss: 0.4517245\n",
      "1_42 train_acc: 0.7378 train_loss: 0.598023\tval_acc: 0.799020 val_loss: 0.4245481\n",
      "1_46 train_acc: 0.7790 train_loss: 0.530444\tval_acc: 0.803922 val_loss: 0.4023427\n",
      "1_48 train_acc: 0.8240 train_loss: 0.472019\tval_acc: 0.823529 val_loss: 0.3968408\n",
      "1_50 train_acc: 0.7865 train_loss: 0.521968\tval_acc: 0.848039 val_loss: 0.3818381\n",
      "1_52 train_acc: 0.8127 train_loss: 0.456062\tval_acc: 0.833333 val_loss: 0.3628763\n",
      "1_56 train_acc: 0.7790 train_loss: 0.517657\tval_acc: 0.848039 val_loss: 0.3617165\n",
      "1_59 train_acc: 0.7603 train_loss: 0.488455\tval_acc: 0.877451 val_loss: 0.3408663\n",
      "1_64 train_acc: 0.8464 train_loss: 0.383617\tval_acc: 0.867647 val_loss: 0.3230591\n",
      "1_66 train_acc: 0.7453 train_loss: 0.511926\tval_acc: 0.857843 val_loss: 0.3136486\n",
      "1_70 train_acc: 0.7865 train_loss: 0.465571\tval_acc: 0.882353 val_loss: 0.3036615\n",
      "1_73 train_acc: 0.8352 train_loss: 0.380485\tval_acc: 0.882353 val_loss: 0.2723840\n",
      "1_80 train_acc: 0.8202 train_loss: 0.407172\tval_acc: 0.901961 val_loss: 0.2411887\n",
      "1_87 train_acc: 0.8052 train_loss: 0.448126\tval_acc: 0.897059 val_loss: 0.2394954\n",
      "1_88 train_acc: 0.8390 train_loss: 0.424482\tval_acc: 0.921569 val_loss: 0.2307777\n",
      "1_94 train_acc: 0.8652 train_loss: 0.327696\tval_acc: 0.921569 val_loss: 0.2004147\n",
      "1_102 train_acc: 0.8427 train_loss: 0.376928\tval_acc: 0.936275 val_loss: 0.1854720\n",
      "1_121 train_acc: 0.8614 train_loss: 0.300665\tval_acc: 0.936275 val_loss: 0.1796817\n",
      "1_123 train_acc: 0.8352 train_loss: 0.391870\tval_acc: 0.941176 val_loss: 0.1692063\n",
      "1_142 train_acc: 0.8801 train_loss: 0.329327\tval_acc: 0.946078 val_loss: 0.1593691\n",
      "1_149 train_acc: 0.8689 train_loss: 0.352207\tval_acc: 0.955882 val_loss: 0.1447530\n",
      "1_155 train_acc: 0.8464 train_loss: 0.361060\tval_acc: 0.955882 val_loss: 0.1402079\n",
      "1_210 train_acc: 0.8764 train_loss: 0.301777\tval_acc: 0.946078 val_loss: 0.1376676\n",
      "1_214 train_acc: 0.9176 train_loss: 0.206359\tval_acc: 0.975490 val_loss: 0.1084511\n",
      "1_223 train_acc: 0.9326 train_loss: 0.193025\tval_acc: 0.975490 val_loss: 0.1048734\n",
      "1_260 train_acc: 0.9026 train_loss: 0.254032\tval_acc: 0.960784 val_loss: 0.0815763\n",
      "1_321 train_acc: 0.9438 train_loss: 0.170006\tval_acc: 0.980392 val_loss: 0.0763911\n",
      "1_326 train_acc: 0.9251 train_loss: 0.204317\tval_acc: 0.960784 val_loss: 0.0733859\n",
      "1_340 train_acc: 0.9363 train_loss: 0.195708\tval_acc: 0.970588 val_loss: 0.0710811\n",
      "1_359 train_acc: 0.9326 train_loss: 0.165473\tval_acc: 0.975490 val_loss: 0.0709283\n",
      "1_372 train_acc: 0.9401 train_loss: 0.155589\tval_acc: 0.975490 val_loss: 0.0699118\n",
      "1_376 train_acc: 0.9176 train_loss: 0.187363\tval_acc: 0.970588 val_loss: 0.0693022\n",
      "1_377 train_acc: 0.9064 train_loss: 0.214470\tval_acc: 0.985294 val_loss: 0.0609905\n",
      "1_400 train_acc: 0.9026 train_loss: 0.226160\tval_acc: 0.980392 val_loss: 0.0540971\n",
      "1_433 train_acc: 0.9288 train_loss: 0.178425\tval_acc: 0.985294 val_loss: 0.0461678\n",
      "1_492 train_acc: 0.9288 train_loss: 0.162317\tval_acc: 0.985294 val_loss: 0.0431552\n",
      "1_496 train_acc: 0.9588 train_loss: 0.125753\tval_acc: 0.985294 val_loss: 0.0394924\n",
      "1_501 train_acc: 0.9588 train_loss: 0.116974\tval_acc: 0.995098 val_loss: 0.0352795\n",
      "1_540 train_acc: 0.9438 train_loss: 0.187818\tval_acc: 0.990196 val_loss: 0.0307810\n",
      "1_559 train_acc: 0.9438 train_loss: 0.151138\tval_acc: 1.000000 val_loss: 0.0279172\n",
      "1_602 train_acc: 0.9551 train_loss: 0.132581\tval_acc: 0.990196 val_loss: 0.0263172\n",
      "1_611 train_acc: 0.9625 train_loss: 0.119640\tval_acc: 1.000000 val_loss: 0.0259894\n",
      "1_648 train_acc: 0.9513 train_loss: 0.146309\tval_acc: 0.995098 val_loss: 0.0230743\n",
      "1_654 train_acc: 0.9363 train_loss: 0.177329\tval_acc: 0.995098 val_loss: 0.0230125\n",
      "1_667 train_acc: 0.9625 train_loss: 0.084298\tval_acc: 1.000000 val_loss: 0.0186650\n",
      "1_740 train_acc: 0.9551 train_loss: 0.138389\tval_acc: 1.000000 val_loss: 0.0178963\n",
      "1_747 train_acc: 0.9588 train_loss: 0.106440\tval_acc: 1.000000 val_loss: 0.0170205\n",
      "1_756 train_acc: 0.9513 train_loss: 0.104966\tval_acc: 1.000000 val_loss: 0.0152661\n",
      "1_760 train_acc: 0.9588 train_loss: 0.097484\tval_acc: 1.000000 val_loss: 0.0118460\n",
      "1_805 train_acc: 0.9738 train_loss: 0.086945\tval_acc: 1.000000 val_loss: 0.0098624\n",
      "1_854 train_acc: 0.9513 train_loss: 0.106563\tval_acc: 1.000000 val_loss: 0.0092924\n",
      "1_859 train_acc: 0.9363 train_loss: 0.156253\tval_acc: 1.000000 val_loss: 0.0089851\n",
      "1_862 train_acc: 0.9663 train_loss: 0.086810\tval_acc: 0.995098 val_loss: 0.0086394\n",
      "1_896 train_acc: 0.9401 train_loss: 0.161707\tval_acc: 1.000000 val_loss: 0.0073962\n",
      "1_964 train_acc: 0.9625 train_loss: 0.117983\tval_acc: 1.000000 val_loss: 0.0065336\n",
      "epoch:  964 \tThe test accuracy is: 0.9236111111111112\n",
      " THE BEST ACCURACY IS 0.9236111111111112\tkappa is 0.8981481481481481\n",
      "subject 1 duration: 0:09:32.456606\n",
      "seed is 480\n",
      "Subject 2\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "2_0 train_acc: 0.2622 train_loss: 2.415275\tval_acc: 0.269608 val_loss: 1.4900804\n",
      "2_1 train_acc: 0.2734 train_loss: 2.152667\tval_acc: 0.299020 val_loss: 1.4121202\n",
      "2_2 train_acc: 0.2846 train_loss: 1.907314\tval_acc: 0.338235 val_loss: 1.3595730\n",
      "2_3 train_acc: 0.3371 train_loss: 1.732292\tval_acc: 0.421569 val_loss: 1.2993692\n",
      "2_4 train_acc: 0.3558 train_loss: 1.686342\tval_acc: 0.450980 val_loss: 1.2609967\n",
      "2_5 train_acc: 0.2959 train_loss: 1.725261\tval_acc: 0.485294 val_loss: 1.2166889\n",
      "2_6 train_acc: 0.3633 train_loss: 1.556651\tval_acc: 0.539216 val_loss: 1.1771536\n",
      "2_7 train_acc: 0.3596 train_loss: 1.468724\tval_acc: 0.544118 val_loss: 1.1263467\n",
      "2_8 train_acc: 0.3858 train_loss: 1.436747\tval_acc: 0.593137 val_loss: 1.0871961\n",
      "2_9 train_acc: 0.3970 train_loss: 1.382230\tval_acc: 0.632353 val_loss: 1.0687526\n",
      "2_10 train_acc: 0.4082 train_loss: 1.393627\tval_acc: 0.661765 val_loss: 1.0406370\n",
      "2_11 train_acc: 0.4082 train_loss: 1.371938\tval_acc: 0.671569 val_loss: 1.0037334\n",
      "2_12 train_acc: 0.4307 train_loss: 1.342383\tval_acc: 0.676471 val_loss: 0.9874239\n",
      "2_13 train_acc: 0.4757 train_loss: 1.265492\tval_acc: 0.666667 val_loss: 0.9665970\n",
      "2_15 train_acc: 0.4719 train_loss: 1.229802\tval_acc: 0.725490 val_loss: 0.9398376\n",
      "2_16 train_acc: 0.4569 train_loss: 1.267846\tval_acc: 0.710784 val_loss: 0.9198966\n",
      "2_17 train_acc: 0.4345 train_loss: 1.269402\tval_acc: 0.725490 val_loss: 0.9090093\n",
      "2_18 train_acc: 0.5169 train_loss: 1.160733\tval_acc: 0.750000 val_loss: 0.8675821\n",
      "2_20 train_acc: 0.5094 train_loss: 1.096238\tval_acc: 0.725490 val_loss: 0.8528286\n",
      "2_21 train_acc: 0.5356 train_loss: 1.050758\tval_acc: 0.789216 val_loss: 0.8076065\n",
      "2_22 train_acc: 0.5468 train_loss: 1.065224\tval_acc: 0.725490 val_loss: 0.7963116\n",
      "2_23 train_acc: 0.5768 train_loss: 1.030257\tval_acc: 0.750000 val_loss: 0.7858891\n",
      "2_25 train_acc: 0.5019 train_loss: 1.131506\tval_acc: 0.774510 val_loss: 0.7528875\n",
      "2_26 train_acc: 0.5581 train_loss: 1.073775\tval_acc: 0.784314 val_loss: 0.7411712\n",
      "2_31 train_acc: 0.5581 train_loss: 1.028941\tval_acc: 0.764706 val_loss: 0.7254288\n",
      "2_32 train_acc: 0.6180 train_loss: 0.892028\tval_acc: 0.754902 val_loss: 0.7008941\n",
      "2_35 train_acc: 0.6030 train_loss: 0.939176\tval_acc: 0.813725 val_loss: 0.6553163\n",
      "2_44 train_acc: 0.6330 train_loss: 0.897043\tval_acc: 0.784314 val_loss: 0.6433377\n",
      "2_46 train_acc: 0.6442 train_loss: 0.877054\tval_acc: 0.799020 val_loss: 0.6282248\n",
      "2_47 train_acc: 0.6854 train_loss: 0.836144\tval_acc: 0.803922 val_loss: 0.6272488\n",
      "2_48 train_acc: 0.6404 train_loss: 0.844676\tval_acc: 0.789216 val_loss: 0.6239170\n",
      "2_50 train_acc: 0.6255 train_loss: 0.866468\tval_acc: 0.779412 val_loss: 0.6143799\n",
      "2_51 train_acc: 0.5993 train_loss: 0.995685\tval_acc: 0.784314 val_loss: 0.6101965\n",
      "2_52 train_acc: 0.6629 train_loss: 0.826183\tval_acc: 0.818627 val_loss: 0.5950840\n",
      "2_53 train_acc: 0.7079 train_loss: 0.703949\tval_acc: 0.799020 val_loss: 0.5926734\n",
      "2_59 train_acc: 0.6554 train_loss: 0.891508\tval_acc: 0.828431 val_loss: 0.5651052\n",
      "2_65 train_acc: 0.7079 train_loss: 0.771924\tval_acc: 0.818627 val_loss: 0.5615059\n",
      "2_66 train_acc: 0.7041 train_loss: 0.739451\tval_acc: 0.843137 val_loss: 0.5112658\n",
      "2_76 train_acc: 0.6592 train_loss: 0.751005\tval_acc: 0.857843 val_loss: 0.4990891\n",
      "2_77 train_acc: 0.6629 train_loss: 0.843288\tval_acc: 0.857843 val_loss: 0.4737822\n",
      "2_83 train_acc: 0.6854 train_loss: 0.781599\tval_acc: 0.892157 val_loss: 0.4535049\n",
      "2_90 train_acc: 0.7191 train_loss: 0.750021\tval_acc: 0.862745 val_loss: 0.4509676\n",
      "2_97 train_acc: 0.7416 train_loss: 0.715101\tval_acc: 0.857843 val_loss: 0.4453822\n",
      "2_99 train_acc: 0.7266 train_loss: 0.673537\tval_acc: 0.872549 val_loss: 0.4184631\n",
      "2_101 train_acc: 0.7266 train_loss: 0.733481\tval_acc: 0.877451 val_loss: 0.4149924\n",
      "2_104 train_acc: 0.7004 train_loss: 0.758100\tval_acc: 0.857843 val_loss: 0.4127530\n",
      "2_108 train_acc: 0.6966 train_loss: 0.734782\tval_acc: 0.916667 val_loss: 0.3824839\n",
      "2_115 train_acc: 0.6929 train_loss: 0.784543\tval_acc: 0.882353 val_loss: 0.3748686\n",
      "2_131 train_acc: 0.7491 train_loss: 0.619220\tval_acc: 0.867647 val_loss: 0.3712224\n",
      "2_132 train_acc: 0.7079 train_loss: 0.772702\tval_acc: 0.892157 val_loss: 0.3534193\n",
      "2_134 train_acc: 0.7191 train_loss: 0.660738\tval_acc: 0.892157 val_loss: 0.3482840\n",
      "2_135 train_acc: 0.7453 train_loss: 0.638114\tval_acc: 0.921569 val_loss: 0.3432214\n",
      "2_137 train_acc: 0.7491 train_loss: 0.637556\tval_acc: 0.887255 val_loss: 0.3365818\n",
      "2_139 train_acc: 0.7191 train_loss: 0.708718\tval_acc: 0.916667 val_loss: 0.3193828\n",
      "2_141 train_acc: 0.7453 train_loss: 0.651963\tval_acc: 0.906863 val_loss: 0.3096139\n",
      "2_144 train_acc: 0.7116 train_loss: 0.669360\tval_acc: 0.926471 val_loss: 0.3047329\n",
      "2_148 train_acc: 0.7678 train_loss: 0.600531\tval_acc: 0.926471 val_loss: 0.2869820\n",
      "2_149 train_acc: 0.7453 train_loss: 0.630831\tval_acc: 0.936275 val_loss: 0.2743835\n",
      "2_153 train_acc: 0.7528 train_loss: 0.616466\tval_acc: 0.916667 val_loss: 0.2613862\n",
      "2_161 train_acc: 0.7603 train_loss: 0.657849\tval_acc: 0.941176 val_loss: 0.2564851\n",
      "2_165 train_acc: 0.8090 train_loss: 0.526473\tval_acc: 0.911765 val_loss: 0.2520066\n",
      "2_169 train_acc: 0.7978 train_loss: 0.468472\tval_acc: 0.926471 val_loss: 0.2476390\n",
      "2_171 train_acc: 0.8090 train_loss: 0.532895\tval_acc: 0.941176 val_loss: 0.2465737\n",
      "2_172 train_acc: 0.8202 train_loss: 0.448810\tval_acc: 0.931373 val_loss: 0.2452111\n",
      "2_174 train_acc: 0.8277 train_loss: 0.462344\tval_acc: 0.950980 val_loss: 0.2281613\n",
      "2_183 train_acc: 0.8015 train_loss: 0.551428\tval_acc: 0.936275 val_loss: 0.2202899\n",
      "2_186 train_acc: 0.7865 train_loss: 0.562276\tval_acc: 0.941176 val_loss: 0.2112774\n",
      "2_193 train_acc: 0.8689 train_loss: 0.395629\tval_acc: 0.946078 val_loss: 0.1971980\n",
      "2_198 train_acc: 0.7940 train_loss: 0.527309\tval_acc: 0.965686 val_loss: 0.1818171\n",
      "2_212 train_acc: 0.8352 train_loss: 0.448371\tval_acc: 0.960784 val_loss: 0.1753482\n",
      "2_216 train_acc: 0.8427 train_loss: 0.438290\tval_acc: 0.965686 val_loss: 0.1472634\n",
      "2_234 train_acc: 0.8015 train_loss: 0.510032\tval_acc: 0.980392 val_loss: 0.1354276\n",
      "2_252 train_acc: 0.8577 train_loss: 0.379780\tval_acc: 0.970588 val_loss: 0.1136313\n",
      "2_270 train_acc: 0.8577 train_loss: 0.337506\tval_acc: 0.965686 val_loss: 0.1135400\n",
      "2_272 train_acc: 0.8202 train_loss: 0.441158\tval_acc: 0.980392 val_loss: 0.1019570\n",
      "2_284 train_acc: 0.8464 train_loss: 0.396351\tval_acc: 0.985294 val_loss: 0.0986124\n",
      "2_287 train_acc: 0.8390 train_loss: 0.443742\tval_acc: 0.985294 val_loss: 0.0981992\n",
      "2_288 train_acc: 0.8427 train_loss: 0.415229\tval_acc: 0.985294 val_loss: 0.0975751\n",
      "2_296 train_acc: 0.8727 train_loss: 0.380733\tval_acc: 0.975490 val_loss: 0.0861612\n",
      "2_352 train_acc: 0.9064 train_loss: 0.252755\tval_acc: 0.975490 val_loss: 0.0791596\n",
      "2_359 train_acc: 0.8727 train_loss: 0.298228\tval_acc: 0.990196 val_loss: 0.0723758\n",
      "2_388 train_acc: 0.8951 train_loss: 0.267817\tval_acc: 0.980392 val_loss: 0.0622326\n",
      "2_397 train_acc: 0.9064 train_loss: 0.298088\tval_acc: 0.995098 val_loss: 0.0575990\n",
      "2_438 train_acc: 0.8914 train_loss: 0.286786\tval_acc: 0.990196 val_loss: 0.0477790\n",
      "2_454 train_acc: 0.8989 train_loss: 0.260141\tval_acc: 0.995098 val_loss: 0.0443949\n",
      "2_470 train_acc: 0.8801 train_loss: 0.341821\tval_acc: 0.995098 val_loss: 0.0411614\n",
      "2_536 train_acc: 0.9176 train_loss: 0.217954\tval_acc: 1.000000 val_loss: 0.0350133\n",
      "2_542 train_acc: 0.9064 train_loss: 0.245940\tval_acc: 1.000000 val_loss: 0.0277316\n",
      "2_623 train_acc: 0.9251 train_loss: 0.213266\tval_acc: 0.995098 val_loss: 0.0238826\n",
      "2_639 train_acc: 0.9401 train_loss: 0.187572\tval_acc: 0.995098 val_loss: 0.0222134\n",
      "2_669 train_acc: 0.8876 train_loss: 0.327241\tval_acc: 1.000000 val_loss: 0.0158900\n",
      "2_828 train_acc: 0.9326 train_loss: 0.199505\tval_acc: 1.000000 val_loss: 0.0124692\n",
      "2_835 train_acc: 0.9363 train_loss: 0.195429\tval_acc: 1.000000 val_loss: 0.0116124\n",
      "2_856 train_acc: 0.9176 train_loss: 0.240770\tval_acc: 1.000000 val_loss: 0.0110081\n",
      "2_904 train_acc: 0.9288 train_loss: 0.204973\tval_acc: 1.000000 val_loss: 0.0098156\n",
      "2_959 train_acc: 0.9139 train_loss: 0.233894\tval_acc: 1.000000 val_loss: 0.0079820\n",
      "epoch:  959 \tThe test accuracy is: 0.78125\n",
      " THE BEST ACCURACY IS 0.78125\tkappa is 0.7083333333333333\n",
      "subject 2 duration: 0:09:32.815163\n",
      "seed is 2019\n",
      "Subject 3\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "3_0 train_acc: 0.2734 train_loss: 2.145917\tval_acc: 0.240196 val_loss: 1.4630418\n",
      "3_1 train_acc: 0.2772 train_loss: 1.963520\tval_acc: 0.328431 val_loss: 1.3634341\n",
      "3_2 train_acc: 0.3258 train_loss: 1.811567\tval_acc: 0.333333 val_loss: 1.3606128\n",
      "3_3 train_acc: 0.3446 train_loss: 1.716796\tval_acc: 0.338235 val_loss: 1.3605398\n",
      "3_4 train_acc: 0.4007 train_loss: 1.545422\tval_acc: 0.426471 val_loss: 1.3169110\n",
      "3_5 train_acc: 0.3483 train_loss: 1.536063\tval_acc: 0.495098 val_loss: 1.2338394\n",
      "3_6 train_acc: 0.3483 train_loss: 1.494348\tval_acc: 0.500000 val_loss: 1.1937563\n",
      "3_7 train_acc: 0.3596 train_loss: 1.408687\tval_acc: 0.549020 val_loss: 1.1490692\n",
      "3_8 train_acc: 0.3820 train_loss: 1.412503\tval_acc: 0.519608 val_loss: 1.1337128\n",
      "3_9 train_acc: 0.3670 train_loss: 1.372462\tval_acc: 0.661765 val_loss: 1.0401173\n",
      "3_10 train_acc: 0.5019 train_loss: 1.144644\tval_acc: 0.666667 val_loss: 1.0005274\n",
      "3_11 train_acc: 0.4532 train_loss: 1.301863\tval_acc: 0.696078 val_loss: 0.9390385\n",
      "3_12 train_acc: 0.5506 train_loss: 1.140708\tval_acc: 0.705882 val_loss: 0.8926187\n",
      "3_13 train_acc: 0.4644 train_loss: 1.166758\tval_acc: 0.769608 val_loss: 0.8464797\n",
      "3_15 train_acc: 0.5655 train_loss: 1.062594\tval_acc: 0.784314 val_loss: 0.7928177\n",
      "3_16 train_acc: 0.5131 train_loss: 1.164735\tval_acc: 0.818627 val_loss: 0.7635337\n",
      "3_17 train_acc: 0.5655 train_loss: 1.025520\tval_acc: 0.808824 val_loss: 0.7520518\n",
      "3_19 train_acc: 0.5356 train_loss: 1.047455\tval_acc: 0.789216 val_loss: 0.7279747\n",
      "3_20 train_acc: 0.5730 train_loss: 0.887796\tval_acc: 0.789216 val_loss: 0.6915785\n",
      "3_21 train_acc: 0.7079 train_loss: 0.773303\tval_acc: 0.774510 val_loss: 0.6905304\n",
      "3_22 train_acc: 0.5730 train_loss: 1.009238\tval_acc: 0.813725 val_loss: 0.6566784\n",
      "3_24 train_acc: 0.6479 train_loss: 0.886464\tval_acc: 0.833333 val_loss: 0.5961314\n",
      "3_27 train_acc: 0.6554 train_loss: 0.804997\tval_acc: 0.838235 val_loss: 0.5660259\n",
      "3_28 train_acc: 0.7004 train_loss: 0.765837\tval_acc: 0.848039 val_loss: 0.5255275\n",
      "3_32 train_acc: 0.6592 train_loss: 0.791011\tval_acc: 0.852941 val_loss: 0.5128901\n",
      "3_33 train_acc: 0.7303 train_loss: 0.741542\tval_acc: 0.852941 val_loss: 0.5077602\n",
      "3_34 train_acc: 0.7528 train_loss: 0.629402\tval_acc: 0.862745 val_loss: 0.4938793\n",
      "3_35 train_acc: 0.7116 train_loss: 0.698153\tval_acc: 0.877451 val_loss: 0.4811451\n",
      "3_36 train_acc: 0.7116 train_loss: 0.658239\tval_acc: 0.877451 val_loss: 0.4613496\n",
      "3_38 train_acc: 0.7640 train_loss: 0.656463\tval_acc: 0.897059 val_loss: 0.4485556\n",
      "3_39 train_acc: 0.7041 train_loss: 0.696241\tval_acc: 0.882353 val_loss: 0.4427270\n",
      "3_41 train_acc: 0.7341 train_loss: 0.680061\tval_acc: 0.892157 val_loss: 0.4088276\n",
      "3_42 train_acc: 0.7191 train_loss: 0.630477\tval_acc: 0.897059 val_loss: 0.3741921\n",
      "3_47 train_acc: 0.7640 train_loss: 0.591871\tval_acc: 0.906863 val_loss: 0.3512491\n",
      "3_53 train_acc: 0.7753 train_loss: 0.626402\tval_acc: 0.906863 val_loss: 0.3107303\n",
      "3_57 train_acc: 0.7828 train_loss: 0.579940\tval_acc: 0.906863 val_loss: 0.3074436\n",
      "3_60 train_acc: 0.7828 train_loss: 0.535665\tval_acc: 0.916667 val_loss: 0.2798489\n",
      "3_61 train_acc: 0.8090 train_loss: 0.529661\tval_acc: 0.931373 val_loss: 0.2322775\n",
      "3_74 train_acc: 0.8052 train_loss: 0.490675\tval_acc: 0.941176 val_loss: 0.2284762\n",
      "3_75 train_acc: 0.8352 train_loss: 0.508759\tval_acc: 0.916667 val_loss: 0.2250215\n",
      "3_80 train_acc: 0.8165 train_loss: 0.442117\tval_acc: 0.931373 val_loss: 0.2185618\n",
      "3_81 train_acc: 0.8352 train_loss: 0.427447\tval_acc: 0.936275 val_loss: 0.2015396\n",
      "3_82 train_acc: 0.8464 train_loss: 0.423523\tval_acc: 0.946078 val_loss: 0.1796550\n",
      "3_95 train_acc: 0.8240 train_loss: 0.408449\tval_acc: 0.950980 val_loss: 0.1658715\n",
      "3_102 train_acc: 0.8090 train_loss: 0.473710\tval_acc: 0.965686 val_loss: 0.1625442\n",
      "3_113 train_acc: 0.8352 train_loss: 0.433238\tval_acc: 0.950980 val_loss: 0.1593926\n",
      "3_117 train_acc: 0.8277 train_loss: 0.460631\tval_acc: 0.960784 val_loss: 0.1503490\n",
      "3_118 train_acc: 0.8914 train_loss: 0.316721\tval_acc: 0.946078 val_loss: 0.1502538\n",
      "3_122 train_acc: 0.8989 train_loss: 0.350959\tval_acc: 0.955882 val_loss: 0.1395331\n",
      "3_127 train_acc: 0.8764 train_loss: 0.356504\tval_acc: 0.970588 val_loss: 0.1073353\n",
      "3_148 train_acc: 0.8839 train_loss: 0.289497\tval_acc: 0.990196 val_loss: 0.0896452\n",
      "3_157 train_acc: 0.9064 train_loss: 0.230792\tval_acc: 0.990196 val_loss: 0.0769457\n",
      "3_159 train_acc: 0.8914 train_loss: 0.324616\tval_acc: 0.980392 val_loss: 0.0750383\n",
      "3_168 train_acc: 0.9026 train_loss: 0.250249\tval_acc: 0.985294 val_loss: 0.0641226\n",
      "3_181 train_acc: 0.9101 train_loss: 0.231630\tval_acc: 0.980392 val_loss: 0.0631541\n",
      "3_182 train_acc: 0.9288 train_loss: 0.222305\tval_acc: 0.990196 val_loss: 0.0452309\n",
      "3_220 train_acc: 0.9064 train_loss: 0.225769\tval_acc: 0.995098 val_loss: 0.0332429\n",
      "3_246 train_acc: 0.9551 train_loss: 0.134575\tval_acc: 0.995098 val_loss: 0.0239915\n",
      "3_310 train_acc: 0.9363 train_loss: 0.155143\tval_acc: 0.995098 val_loss: 0.0239141\n",
      "3_317 train_acc: 0.9288 train_loss: 0.190156\tval_acc: 0.995098 val_loss: 0.0144175\n",
      "3_388 train_acc: 0.9738 train_loss: 0.079211\tval_acc: 0.995098 val_loss: 0.0118867\n",
      "3_440 train_acc: 0.9476 train_loss: 0.155969\tval_acc: 0.995098 val_loss: 0.0102599\n",
      "3_461 train_acc: 0.9551 train_loss: 0.126575\tval_acc: 0.995098 val_loss: 0.0099607\n",
      "3_472 train_acc: 0.9700 train_loss: 0.082535\tval_acc: 1.000000 val_loss: 0.0076682\n",
      "3_475 train_acc: 0.9738 train_loss: 0.088876\tval_acc: 1.000000 val_loss: 0.0065138\n",
      "3_495 train_acc: 0.9738 train_loss: 0.093292\tval_acc: 1.000000 val_loss: 0.0044033\n",
      "3_580 train_acc: 0.9775 train_loss: 0.071163\tval_acc: 1.000000 val_loss: 0.0029962\n",
      "3_616 train_acc: 0.9625 train_loss: 0.117059\tval_acc: 1.000000 val_loss: 0.0028463\n",
      "3_662 train_acc: 0.9588 train_loss: 0.152773\tval_acc: 1.000000 val_loss: 0.0016945\n",
      "3_736 train_acc: 0.9775 train_loss: 0.063226\tval_acc: 1.000000 val_loss: 0.0016116\n",
      "3_753 train_acc: 0.9588 train_loss: 0.139852\tval_acc: 1.000000 val_loss: 0.0011427\n",
      "3_848 train_acc: 0.9551 train_loss: 0.123628\tval_acc: 1.000000 val_loss: 0.0009666\n",
      "3_939 train_acc: 0.9813 train_loss: 0.042063\tval_acc: 1.000000 val_loss: 0.0009617\n",
      "epoch:  939 \tThe test accuracy is: 0.9583333333333334\n",
      " THE BEST ACCURACY IS 0.9583333333333334\tkappa is 0.9444444444444444\n",
      "subject 3 duration: 0:09:32.529770\n",
      "seed is 1510\n",
      "Subject 4\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "4_0 train_acc: 0.2472 train_loss: 2.423151\tval_acc: 0.284314 val_loss: 1.4588392\n",
      "4_1 train_acc: 0.2846 train_loss: 2.101697\tval_acc: 0.372549 val_loss: 1.3832642\n",
      "4_2 train_acc: 0.3109 train_loss: 1.940966\tval_acc: 0.343137 val_loss: 1.3812349\n",
      "4_3 train_acc: 0.2884 train_loss: 1.834007\tval_acc: 0.382353 val_loss: 1.3438048\n",
      "4_4 train_acc: 0.2996 train_loss: 1.746464\tval_acc: 0.372549 val_loss: 1.3275800\n",
      "4_5 train_acc: 0.3221 train_loss: 1.680381\tval_acc: 0.446078 val_loss: 1.2672116\n",
      "4_6 train_acc: 0.3521 train_loss: 1.667609\tval_acc: 0.558824 val_loss: 1.1934390\n",
      "4_8 train_acc: 0.4157 train_loss: 1.453762\tval_acc: 0.519608 val_loss: 1.1892080\n",
      "4_9 train_acc: 0.3184 train_loss: 1.518991\tval_acc: 0.549020 val_loss: 1.1397817\n",
      "4_10 train_acc: 0.4382 train_loss: 1.333395\tval_acc: 0.558824 val_loss: 1.1251216\n",
      "4_11 train_acc: 0.3745 train_loss: 1.431435\tval_acc: 0.612745 val_loss: 1.0949730\n",
      "4_12 train_acc: 0.4569 train_loss: 1.306956\tval_acc: 0.612745 val_loss: 1.0558952\n",
      "4_13 train_acc: 0.4345 train_loss: 1.300139\tval_acc: 0.676471 val_loss: 1.0416586\n",
      "4_14 train_acc: 0.4757 train_loss: 1.283658\tval_acc: 0.622549 val_loss: 1.0254716\n",
      "4_16 train_acc: 0.4345 train_loss: 1.342122\tval_acc: 0.686275 val_loss: 0.9899082\n",
      "4_17 train_acc: 0.4906 train_loss: 1.182228\tval_acc: 0.676471 val_loss: 0.9775268\n",
      "4_18 train_acc: 0.4831 train_loss: 1.196488\tval_acc: 0.671569 val_loss: 0.9479225\n",
      "4_19 train_acc: 0.4831 train_loss: 1.170038\tval_acc: 0.691176 val_loss: 0.9297562\n",
      "4_20 train_acc: 0.4794 train_loss: 1.163827\tval_acc: 0.681373 val_loss: 0.9213015\n",
      "4_21 train_acc: 0.4944 train_loss: 1.160859\tval_acc: 0.676471 val_loss: 0.8866003\n",
      "4_22 train_acc: 0.5468 train_loss: 1.060660\tval_acc: 0.750000 val_loss: 0.8378557\n",
      "4_23 train_acc: 0.5543 train_loss: 1.024374\tval_acc: 0.754902 val_loss: 0.8107421\n",
      "4_25 train_acc: 0.5618 train_loss: 1.096562\tval_acc: 0.794118 val_loss: 0.7699285\n",
      "4_26 train_acc: 0.6142 train_loss: 0.978699\tval_acc: 0.774510 val_loss: 0.7691386\n",
      "4_27 train_acc: 0.5581 train_loss: 1.026429\tval_acc: 0.784314 val_loss: 0.7669141\n",
      "4_28 train_acc: 0.5318 train_loss: 1.034584\tval_acc: 0.789216 val_loss: 0.7339063\n",
      "4_29 train_acc: 0.5768 train_loss: 1.023562\tval_acc: 0.808824 val_loss: 0.7007763\n",
      "4_31 train_acc: 0.6217 train_loss: 0.984510\tval_acc: 0.813725 val_loss: 0.6944347\n",
      "4_32 train_acc: 0.6816 train_loss: 0.823640\tval_acc: 0.818627 val_loss: 0.6930711\n",
      "4_33 train_acc: 0.6442 train_loss: 0.866792\tval_acc: 0.813725 val_loss: 0.6355109\n",
      "4_37 train_acc: 0.6629 train_loss: 0.820638\tval_acc: 0.823529 val_loss: 0.6059232\n",
      "4_38 train_acc: 0.6592 train_loss: 0.868675\tval_acc: 0.882353 val_loss: 0.5630828\n",
      "4_40 train_acc: 0.6330 train_loss: 0.883122\tval_acc: 0.872549 val_loss: 0.5544445\n",
      "4_43 train_acc: 0.6255 train_loss: 0.912194\tval_acc: 0.843137 val_loss: 0.5462161\n",
      "4_45 train_acc: 0.7079 train_loss: 0.721053\tval_acc: 0.877451 val_loss: 0.5400709\n",
      "4_47 train_acc: 0.6517 train_loss: 0.880110\tval_acc: 0.867647 val_loss: 0.5106894\n",
      "4_50 train_acc: 0.6367 train_loss: 0.874081\tval_acc: 0.862745 val_loss: 0.4714971\n",
      "4_55 train_acc: 0.7116 train_loss: 0.744468\tval_acc: 0.872549 val_loss: 0.4631158\n",
      "4_56 train_acc: 0.7266 train_loss: 0.731974\tval_acc: 0.867647 val_loss: 0.4470245\n",
      "4_57 train_acc: 0.6891 train_loss: 0.761177\tval_acc: 0.911765 val_loss: 0.4137120\n",
      "4_65 train_acc: 0.7303 train_loss: 0.710856\tval_acc: 0.897059 val_loss: 0.3981823\n",
      "4_69 train_acc: 0.6891 train_loss: 0.727265\tval_acc: 0.887255 val_loss: 0.3885778\n",
      "4_74 train_acc: 0.7416 train_loss: 0.576895\tval_acc: 0.901961 val_loss: 0.3860703\n",
      "4_75 train_acc: 0.7266 train_loss: 0.726913\tval_acc: 0.872549 val_loss: 0.3801615\n",
      "4_77 train_acc: 0.7228 train_loss: 0.631100\tval_acc: 0.877451 val_loss: 0.3704787\n",
      "4_79 train_acc: 0.6816 train_loss: 0.785641\tval_acc: 0.867647 val_loss: 0.3539586\n",
      "4_81 train_acc: 0.7266 train_loss: 0.677690\tval_acc: 0.892157 val_loss: 0.3230363\n",
      "4_83 train_acc: 0.6854 train_loss: 0.721154\tval_acc: 0.897059 val_loss: 0.3123856\n",
      "4_95 train_acc: 0.7603 train_loss: 0.616774\tval_acc: 0.897059 val_loss: 0.3041008\n",
      "4_100 train_acc: 0.7528 train_loss: 0.596849\tval_acc: 0.911765 val_loss: 0.2727671\n",
      "4_115 train_acc: 0.7753 train_loss: 0.589019\tval_acc: 0.926471 val_loss: 0.2568082\n",
      "4_126 train_acc: 0.8052 train_loss: 0.522312\tval_acc: 0.941176 val_loss: 0.2378177\n",
      "4_133 train_acc: 0.7491 train_loss: 0.616973\tval_acc: 0.936275 val_loss: 0.2360942\n",
      "4_134 train_acc: 0.7978 train_loss: 0.548726\tval_acc: 0.926471 val_loss: 0.2338042\n",
      "4_141 train_acc: 0.7715 train_loss: 0.525569\tval_acc: 0.926471 val_loss: 0.2278402\n",
      "4_149 train_acc: 0.7978 train_loss: 0.487706\tval_acc: 0.936275 val_loss: 0.2235366\n",
      "4_151 train_acc: 0.8090 train_loss: 0.499197\tval_acc: 0.936275 val_loss: 0.2218536\n",
      "4_157 train_acc: 0.7753 train_loss: 0.560582\tval_acc: 0.941176 val_loss: 0.2203345\n",
      "4_160 train_acc: 0.7753 train_loss: 0.561709\tval_acc: 0.950980 val_loss: 0.2131089\n",
      "4_162 train_acc: 0.8315 train_loss: 0.461605\tval_acc: 0.926471 val_loss: 0.2129523\n",
      "4_164 train_acc: 0.8165 train_loss: 0.530256\tval_acc: 0.955882 val_loss: 0.1882486\n",
      "4_169 train_acc: 0.7940 train_loss: 0.550658\tval_acc: 0.941176 val_loss: 0.1865571\n",
      "4_176 train_acc: 0.7640 train_loss: 0.556440\tval_acc: 0.960784 val_loss: 0.1704244\n",
      "4_194 train_acc: 0.8464 train_loss: 0.468625\tval_acc: 0.960784 val_loss: 0.1609572\n",
      "4_198 train_acc: 0.8652 train_loss: 0.411994\tval_acc: 0.960784 val_loss: 0.1608702\n",
      "4_199 train_acc: 0.7978 train_loss: 0.592957\tval_acc: 0.946078 val_loss: 0.1490493\n",
      "4_217 train_acc: 0.8315 train_loss: 0.444643\tval_acc: 0.965686 val_loss: 0.1365439\n",
      "4_243 train_acc: 0.8390 train_loss: 0.400814\tval_acc: 0.975490 val_loss: 0.1192967\n",
      "4_266 train_acc: 0.8277 train_loss: 0.447462\tval_acc: 0.975490 val_loss: 0.1179297\n",
      "4_269 train_acc: 0.8464 train_loss: 0.397618\tval_acc: 0.975490 val_loss: 0.1175892\n",
      "4_274 train_acc: 0.8464 train_loss: 0.384980\tval_acc: 0.975490 val_loss: 0.1149924\n",
      "4_276 train_acc: 0.8764 train_loss: 0.317321\tval_acc: 0.975490 val_loss: 0.1004024\n",
      "4_293 train_acc: 0.8502 train_loss: 0.432394\tval_acc: 0.985294 val_loss: 0.0995892\n",
      "4_298 train_acc: 0.8352 train_loss: 0.447379\tval_acc: 0.985294 val_loss: 0.0901901\n",
      "4_340 train_acc: 0.8539 train_loss: 0.445290\tval_acc: 0.975490 val_loss: 0.0808534\n",
      "4_342 train_acc: 0.8502 train_loss: 0.384481\tval_acc: 0.990196 val_loss: 0.0683698\n",
      "4_370 train_acc: 0.8614 train_loss: 0.355350\tval_acc: 0.985294 val_loss: 0.0674261\n",
      "4_396 train_acc: 0.8689 train_loss: 0.378571\tval_acc: 0.985294 val_loss: 0.0528256\n",
      "4_421 train_acc: 0.8914 train_loss: 0.331373\tval_acc: 0.990196 val_loss: 0.0521098\n",
      "4_432 train_acc: 0.8502 train_loss: 0.352267\tval_acc: 0.990196 val_loss: 0.0483682\n",
      "4_433 train_acc: 0.8876 train_loss: 0.257216\tval_acc: 0.995098 val_loss: 0.0419097\n",
      "4_468 train_acc: 0.8727 train_loss: 0.346138\tval_acc: 1.000000 val_loss: 0.0403927\n",
      "4_474 train_acc: 0.9176 train_loss: 0.239753\tval_acc: 0.995098 val_loss: 0.0382696\n",
      "4_476 train_acc: 0.9026 train_loss: 0.290287\tval_acc: 1.000000 val_loss: 0.0379466\n",
      "4_495 train_acc: 0.8914 train_loss: 0.291353\tval_acc: 0.990196 val_loss: 0.0364767\n",
      "4_509 train_acc: 0.8764 train_loss: 0.325388\tval_acc: 1.000000 val_loss: 0.0337507\n",
      "4_523 train_acc: 0.8689 train_loss: 0.362727\tval_acc: 0.995098 val_loss: 0.0326698\n",
      "4_527 train_acc: 0.8951 train_loss: 0.339272\tval_acc: 1.000000 val_loss: 0.0322812\n",
      "4_559 train_acc: 0.8689 train_loss: 0.335620\tval_acc: 0.995098 val_loss: 0.0284098\n",
      "4_584 train_acc: 0.8876 train_loss: 0.273865\tval_acc: 1.000000 val_loss: 0.0264436\n",
      "4_589 train_acc: 0.8652 train_loss: 0.341753\tval_acc: 1.000000 val_loss: 0.0243859\n",
      "4_625 train_acc: 0.8989 train_loss: 0.247060\tval_acc: 0.995098 val_loss: 0.0237453\n",
      "4_642 train_acc: 0.8876 train_loss: 0.261924\tval_acc: 1.000000 val_loss: 0.0216126\n",
      "4_645 train_acc: 0.9064 train_loss: 0.263379\tval_acc: 1.000000 val_loss: 0.0192828\n",
      "4_702 train_acc: 0.9101 train_loss: 0.265036\tval_acc: 0.995098 val_loss: 0.0179911\n",
      "4_730 train_acc: 0.9064 train_loss: 0.269396\tval_acc: 1.000000 val_loss: 0.0168489\n",
      "4_754 train_acc: 0.9026 train_loss: 0.243963\tval_acc: 1.000000 val_loss: 0.0155297\n",
      "4_759 train_acc: 0.9101 train_loss: 0.232039\tval_acc: 1.000000 val_loss: 0.0132894\n",
      "4_768 train_acc: 0.9251 train_loss: 0.237875\tval_acc: 1.000000 val_loss: 0.0131579\n",
      "4_776 train_acc: 0.9176 train_loss: 0.224474\tval_acc: 1.000000 val_loss: 0.0120450\n",
      "4_786 train_acc: 0.9101 train_loss: 0.248237\tval_acc: 1.000000 val_loss: 0.0111686\n",
      "4_791 train_acc: 0.9213 train_loss: 0.220439\tval_acc: 1.000000 val_loss: 0.0104842\n",
      "4_824 train_acc: 0.9513 train_loss: 0.142809\tval_acc: 1.000000 val_loss: 0.0102749\n",
      "4_859 train_acc: 0.9026 train_loss: 0.300973\tval_acc: 1.000000 val_loss: 0.0101842\n",
      "4_871 train_acc: 0.9064 train_loss: 0.256279\tval_acc: 1.000000 val_loss: 0.0093612\n",
      "4_893 train_acc: 0.9176 train_loss: 0.241442\tval_acc: 1.000000 val_loss: 0.0089965\n",
      "4_902 train_acc: 0.9026 train_loss: 0.270160\tval_acc: 1.000000 val_loss: 0.0079765\n",
      "4_945 train_acc: 0.9288 train_loss: 0.179977\tval_acc: 1.000000 val_loss: 0.0077013\n",
      "4_950 train_acc: 0.8951 train_loss: 0.256608\tval_acc: 1.000000 val_loss: 0.0070222\n",
      "4_957 train_acc: 0.9176 train_loss: 0.229494\tval_acc: 1.000000 val_loss: 0.0066827\n",
      "4_989 train_acc: 0.9551 train_loss: 0.118409\tval_acc: 1.000000 val_loss: 0.0057189\n",
      "epoch:  989 \tThe test accuracy is: 0.8645833333333334\n",
      " THE BEST ACCURACY IS 0.8645833333333334\tkappa is 0.8194444444444444\n",
      "subject 4 duration: 0:09:32.676998\n",
      "seed is 845\n",
      "Subject 5\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "5_0 train_acc: 0.2584 train_loss: 2.192267\tval_acc: 0.269608 val_loss: 1.4992311\n",
      "5_1 train_acc: 0.2734 train_loss: 2.079959\tval_acc: 0.289216 val_loss: 1.3776740\n",
      "5_2 train_acc: 0.3146 train_loss: 1.898362\tval_acc: 0.323529 val_loss: 1.3603885\n",
      "5_3 train_acc: 0.2959 train_loss: 1.749409\tval_acc: 0.450980 val_loss: 1.2736347\n",
      "5_4 train_acc: 0.3221 train_loss: 1.763416\tval_acc: 0.524510 val_loss: 1.2373923\n",
      "5_5 train_acc: 0.3783 train_loss: 1.651221\tval_acc: 0.588235 val_loss: 1.1731017\n",
      "5_6 train_acc: 0.3708 train_loss: 1.565384\tval_acc: 0.588235 val_loss: 1.1531914\n",
      "5_7 train_acc: 0.4270 train_loss: 1.429250\tval_acc: 0.671569 val_loss: 1.0748292\n",
      "5_8 train_acc: 0.3970 train_loss: 1.529991\tval_acc: 0.710784 val_loss: 1.0058614\n",
      "5_9 train_acc: 0.3783 train_loss: 1.489845\tval_acc: 0.705882 val_loss: 0.9612224\n",
      "5_10 train_acc: 0.4307 train_loss: 1.316485\tval_acc: 0.686275 val_loss: 0.9313717\n",
      "5_11 train_acc: 0.4157 train_loss: 1.292073\tval_acc: 0.769608 val_loss: 0.8853987\n",
      "5_12 train_acc: 0.4607 train_loss: 1.204318\tval_acc: 0.750000 val_loss: 0.8438187\n",
      "5_13 train_acc: 0.5768 train_loss: 1.075286\tval_acc: 0.759804 val_loss: 0.7922897\n",
      "5_14 train_acc: 0.5393 train_loss: 1.176669\tval_acc: 0.774510 val_loss: 0.7814090\n",
      "5_15 train_acc: 0.5506 train_loss: 1.067910\tval_acc: 0.774510 val_loss: 0.7580540\n",
      "5_16 train_acc: 0.5281 train_loss: 1.053275\tval_acc: 0.774510 val_loss: 0.7150977\n",
      "5_17 train_acc: 0.6142 train_loss: 0.963597\tval_acc: 0.764706 val_loss: 0.7146091\n",
      "5_18 train_acc: 0.6067 train_loss: 0.959471\tval_acc: 0.803922 val_loss: 0.6755859\n",
      "5_19 train_acc: 0.5543 train_loss: 1.091713\tval_acc: 0.833333 val_loss: 0.6601934\n",
      "5_20 train_acc: 0.6330 train_loss: 0.879105\tval_acc: 0.794118 val_loss: 0.6482679\n",
      "5_21 train_acc: 0.6929 train_loss: 0.801109\tval_acc: 0.808824 val_loss: 0.6201499\n",
      "5_22 train_acc: 0.6217 train_loss: 0.869298\tval_acc: 0.833333 val_loss: 0.5903291\n",
      "5_24 train_acc: 0.6030 train_loss: 0.962609\tval_acc: 0.838235 val_loss: 0.5596671\n",
      "5_25 train_acc: 0.7004 train_loss: 0.775191\tval_acc: 0.843137 val_loss: 0.5262282\n",
      "5_28 train_acc: 0.6629 train_loss: 0.771990\tval_acc: 0.857843 val_loss: 0.5139027\n",
      "5_29 train_acc: 0.6629 train_loss: 0.816976\tval_acc: 0.862745 val_loss: 0.4989150\n",
      "5_31 train_acc: 0.7004 train_loss: 0.770139\tval_acc: 0.862745 val_loss: 0.4943969\n",
      "5_32 train_acc: 0.7191 train_loss: 0.703801\tval_acc: 0.867647 val_loss: 0.4639407\n",
      "5_34 train_acc: 0.7079 train_loss: 0.721551\tval_acc: 0.887255 val_loss: 0.4245344\n",
      "5_36 train_acc: 0.7978 train_loss: 0.571643\tval_acc: 0.901961 val_loss: 0.4042428\n",
      "5_43 train_acc: 0.7378 train_loss: 0.706626\tval_acc: 0.867647 val_loss: 0.4000641\n",
      "5_46 train_acc: 0.7528 train_loss: 0.590441\tval_acc: 0.882353 val_loss: 0.3685524\n",
      "5_47 train_acc: 0.7640 train_loss: 0.592840\tval_acc: 0.872549 val_loss: 0.3624807\n",
      "5_50 train_acc: 0.7266 train_loss: 0.671262\tval_acc: 0.882353 val_loss: 0.3620896\n",
      "5_54 train_acc: 0.7790 train_loss: 0.589270\tval_acc: 0.892157 val_loss: 0.3359353\n",
      "5_63 train_acc: 0.8165 train_loss: 0.518834\tval_acc: 0.897059 val_loss: 0.3045276\n",
      "5_64 train_acc: 0.8165 train_loss: 0.481552\tval_acc: 0.911765 val_loss: 0.3003509\n",
      "5_68 train_acc: 0.8052 train_loss: 0.547858\tval_acc: 0.926471 val_loss: 0.2822376\n",
      "5_77 train_acc: 0.7940 train_loss: 0.567553\tval_acc: 0.946078 val_loss: 0.2597107\n",
      "5_81 train_acc: 0.8127 train_loss: 0.461543\tval_acc: 0.926471 val_loss: 0.2578239\n",
      "5_86 train_acc: 0.8015 train_loss: 0.517266\tval_acc: 0.926471 val_loss: 0.2474455\n",
      "5_87 train_acc: 0.8090 train_loss: 0.486497\tval_acc: 0.931373 val_loss: 0.2391622\n",
      "5_90 train_acc: 0.8240 train_loss: 0.454239\tval_acc: 0.946078 val_loss: 0.2330129\n",
      "5_94 train_acc: 0.7940 train_loss: 0.530413\tval_acc: 0.946078 val_loss: 0.2230315\n",
      "5_98 train_acc: 0.8427 train_loss: 0.471436\tval_acc: 0.921569 val_loss: 0.2153202\n",
      "5_110 train_acc: 0.8277 train_loss: 0.405123\tval_acc: 0.946078 val_loss: 0.2053437\n",
      "5_120 train_acc: 0.8352 train_loss: 0.477105\tval_acc: 0.955882 val_loss: 0.1977318\n",
      "5_126 train_acc: 0.8502 train_loss: 0.419869\tval_acc: 0.941176 val_loss: 0.1969092\n",
      "5_127 train_acc: 0.8464 train_loss: 0.415577\tval_acc: 0.926471 val_loss: 0.1948541\n",
      "5_130 train_acc: 0.8614 train_loss: 0.366898\tval_acc: 0.946078 val_loss: 0.1852076\n",
      "5_131 train_acc: 0.8539 train_loss: 0.430114\tval_acc: 0.936275 val_loss: 0.1825842\n",
      "5_134 train_acc: 0.8539 train_loss: 0.401352\tval_acc: 0.950980 val_loss: 0.1702307\n",
      "5_139 train_acc: 0.8502 train_loss: 0.474676\tval_acc: 0.955882 val_loss: 0.1503065\n",
      "5_149 train_acc: 0.8764 train_loss: 0.333049\tval_acc: 0.965686 val_loss: 0.1399772\n",
      "5_166 train_acc: 0.8165 train_loss: 0.411705\tval_acc: 0.970588 val_loss: 0.1387273\n",
      "5_169 train_acc: 0.8801 train_loss: 0.356827\tval_acc: 0.965686 val_loss: 0.1333636\n",
      "5_170 train_acc: 0.8390 train_loss: 0.382906\tval_acc: 0.975490 val_loss: 0.1269493\n",
      "5_193 train_acc: 0.8951 train_loss: 0.312037\tval_acc: 0.955882 val_loss: 0.1192800\n",
      "5_198 train_acc: 0.8390 train_loss: 0.406551\tval_acc: 0.980392 val_loss: 0.1101658\n",
      "5_204 train_acc: 0.8502 train_loss: 0.364653\tval_acc: 0.980392 val_loss: 0.1097998\n",
      "5_211 train_acc: 0.8502 train_loss: 0.410173\tval_acc: 0.980392 val_loss: 0.1006765\n",
      "5_221 train_acc: 0.8689 train_loss: 0.373144\tval_acc: 0.980392 val_loss: 0.0943267\n",
      "5_228 train_acc: 0.8689 train_loss: 0.374298\tval_acc: 0.990196 val_loss: 0.0880880\n",
      "5_238 train_acc: 0.8989 train_loss: 0.293217\tval_acc: 0.990196 val_loss: 0.0779021\n",
      "5_286 train_acc: 0.8689 train_loss: 0.292715\tval_acc: 0.975490 val_loss: 0.0755994\n",
      "5_291 train_acc: 0.8914 train_loss: 0.287962\tval_acc: 0.975490 val_loss: 0.0714917\n",
      "5_298 train_acc: 0.9026 train_loss: 0.306638\tval_acc: 0.980392 val_loss: 0.0708098\n",
      "5_305 train_acc: 0.8951 train_loss: 0.268920\tval_acc: 0.980392 val_loss: 0.0691109\n",
      "5_306 train_acc: 0.8839 train_loss: 0.327906\tval_acc: 0.985294 val_loss: 0.0597136\n",
      "5_307 train_acc: 0.8764 train_loss: 0.361758\tval_acc: 0.985294 val_loss: 0.0582400\n",
      "5_336 train_acc: 0.8914 train_loss: 0.240022\tval_acc: 0.990196 val_loss: 0.0540137\n",
      "5_366 train_acc: 0.9139 train_loss: 0.240780\tval_acc: 0.985294 val_loss: 0.0538749\n",
      "5_371 train_acc: 0.9326 train_loss: 0.231453\tval_acc: 0.995098 val_loss: 0.0494972\n",
      "5_386 train_acc: 0.9064 train_loss: 0.239411\tval_acc: 1.000000 val_loss: 0.0474322\n",
      "5_390 train_acc: 0.9026 train_loss: 0.321300\tval_acc: 0.995098 val_loss: 0.0444773\n",
      "5_404 train_acc: 0.8914 train_loss: 0.246402\tval_acc: 1.000000 val_loss: 0.0411701\n",
      "5_405 train_acc: 0.9213 train_loss: 0.228945\tval_acc: 0.995098 val_loss: 0.0364383\n",
      "5_408 train_acc: 0.8989 train_loss: 0.311710\tval_acc: 0.995098 val_loss: 0.0361488\n",
      "5_450 train_acc: 0.9026 train_loss: 0.263937\tval_acc: 0.995098 val_loss: 0.0316242\n",
      "5_500 train_acc: 0.8689 train_loss: 0.295033\tval_acc: 1.000000 val_loss: 0.0310785\n",
      "5_531 train_acc: 0.9064 train_loss: 0.290043\tval_acc: 0.990196 val_loss: 0.0296112\n",
      "5_551 train_acc: 0.9101 train_loss: 0.233078\tval_acc: 0.995098 val_loss: 0.0285561\n",
      "5_559 train_acc: 0.9101 train_loss: 0.211066\tval_acc: 0.995098 val_loss: 0.0266656\n",
      "5_574 train_acc: 0.8876 train_loss: 0.306975\tval_acc: 0.995098 val_loss: 0.0265786\n",
      "5_585 train_acc: 0.8989 train_loss: 0.269613\tval_acc: 1.000000 val_loss: 0.0208526\n",
      "5_626 train_acc: 0.9101 train_loss: 0.277005\tval_acc: 1.000000 val_loss: 0.0190887\n",
      "5_655 train_acc: 0.9139 train_loss: 0.234822\tval_acc: 1.000000 val_loss: 0.0145756\n",
      "5_731 train_acc: 0.9139 train_loss: 0.253404\tval_acc: 1.000000 val_loss: 0.0137663\n",
      "5_744 train_acc: 0.9213 train_loss: 0.206225\tval_acc: 1.000000 val_loss: 0.0134919\n",
      "5_748 train_acc: 0.9176 train_loss: 0.241024\tval_acc: 1.000000 val_loss: 0.0130906\n",
      "5_751 train_acc: 0.9513 train_loss: 0.134022\tval_acc: 1.000000 val_loss: 0.0106422\n",
      "5_794 train_acc: 0.9139 train_loss: 0.221664\tval_acc: 1.000000 val_loss: 0.0098669\n",
      "5_845 train_acc: 0.9288 train_loss: 0.185730\tval_acc: 1.000000 val_loss: 0.0092757\n",
      "5_889 train_acc: 0.9101 train_loss: 0.184478\tval_acc: 1.000000 val_loss: 0.0084286\n",
      "5_892 train_acc: 0.9288 train_loss: 0.186921\tval_acc: 1.000000 val_loss: 0.0075488\n",
      "5_981 train_acc: 0.9476 train_loss: 0.169045\tval_acc: 1.000000 val_loss: 0.0070853\n",
      "epoch:  981 \tThe test accuracy is: 0.7881944444444444\n",
      " THE BEST ACCURACY IS 0.7881944444444444\tkappa is 0.7175925925925926\n",
      "subject 5 duration: 0:09:33.180147\n",
      "seed is 1186\n",
      "Subject 6\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "6_0 train_acc: 0.2210 train_loss: 2.284434\tval_acc: 0.254902 val_loss: 1.4334168\n",
      "6_1 train_acc: 0.2734 train_loss: 1.999354\tval_acc: 0.318627 val_loss: 1.3948333\n",
      "6_3 train_acc: 0.2846 train_loss: 1.806758\tval_acc: 0.343137 val_loss: 1.3392043\n",
      "6_4 train_acc: 0.3296 train_loss: 1.605777\tval_acc: 0.411765 val_loss: 1.2832208\n",
      "6_5 train_acc: 0.3670 train_loss: 1.535790\tval_acc: 0.397059 val_loss: 1.2605865\n",
      "6_6 train_acc: 0.3184 train_loss: 1.584506\tval_acc: 0.583333 val_loss: 1.1857342\n",
      "6_7 train_acc: 0.3258 train_loss: 1.474590\tval_acc: 0.637255 val_loss: 1.1525332\n",
      "6_8 train_acc: 0.3558 train_loss: 1.460693\tval_acc: 0.651961 val_loss: 1.1087055\n",
      "6_9 train_acc: 0.3596 train_loss: 1.393030\tval_acc: 0.627451 val_loss: 1.0904220\n",
      "6_10 train_acc: 0.3933 train_loss: 1.357926\tval_acc: 0.666667 val_loss: 1.0444264\n",
      "6_11 train_acc: 0.3783 train_loss: 1.361700\tval_acc: 0.681373 val_loss: 1.0189334\n",
      "6_12 train_acc: 0.5131 train_loss: 1.215238\tval_acc: 0.686275 val_loss: 0.9853967\n",
      "6_14 train_acc: 0.4494 train_loss: 1.273324\tval_acc: 0.696078 val_loss: 0.9535684\n",
      "6_16 train_acc: 0.4869 train_loss: 1.180765\tval_acc: 0.715686 val_loss: 0.9053888\n",
      "6_18 train_acc: 0.4981 train_loss: 1.146311\tval_acc: 0.735294 val_loss: 0.8811862\n",
      "6_19 train_acc: 0.5506 train_loss: 1.094674\tval_acc: 0.725490 val_loss: 0.8481228\n",
      "6_22 train_acc: 0.5843 train_loss: 1.006090\tval_acc: 0.730392 val_loss: 0.8370526\n",
      "6_26 train_acc: 0.5693 train_loss: 1.007675\tval_acc: 0.769608 val_loss: 0.7936605\n",
      "6_28 train_acc: 0.5993 train_loss: 0.946209\tval_acc: 0.754902 val_loss: 0.7709668\n",
      "6_29 train_acc: 0.6517 train_loss: 0.919266\tval_acc: 0.774510 val_loss: 0.7366532\n",
      "6_33 train_acc: 0.5918 train_loss: 0.954441\tval_acc: 0.774510 val_loss: 0.7229154\n",
      "6_34 train_acc: 0.6629 train_loss: 0.827234\tval_acc: 0.794118 val_loss: 0.6912240\n",
      "6_35 train_acc: 0.5955 train_loss: 0.944282\tval_acc: 0.803922 val_loss: 0.6810719\n",
      "6_36 train_acc: 0.6105 train_loss: 0.928227\tval_acc: 0.818627 val_loss: 0.6643147\n",
      "6_39 train_acc: 0.6442 train_loss: 0.850683\tval_acc: 0.828431 val_loss: 0.6625463\n",
      "6_40 train_acc: 0.6929 train_loss: 0.842492\tval_acc: 0.823529 val_loss: 0.6407700\n",
      "6_42 train_acc: 0.6667 train_loss: 0.843674\tval_acc: 0.833333 val_loss: 0.5953478\n",
      "6_44 train_acc: 0.6517 train_loss: 0.853624\tval_acc: 0.818627 val_loss: 0.5569388\n",
      "6_48 train_acc: 0.6292 train_loss: 0.853070\tval_acc: 0.862745 val_loss: 0.5405524\n",
      "6_50 train_acc: 0.6929 train_loss: 0.815498\tval_acc: 0.843137 val_loss: 0.5300446\n",
      "6_52 train_acc: 0.6891 train_loss: 0.722702\tval_acc: 0.833333 val_loss: 0.5289208\n",
      "6_53 train_acc: 0.7228 train_loss: 0.698500\tval_acc: 0.862745 val_loss: 0.4627297\n",
      "6_62 train_acc: 0.7491 train_loss: 0.710806\tval_acc: 0.897059 val_loss: 0.4256758\n",
      "6_68 train_acc: 0.7004 train_loss: 0.741535\tval_acc: 0.897059 val_loss: 0.4078648\n",
      "6_72 train_acc: 0.8052 train_loss: 0.545310\tval_acc: 0.877451 val_loss: 0.3938156\n",
      "6_80 train_acc: 0.7715 train_loss: 0.565629\tval_acc: 0.897059 val_loss: 0.3910984\n",
      "6_82 train_acc: 0.7903 train_loss: 0.565049\tval_acc: 0.892157 val_loss: 0.3861506\n",
      "6_85 train_acc: 0.7228 train_loss: 0.725823\tval_acc: 0.887255 val_loss: 0.3638798\n",
      "6_90 train_acc: 0.7491 train_loss: 0.611443\tval_acc: 0.882353 val_loss: 0.3607599\n",
      "6_91 train_acc: 0.8202 train_loss: 0.540097\tval_acc: 0.887255 val_loss: 0.3331091\n",
      "6_96 train_acc: 0.8090 train_loss: 0.458548\tval_acc: 0.906863 val_loss: 0.3196442\n",
      "6_97 train_acc: 0.7978 train_loss: 0.512837\tval_acc: 0.911765 val_loss: 0.3137059\n",
      "6_98 train_acc: 0.7528 train_loss: 0.691525\tval_acc: 0.901961 val_loss: 0.3109917\n",
      "6_100 train_acc: 0.7566 train_loss: 0.622970\tval_acc: 0.921569 val_loss: 0.2850286\n",
      "6_101 train_acc: 0.7416 train_loss: 0.626989\tval_acc: 0.911765 val_loss: 0.2848302\n",
      "6_121 train_acc: 0.7416 train_loss: 0.628195\tval_acc: 0.906863 val_loss: 0.2707711\n",
      "6_124 train_acc: 0.7790 train_loss: 0.518016\tval_acc: 0.941176 val_loss: 0.2235447\n",
      "6_137 train_acc: 0.7828 train_loss: 0.555401\tval_acc: 0.960784 val_loss: 0.2230909\n",
      "6_153 train_acc: 0.8127 train_loss: 0.449973\tval_acc: 0.926471 val_loss: 0.2070633\n",
      "6_169 train_acc: 0.7828 train_loss: 0.621410\tval_acc: 0.926471 val_loss: 0.2037766\n",
      "6_177 train_acc: 0.8015 train_loss: 0.503878\tval_acc: 0.950980 val_loss: 0.1725095\n",
      "6_196 train_acc: 0.8502 train_loss: 0.373255\tval_acc: 0.960784 val_loss: 0.1696209\n",
      "6_198 train_acc: 0.8315 train_loss: 0.437700\tval_acc: 0.936275 val_loss: 0.1665546\n",
      "6_219 train_acc: 0.7940 train_loss: 0.508288\tval_acc: 0.950980 val_loss: 0.1521906\n",
      "6_238 train_acc: 0.8614 train_loss: 0.386740\tval_acc: 0.950980 val_loss: 0.1472356\n",
      "6_248 train_acc: 0.8315 train_loss: 0.444328\tval_acc: 0.965686 val_loss: 0.1355363\n",
      "6_249 train_acc: 0.8240 train_loss: 0.470543\tval_acc: 0.970588 val_loss: 0.1330657\n",
      "6_257 train_acc: 0.8727 train_loss: 0.346106\tval_acc: 0.970588 val_loss: 0.1217543\n",
      "6_289 train_acc: 0.8539 train_loss: 0.405489\tval_acc: 0.970588 val_loss: 0.1149475\n",
      "6_325 train_acc: 0.8652 train_loss: 0.382197\tval_acc: 0.965686 val_loss: 0.1131278\n",
      "6_347 train_acc: 0.8764 train_loss: 0.361480\tval_acc: 0.975490 val_loss: 0.0991405\n",
      "6_357 train_acc: 0.8614 train_loss: 0.373424\tval_acc: 0.970588 val_loss: 0.0917026\n",
      "6_389 train_acc: 0.8390 train_loss: 0.471228\tval_acc: 0.975490 val_loss: 0.0916065\n",
      "6_414 train_acc: 0.8614 train_loss: 0.374317\tval_acc: 0.970588 val_loss: 0.0878005\n",
      "6_424 train_acc: 0.8764 train_loss: 0.328097\tval_acc: 0.980392 val_loss: 0.0799298\n",
      "6_428 train_acc: 0.8539 train_loss: 0.391838\tval_acc: 0.990196 val_loss: 0.0706643\n",
      "6_450 train_acc: 0.8577 train_loss: 0.358208\tval_acc: 0.980392 val_loss: 0.0665087\n",
      "6_455 train_acc: 0.8801 train_loss: 0.359008\tval_acc: 0.985294 val_loss: 0.0664130\n",
      "6_511 train_acc: 0.8689 train_loss: 0.361236\tval_acc: 0.985294 val_loss: 0.0633391\n",
      "6_520 train_acc: 0.8727 train_loss: 0.331970\tval_acc: 0.980392 val_loss: 0.0559195\n",
      "6_554 train_acc: 0.8914 train_loss: 0.338403\tval_acc: 0.980392 val_loss: 0.0542505\n",
      "6_582 train_acc: 0.8989 train_loss: 0.267404\tval_acc: 0.995098 val_loss: 0.0522750\n",
      "6_666 train_acc: 0.8914 train_loss: 0.319066\tval_acc: 0.995098 val_loss: 0.0478976\n",
      "6_718 train_acc: 0.9213 train_loss: 0.271632\tval_acc: 0.990196 val_loss: 0.0426092\n",
      "6_728 train_acc: 0.9139 train_loss: 0.227378\tval_acc: 0.990196 val_loss: 0.0422098\n",
      "6_747 train_acc: 0.8989 train_loss: 0.305557\tval_acc: 0.995098 val_loss: 0.0372429\n",
      "6_753 train_acc: 0.8727 train_loss: 0.349458\tval_acc: 0.990196 val_loss: 0.0355098\n",
      "6_811 train_acc: 0.9213 train_loss: 0.219723\tval_acc: 0.990196 val_loss: 0.0341437\n",
      "6_812 train_acc: 0.9251 train_loss: 0.237501\tval_acc: 0.995098 val_loss: 0.0332297\n",
      "6_818 train_acc: 0.8727 train_loss: 0.329245\tval_acc: 1.000000 val_loss: 0.0262237\n",
      "6_882 train_acc: 0.8727 train_loss: 0.320560\tval_acc: 1.000000 val_loss: 0.0256110\n",
      "6_991 train_acc: 0.9288 train_loss: 0.197952\tval_acc: 1.000000 val_loss: 0.0244278\n",
      "epoch:  991 \tThe test accuracy is: 0.7222222222222222\n",
      " THE BEST ACCURACY IS 0.7222222222222222\tkappa is 0.6296296296296297\n",
      "subject 6 duration: 0:09:32.486386\n",
      "seed is 1510\n",
      "Subject 7\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "7_0 train_acc: 0.2509 train_loss: 2.510489\tval_acc: 0.264706 val_loss: 1.4254644\n",
      "7_1 train_acc: 0.3446 train_loss: 2.024120\tval_acc: 0.416667 val_loss: 1.3295462\n",
      "7_2 train_acc: 0.3333 train_loss: 1.764425\tval_acc: 0.421569 val_loss: 1.3021860\n",
      "7_3 train_acc: 0.3408 train_loss: 1.707322\tval_acc: 0.500000 val_loss: 1.1870453\n",
      "7_4 train_acc: 0.3933 train_loss: 1.567361\tval_acc: 0.495098 val_loss: 1.1851360\n",
      "7_5 train_acc: 0.3858 train_loss: 1.520208\tval_acc: 0.612745 val_loss: 1.0347955\n",
      "7_6 train_acc: 0.3670 train_loss: 1.492638\tval_acc: 0.622549 val_loss: 0.9569705\n",
      "7_7 train_acc: 0.4607 train_loss: 1.450167\tval_acc: 0.700980 val_loss: 0.9160158\n",
      "7_8 train_acc: 0.5431 train_loss: 1.184743\tval_acc: 0.700980 val_loss: 0.8667710\n",
      "7_9 train_acc: 0.4494 train_loss: 1.327449\tval_acc: 0.769608 val_loss: 0.7980086\n",
      "7_10 train_acc: 0.5543 train_loss: 1.108948\tval_acc: 0.803922 val_loss: 0.6985895\n",
      "7_12 train_acc: 0.5356 train_loss: 1.036421\tval_acc: 0.818627 val_loss: 0.6563533\n",
      "7_14 train_acc: 0.6629 train_loss: 0.890243\tval_acc: 0.808824 val_loss: 0.6263121\n",
      "7_15 train_acc: 0.5581 train_loss: 1.021096\tval_acc: 0.799020 val_loss: 0.6135132\n",
      "7_16 train_acc: 0.6367 train_loss: 1.003587\tval_acc: 0.789216 val_loss: 0.6130092\n",
      "7_17 train_acc: 0.6704 train_loss: 0.910277\tval_acc: 0.799020 val_loss: 0.6113523\n",
      "7_18 train_acc: 0.6330 train_loss: 0.946863\tval_acc: 0.838235 val_loss: 0.5102198\n",
      "7_23 train_acc: 0.6592 train_loss: 0.877511\tval_acc: 0.828431 val_loss: 0.5080959\n",
      "7_24 train_acc: 0.7116 train_loss: 0.747382\tval_acc: 0.833333 val_loss: 0.4790730\n",
      "7_25 train_acc: 0.7228 train_loss: 0.669780\tval_acc: 0.813725 val_loss: 0.4779500\n",
      "7_28 train_acc: 0.6816 train_loss: 0.707306\tval_acc: 0.833333 val_loss: 0.4696126\n",
      "7_30 train_acc: 0.6779 train_loss: 0.864297\tval_acc: 0.843137 val_loss: 0.4468428\n",
      "7_32 train_acc: 0.6629 train_loss: 0.790044\tval_acc: 0.838235 val_loss: 0.4090988\n",
      "7_40 train_acc: 0.7266 train_loss: 0.692008\tval_acc: 0.852941 val_loss: 0.3802546\n",
      "7_46 train_acc: 0.7678 train_loss: 0.606315\tval_acc: 0.838235 val_loss: 0.3798698\n",
      "7_47 train_acc: 0.7378 train_loss: 0.702678\tval_acc: 0.852941 val_loss: 0.3651575\n",
      "7_48 train_acc: 0.7865 train_loss: 0.573026\tval_acc: 0.848039 val_loss: 0.3423306\n",
      "7_55 train_acc: 0.7640 train_loss: 0.565982\tval_acc: 0.852941 val_loss: 0.3391364\n",
      "7_56 train_acc: 0.7004 train_loss: 0.673674\tval_acc: 0.872549 val_loss: 0.3237857\n",
      "7_57 train_acc: 0.8090 train_loss: 0.559996\tval_acc: 0.882353 val_loss: 0.3186705\n",
      "7_63 train_acc: 0.7828 train_loss: 0.618829\tval_acc: 0.872549 val_loss: 0.2979994\n",
      "7_67 train_acc: 0.7753 train_loss: 0.585492\tval_acc: 0.887255 val_loss: 0.2691710\n",
      "7_69 train_acc: 0.7978 train_loss: 0.540636\tval_acc: 0.906863 val_loss: 0.2665585\n",
      "7_74 train_acc: 0.7903 train_loss: 0.550578\tval_acc: 0.911765 val_loss: 0.2564472\n",
      "7_81 train_acc: 0.7903 train_loss: 0.519213\tval_acc: 0.916667 val_loss: 0.2441703\n",
      "7_89 train_acc: 0.8127 train_loss: 0.507421\tval_acc: 0.911765 val_loss: 0.2266999\n",
      "7_98 train_acc: 0.8165 train_loss: 0.466668\tval_acc: 0.921569 val_loss: 0.2249959\n",
      "7_100 train_acc: 0.8090 train_loss: 0.489037\tval_acc: 0.921569 val_loss: 0.2075118\n",
      "7_113 train_acc: 0.8727 train_loss: 0.379544\tval_acc: 0.936275 val_loss: 0.1963727\n",
      "7_118 train_acc: 0.8052 train_loss: 0.476593\tval_acc: 0.941176 val_loss: 0.1882630\n",
      "7_129 train_acc: 0.8502 train_loss: 0.407899\tval_acc: 0.936275 val_loss: 0.1864137\n",
      "7_134 train_acc: 0.8464 train_loss: 0.416843\tval_acc: 0.936275 val_loss: 0.1845901\n",
      "7_138 train_acc: 0.8876 train_loss: 0.330789\tval_acc: 0.950980 val_loss: 0.1704066\n",
      "7_145 train_acc: 0.9026 train_loss: 0.303396\tval_acc: 0.936275 val_loss: 0.1683370\n",
      "7_147 train_acc: 0.8352 train_loss: 0.395239\tval_acc: 0.936275 val_loss: 0.1610067\n",
      "7_151 train_acc: 0.8801 train_loss: 0.368131\tval_acc: 0.960784 val_loss: 0.1421592\n",
      "7_156 train_acc: 0.8277 train_loss: 0.501958\tval_acc: 0.960784 val_loss: 0.1402131\n",
      "7_184 train_acc: 0.8277 train_loss: 0.441078\tval_acc: 0.970588 val_loss: 0.1366517\n",
      "7_189 train_acc: 0.8652 train_loss: 0.365811\tval_acc: 0.950980 val_loss: 0.1345855\n",
      "7_191 train_acc: 0.8951 train_loss: 0.301781\tval_acc: 0.960784 val_loss: 0.1206986\n",
      "7_214 train_acc: 0.8839 train_loss: 0.284943\tval_acc: 0.970588 val_loss: 0.1200624\n",
      "7_228 train_acc: 0.8989 train_loss: 0.280255\tval_acc: 0.970588 val_loss: 0.1196345\n",
      "7_235 train_acc: 0.8951 train_loss: 0.266991\tval_acc: 0.960784 val_loss: 0.1068115\n",
      "7_271 train_acc: 0.9064 train_loss: 0.291417\tval_acc: 0.965686 val_loss: 0.1050058\n",
      "7_273 train_acc: 0.8801 train_loss: 0.328429\tval_acc: 0.965686 val_loss: 0.1034843\n",
      "7_282 train_acc: 0.8614 train_loss: 0.320033\tval_acc: 0.965686 val_loss: 0.0949385\n",
      "7_288 train_acc: 0.8951 train_loss: 0.285703\tval_acc: 0.980392 val_loss: 0.0923032\n",
      "7_304 train_acc: 0.8839 train_loss: 0.308437\tval_acc: 0.985294 val_loss: 0.0762172\n",
      "7_356 train_acc: 0.9326 train_loss: 0.199420\tval_acc: 0.975490 val_loss: 0.0721780\n",
      "7_361 train_acc: 0.8989 train_loss: 0.273432\tval_acc: 0.970588 val_loss: 0.0687259\n",
      "7_372 train_acc: 0.8914 train_loss: 0.249641\tval_acc: 0.970588 val_loss: 0.0679877\n",
      "7_378 train_acc: 0.8914 train_loss: 0.347047\tval_acc: 0.985294 val_loss: 0.0641742\n",
      "7_386 train_acc: 0.9064 train_loss: 0.233009\tval_acc: 0.985294 val_loss: 0.0591874\n",
      "7_387 train_acc: 0.8951 train_loss: 0.269973\tval_acc: 0.985294 val_loss: 0.0591668\n",
      "7_400 train_acc: 0.9213 train_loss: 0.271543\tval_acc: 0.990196 val_loss: 0.0490872\n",
      "7_422 train_acc: 0.9064 train_loss: 0.258004\tval_acc: 0.990196 val_loss: 0.0425088\n",
      "7_446 train_acc: 0.8839 train_loss: 0.280118\tval_acc: 0.990196 val_loss: 0.0365421\n",
      "7_490 train_acc: 0.9438 train_loss: 0.137931\tval_acc: 0.990196 val_loss: 0.0364898\n",
      "7_497 train_acc: 0.9438 train_loss: 0.200263\tval_acc: 0.990196 val_loss: 0.0282706\n",
      "7_517 train_acc: 0.9438 train_loss: 0.134205\tval_acc: 0.995098 val_loss: 0.0261149\n",
      "7_523 train_acc: 0.9551 train_loss: 0.114610\tval_acc: 1.000000 val_loss: 0.0137471\n",
      "7_546 train_acc: 0.9625 train_loss: 0.097593\tval_acc: 1.000000 val_loss: 0.0117559\n",
      "7_564 train_acc: 0.9775 train_loss: 0.088392\tval_acc: 1.000000 val_loss: 0.0116951\n",
      "7_571 train_acc: 0.9625 train_loss: 0.105241\tval_acc: 1.000000 val_loss: 0.0115051\n",
      "7_573 train_acc: 0.9625 train_loss: 0.098429\tval_acc: 1.000000 val_loss: 0.0109807\n",
      "7_578 train_acc: 0.9438 train_loss: 0.150420\tval_acc: 1.000000 val_loss: 0.0081882\n",
      "7_581 train_acc: 0.9663 train_loss: 0.106245\tval_acc: 1.000000 val_loss: 0.0081150\n",
      "7_608 train_acc: 0.9513 train_loss: 0.112634\tval_acc: 1.000000 val_loss: 0.0058594\n",
      "7_642 train_acc: 0.9513 train_loss: 0.130471\tval_acc: 1.000000 val_loss: 0.0057504\n",
      "7_650 train_acc: 0.9625 train_loss: 0.081074\tval_acc: 1.000000 val_loss: 0.0046799\n",
      "7_661 train_acc: 0.9663 train_loss: 0.065271\tval_acc: 1.000000 val_loss: 0.0044759\n",
      "7_669 train_acc: 0.9813 train_loss: 0.059016\tval_acc: 1.000000 val_loss: 0.0040592\n",
      "7_674 train_acc: 0.9813 train_loss: 0.063663\tval_acc: 1.000000 val_loss: 0.0038526\n",
      "7_677 train_acc: 0.9476 train_loss: 0.110811\tval_acc: 1.000000 val_loss: 0.0037900\n",
      "7_680 train_acc: 0.9700 train_loss: 0.114082\tval_acc: 1.000000 val_loss: 0.0032209\n",
      "7_682 train_acc: 0.9775 train_loss: 0.061740\tval_acc: 1.000000 val_loss: 0.0030093\n",
      "7_699 train_acc: 0.9738 train_loss: 0.071333\tval_acc: 1.000000 val_loss: 0.0026234\n",
      "7_704 train_acc: 0.9700 train_loss: 0.098630\tval_acc: 1.000000 val_loss: 0.0025512\n",
      "7_718 train_acc: 0.9551 train_loss: 0.131975\tval_acc: 1.000000 val_loss: 0.0023008\n",
      "7_723 train_acc: 0.9700 train_loss: 0.061003\tval_acc: 1.000000 val_loss: 0.0022962\n",
      "7_757 train_acc: 0.9625 train_loss: 0.124735\tval_acc: 1.000000 val_loss: 0.0022368\n",
      "7_768 train_acc: 0.9813 train_loss: 0.062558\tval_acc: 1.000000 val_loss: 0.0017139\n",
      "7_817 train_acc: 0.9775 train_loss: 0.053594\tval_acc: 1.000000 val_loss: 0.0014120\n",
      "7_824 train_acc: 0.9775 train_loss: 0.053882\tval_acc: 1.000000 val_loss: 0.0012115\n",
      "7_831 train_acc: 0.9663 train_loss: 0.104995\tval_acc: 1.000000 val_loss: 0.0010157\n",
      "7_895 train_acc: 0.9813 train_loss: 0.045914\tval_acc: 1.000000 val_loss: 0.0008712\n",
      "7_945 train_acc: 0.9663 train_loss: 0.080331\tval_acc: 1.000000 val_loss: 0.0007162\n",
      "epoch:  945 \tThe test accuracy is: 0.9479166666666666\n",
      " THE BEST ACCURACY IS 0.9479166666666666\tkappa is 0.9305555555555556\n",
      "subject 7 duration: 0:09:32.286174\n",
      "seed is 770\n",
      "Subject 8\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "8_0 train_acc: 0.2547 train_loss: 2.412762\tval_acc: 0.289216 val_loss: 1.4232248\n",
      "8_1 train_acc: 0.2659 train_loss: 2.034042\tval_acc: 0.289216 val_loss: 1.3860432\n",
      "8_2 train_acc: 0.2846 train_loss: 1.975800\tval_acc: 0.348039 val_loss: 1.3672673\n",
      "8_3 train_acc: 0.3258 train_loss: 1.779863\tval_acc: 0.441176 val_loss: 1.3313034\n",
      "8_4 train_acc: 0.3708 train_loss: 1.552726\tval_acc: 0.401961 val_loss: 1.2778510\n",
      "8_5 train_acc: 0.3596 train_loss: 1.620326\tval_acc: 0.431373 val_loss: 1.2506540\n",
      "8_6 train_acc: 0.3596 train_loss: 1.538225\tval_acc: 0.534314 val_loss: 1.1708972\n",
      "8_7 train_acc: 0.4007 train_loss: 1.394801\tval_acc: 0.563725 val_loss: 1.1103994\n",
      "8_8 train_acc: 0.3820 train_loss: 1.407785\tval_acc: 0.617647 val_loss: 1.0068818\n",
      "8_9 train_acc: 0.4195 train_loss: 1.394752\tval_acc: 0.676471 val_loss: 0.9658028\n",
      "8_10 train_acc: 0.4457 train_loss: 1.356559\tval_acc: 0.710784 val_loss: 0.8884289\n",
      "8_12 train_acc: 0.5169 train_loss: 1.123248\tval_acc: 0.740196 val_loss: 0.8504232\n",
      "8_13 train_acc: 0.4906 train_loss: 1.213466\tval_acc: 0.715686 val_loss: 0.8475334\n",
      "8_14 train_acc: 0.5581 train_loss: 1.056805\tval_acc: 0.759804 val_loss: 0.7966906\n",
      "8_15 train_acc: 0.5506 train_loss: 1.118251\tval_acc: 0.754902 val_loss: 0.7806827\n",
      "8_16 train_acc: 0.5805 train_loss: 0.991715\tval_acc: 0.779412 val_loss: 0.7662812\n",
      "8_17 train_acc: 0.6217 train_loss: 0.912119\tval_acc: 0.764706 val_loss: 0.7420112\n",
      "8_19 train_acc: 0.5918 train_loss: 1.001658\tval_acc: 0.813725 val_loss: 0.6700404\n",
      "8_22 train_acc: 0.6404 train_loss: 0.835675\tval_acc: 0.833333 val_loss: 0.6265970\n",
      "8_23 train_acc: 0.6929 train_loss: 0.741534\tval_acc: 0.848039 val_loss: 0.5950484\n",
      "8_24 train_acc: 0.7154 train_loss: 0.701994\tval_acc: 0.833333 val_loss: 0.5924762\n",
      "8_25 train_acc: 0.5993 train_loss: 0.877603\tval_acc: 0.828431 val_loss: 0.5671245\n",
      "8_26 train_acc: 0.7154 train_loss: 0.725416\tval_acc: 0.828431 val_loss: 0.5293387\n",
      "8_27 train_acc: 0.6667 train_loss: 0.751617\tval_acc: 0.848039 val_loss: 0.5289733\n",
      "8_28 train_acc: 0.7116 train_loss: 0.724441\tval_acc: 0.882353 val_loss: 0.4831749\n",
      "8_31 train_acc: 0.7416 train_loss: 0.648777\tval_acc: 0.843137 val_loss: 0.4742952\n",
      "8_33 train_acc: 0.7154 train_loss: 0.674860\tval_acc: 0.857843 val_loss: 0.4450738\n",
      "8_34 train_acc: 0.7266 train_loss: 0.653819\tval_acc: 0.867647 val_loss: 0.4400277\n",
      "8_35 train_acc: 0.7828 train_loss: 0.621092\tval_acc: 0.882353 val_loss: 0.3728104\n",
      "8_39 train_acc: 0.7640 train_loss: 0.559127\tval_acc: 0.892157 val_loss: 0.3474980\n",
      "8_41 train_acc: 0.7678 train_loss: 0.575376\tval_acc: 0.911765 val_loss: 0.3194393\n",
      "8_48 train_acc: 0.7978 train_loss: 0.532977\tval_acc: 0.921569 val_loss: 0.3148946\n",
      "8_50 train_acc: 0.7753 train_loss: 0.516258\tval_acc: 0.936275 val_loss: 0.3012288\n",
      "8_52 train_acc: 0.7640 train_loss: 0.538756\tval_acc: 0.931373 val_loss: 0.2815222\n",
      "8_53 train_acc: 0.7978 train_loss: 0.526735\tval_acc: 0.921569 val_loss: 0.2759861\n",
      "8_57 train_acc: 0.8052 train_loss: 0.515061\tval_acc: 0.931373 val_loss: 0.2709653\n",
      "8_61 train_acc: 0.8202 train_loss: 0.470031\tval_acc: 0.921569 val_loss: 0.2661029\n",
      "8_64 train_acc: 0.8277 train_loss: 0.394619\tval_acc: 0.921569 val_loss: 0.2652107\n",
      "8_65 train_acc: 0.8315 train_loss: 0.448448\tval_acc: 0.941176 val_loss: 0.2642524\n",
      "8_66 train_acc: 0.8390 train_loss: 0.372597\tval_acc: 0.916667 val_loss: 0.2386797\n",
      "8_68 train_acc: 0.7903 train_loss: 0.562491\tval_acc: 0.936275 val_loss: 0.2350171\n",
      "8_69 train_acc: 0.8390 train_loss: 0.438557\tval_acc: 0.931373 val_loss: 0.2319451\n",
      "8_71 train_acc: 0.8352 train_loss: 0.414153\tval_acc: 0.936275 val_loss: 0.2275721\n",
      "8_74 train_acc: 0.8464 train_loss: 0.442929\tval_acc: 0.921569 val_loss: 0.2173260\n",
      "8_77 train_acc: 0.8427 train_loss: 0.469686\tval_acc: 0.941176 val_loss: 0.2144350\n",
      "8_78 train_acc: 0.8277 train_loss: 0.433169\tval_acc: 0.941176 val_loss: 0.2105380\n",
      "8_83 train_acc: 0.8052 train_loss: 0.475354\tval_acc: 0.931373 val_loss: 0.2065164\n",
      "8_84 train_acc: 0.8015 train_loss: 0.484100\tval_acc: 0.941176 val_loss: 0.1871529\n",
      "8_88 train_acc: 0.8315 train_loss: 0.402144\tval_acc: 0.936275 val_loss: 0.1854436\n",
      "8_90 train_acc: 0.8240 train_loss: 0.417003\tval_acc: 0.941176 val_loss: 0.1795835\n",
      "8_92 train_acc: 0.8165 train_loss: 0.457921\tval_acc: 0.960784 val_loss: 0.1753668\n",
      "8_94 train_acc: 0.8427 train_loss: 0.393742\tval_acc: 0.960784 val_loss: 0.1751215\n",
      "8_96 train_acc: 0.8127 train_loss: 0.451593\tval_acc: 0.965686 val_loss: 0.1620616\n",
      "8_101 train_acc: 0.8689 train_loss: 0.408175\tval_acc: 0.965686 val_loss: 0.1454193\n",
      "8_109 train_acc: 0.8577 train_loss: 0.346646\tval_acc: 0.960784 val_loss: 0.1411659\n",
      "8_115 train_acc: 0.8727 train_loss: 0.318458\tval_acc: 0.970588 val_loss: 0.1207301\n",
      "8_117 train_acc: 0.8652 train_loss: 0.354180\tval_acc: 0.980392 val_loss: 0.0995409\n",
      "8_139 train_acc: 0.8502 train_loss: 0.343127\tval_acc: 0.990196 val_loss: 0.0951788\n",
      "8_144 train_acc: 0.8652 train_loss: 0.346853\tval_acc: 0.995098 val_loss: 0.0760101\n",
      "8_162 train_acc: 0.8876 train_loss: 0.262132\tval_acc: 0.975490 val_loss: 0.0707806\n",
      "8_189 train_acc: 0.8914 train_loss: 0.307723\tval_acc: 0.980392 val_loss: 0.0676937\n",
      "8_191 train_acc: 0.8689 train_loss: 0.346139\tval_acc: 0.990196 val_loss: 0.0579299\n",
      "8_213 train_acc: 0.9064 train_loss: 0.259469\tval_acc: 0.980392 val_loss: 0.0544537\n",
      "8_219 train_acc: 0.9139 train_loss: 0.211548\tval_acc: 0.990196 val_loss: 0.0539350\n",
      "8_220 train_acc: 0.9064 train_loss: 0.241401\tval_acc: 0.990196 val_loss: 0.0447833\n",
      "8_244 train_acc: 0.8876 train_loss: 0.306029\tval_acc: 0.995098 val_loss: 0.0401594\n",
      "8_264 train_acc: 0.9363 train_loss: 0.149456\tval_acc: 1.000000 val_loss: 0.0345250\n",
      "8_284 train_acc: 0.9139 train_loss: 0.244183\tval_acc: 0.990196 val_loss: 0.0328330\n",
      "8_301 train_acc: 0.9176 train_loss: 0.216625\tval_acc: 0.990196 val_loss: 0.0327818\n",
      "8_321 train_acc: 0.9401 train_loss: 0.176649\tval_acc: 0.990196 val_loss: 0.0282434\n",
      "8_344 train_acc: 0.9176 train_loss: 0.227384\tval_acc: 0.995098 val_loss: 0.0279079\n",
      "8_346 train_acc: 0.9551 train_loss: 0.150530\tval_acc: 0.990196 val_loss: 0.0265497\n",
      "8_367 train_acc: 0.9551 train_loss: 0.133291\tval_acc: 0.995098 val_loss: 0.0211103\n",
      "8_376 train_acc: 0.9476 train_loss: 0.150963\tval_acc: 1.000000 val_loss: 0.0200849\n",
      "8_383 train_acc: 0.9663 train_loss: 0.106624\tval_acc: 1.000000 val_loss: 0.0183121\n",
      "8_402 train_acc: 0.9476 train_loss: 0.151636\tval_acc: 1.000000 val_loss: 0.0170583\n",
      "8_420 train_acc: 0.9288 train_loss: 0.180849\tval_acc: 1.000000 val_loss: 0.0129508\n",
      "8_425 train_acc: 0.9438 train_loss: 0.268722\tval_acc: 1.000000 val_loss: 0.0124680\n",
      "8_438 train_acc: 0.9513 train_loss: 0.174332\tval_acc: 1.000000 val_loss: 0.0105832\n",
      "8_501 train_acc: 0.9551 train_loss: 0.148803\tval_acc: 1.000000 val_loss: 0.0100448\n",
      "8_528 train_acc: 0.9551 train_loss: 0.140096\tval_acc: 1.000000 val_loss: 0.0093871\n",
      "8_550 train_acc: 0.9176 train_loss: 0.249541\tval_acc: 1.000000 val_loss: 0.0073805\n",
      "8_575 train_acc: 0.9476 train_loss: 0.172045\tval_acc: 1.000000 val_loss: 0.0069758\n",
      "8_588 train_acc: 0.9850 train_loss: 0.059798\tval_acc: 1.000000 val_loss: 0.0068981\n",
      "8_599 train_acc: 0.9401 train_loss: 0.121867\tval_acc: 1.000000 val_loss: 0.0064029\n",
      "8_629 train_acc: 0.9401 train_loss: 0.158296\tval_acc: 1.000000 val_loss: 0.0062143\n",
      "8_637 train_acc: 0.9476 train_loss: 0.120179\tval_acc: 1.000000 val_loss: 0.0057312\n",
      "8_640 train_acc: 0.9551 train_loss: 0.128012\tval_acc: 1.000000 val_loss: 0.0046349\n",
      "8_663 train_acc: 0.9476 train_loss: 0.167422\tval_acc: 1.000000 val_loss: 0.0042705\n",
      "8_681 train_acc: 0.9700 train_loss: 0.087753\tval_acc: 1.000000 val_loss: 0.0041536\n",
      "8_702 train_acc: 0.9663 train_loss: 0.085309\tval_acc: 1.000000 val_loss: 0.0037676\n",
      "8_753 train_acc: 0.9625 train_loss: 0.099753\tval_acc: 1.000000 val_loss: 0.0026649\n",
      "8_802 train_acc: 0.9588 train_loss: 0.137998\tval_acc: 1.000000 val_loss: 0.0025747\n",
      "8_866 train_acc: 0.9700 train_loss: 0.090774\tval_acc: 1.000000 val_loss: 0.0022567\n",
      "8_910 train_acc: 0.9551 train_loss: 0.091903\tval_acc: 1.000000 val_loss: 0.0020425\n",
      "8_945 train_acc: 0.9700 train_loss: 0.092658\tval_acc: 1.000000 val_loss: 0.0018898\n",
      "epoch:  945 \tThe test accuracy is: 0.90625\n",
      " THE BEST ACCURACY IS 0.90625\tkappa is 0.875\n",
      "subject 8 duration: 0:09:32.783397\n",
      "seed is 1928\n",
      "Subject 9\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "9_0 train_acc: 0.2285 train_loss: 2.628043\tval_acc: 0.303922 val_loss: 1.3758446\n",
      "9_1 train_acc: 0.3071 train_loss: 1.906346\tval_acc: 0.338235 val_loss: 1.3614347\n",
      "9_2 train_acc: 0.2809 train_loss: 1.994021\tval_acc: 0.313725 val_loss: 1.3598844\n",
      "9_3 train_acc: 0.3783 train_loss: 1.727921\tval_acc: 0.426471 val_loss: 1.2614876\n",
      "9_4 train_acc: 0.3596 train_loss: 1.640852\tval_acc: 0.421569 val_loss: 1.2564613\n",
      "9_5 train_acc: 0.3820 train_loss: 1.488970\tval_acc: 0.598039 val_loss: 1.1194764\n",
      "9_6 train_acc: 0.3933 train_loss: 1.507566\tval_acc: 0.588235 val_loss: 1.1002395\n",
      "9_7 train_acc: 0.4307 train_loss: 1.410556\tval_acc: 0.632353 val_loss: 1.0155640\n",
      "9_8 train_acc: 0.4644 train_loss: 1.395786\tval_acc: 0.691176 val_loss: 0.9454230\n",
      "9_9 train_acc: 0.4644 train_loss: 1.301945\tval_acc: 0.789216 val_loss: 0.8338079\n",
      "9_10 train_acc: 0.4944 train_loss: 1.186302\tval_acc: 0.759804 val_loss: 0.7768723\n",
      "9_11 train_acc: 0.5019 train_loss: 1.137439\tval_acc: 0.750000 val_loss: 0.7407709\n",
      "9_12 train_acc: 0.5880 train_loss: 0.988356\tval_acc: 0.779412 val_loss: 0.6516508\n",
      "9_13 train_acc: 0.5618 train_loss: 1.144520\tval_acc: 0.799020 val_loss: 0.6275464\n",
      "9_14 train_acc: 0.6217 train_loss: 0.909457\tval_acc: 0.779412 val_loss: 0.6266236\n",
      "9_15 train_acc: 0.5993 train_loss: 0.875992\tval_acc: 0.794118 val_loss: 0.6042821\n",
      "9_16 train_acc: 0.6217 train_loss: 0.923354\tval_acc: 0.808824 val_loss: 0.5622348\n",
      "9_17 train_acc: 0.6592 train_loss: 0.955616\tval_acc: 0.784314 val_loss: 0.5282888\n",
      "9_20 train_acc: 0.6704 train_loss: 0.865045\tval_acc: 0.823529 val_loss: 0.4742576\n",
      "9_23 train_acc: 0.6629 train_loss: 0.783312\tval_acc: 0.843137 val_loss: 0.4740334\n",
      "9_25 train_acc: 0.6667 train_loss: 0.752842\tval_acc: 0.852941 val_loss: 0.4601323\n",
      "9_26 train_acc: 0.7041 train_loss: 0.766797\tval_acc: 0.833333 val_loss: 0.4304722\n",
      "9_27 train_acc: 0.7116 train_loss: 0.716534\tval_acc: 0.852941 val_loss: 0.4158174\n",
      "9_30 train_acc: 0.7341 train_loss: 0.680809\tval_acc: 0.857843 val_loss: 0.3642312\n",
      "9_37 train_acc: 0.7753 train_loss: 0.570176\tval_acc: 0.872549 val_loss: 0.3543945\n",
      "9_44 train_acc: 0.8015 train_loss: 0.471002\tval_acc: 0.877451 val_loss: 0.3369151\n",
      "9_47 train_acc: 0.7603 train_loss: 0.561266\tval_acc: 0.887255 val_loss: 0.3116082\n",
      "9_48 train_acc: 0.7715 train_loss: 0.583975\tval_acc: 0.887255 val_loss: 0.2869398\n",
      "9_52 train_acc: 0.8502 train_loss: 0.432635\tval_acc: 0.901961 val_loss: 0.2709474\n",
      "9_56 train_acc: 0.8090 train_loss: 0.459232\tval_acc: 0.916667 val_loss: 0.2506563\n",
      "9_64 train_acc: 0.7790 train_loss: 0.543725\tval_acc: 0.916667 val_loss: 0.2472669\n",
      "9_66 train_acc: 0.8052 train_loss: 0.484050\tval_acc: 0.926471 val_loss: 0.2210840\n",
      "9_68 train_acc: 0.8352 train_loss: 0.420672\tval_acc: 0.926471 val_loss: 0.2103753\n",
      "9_72 train_acc: 0.8689 train_loss: 0.434033\tval_acc: 0.926471 val_loss: 0.2067927\n",
      "9_73 train_acc: 0.8464 train_loss: 0.428429\tval_acc: 0.926471 val_loss: 0.1966024\n",
      "9_75 train_acc: 0.8652 train_loss: 0.377329\tval_acc: 0.936275 val_loss: 0.1944786\n",
      "9_77 train_acc: 0.8727 train_loss: 0.366704\tval_acc: 0.946078 val_loss: 0.1873435\n",
      "9_79 train_acc: 0.8652 train_loss: 0.349244\tval_acc: 0.941176 val_loss: 0.1843810\n",
      "9_82 train_acc: 0.8090 train_loss: 0.480058\tval_acc: 0.931373 val_loss: 0.1777638\n",
      "9_90 train_acc: 0.8727 train_loss: 0.357347\tval_acc: 0.931373 val_loss: 0.1740810\n",
      "9_92 train_acc: 0.8876 train_loss: 0.318065\tval_acc: 0.946078 val_loss: 0.1691734\n",
      "9_96 train_acc: 0.8689 train_loss: 0.331176\tval_acc: 0.965686 val_loss: 0.1449817\n",
      "9_108 train_acc: 0.8989 train_loss: 0.288847\tval_acc: 0.955882 val_loss: 0.1304865\n",
      "9_118 train_acc: 0.8539 train_loss: 0.339811\tval_acc: 0.965686 val_loss: 0.1198588\n",
      "9_130 train_acc: 0.8914 train_loss: 0.250630\tval_acc: 0.970588 val_loss: 0.1131866\n",
      "9_137 train_acc: 0.9176 train_loss: 0.258703\tval_acc: 0.955882 val_loss: 0.1102481\n",
      "9_141 train_acc: 0.8839 train_loss: 0.308320\tval_acc: 0.970588 val_loss: 0.1083872\n",
      "9_142 train_acc: 0.9064 train_loss: 0.284996\tval_acc: 0.975490 val_loss: 0.1046991\n",
      "9_146 train_acc: 0.9101 train_loss: 0.231831\tval_acc: 0.970588 val_loss: 0.1032896\n",
      "9_154 train_acc: 0.9064 train_loss: 0.308711\tval_acc: 0.980392 val_loss: 0.0815624\n",
      "9_167 train_acc: 0.8951 train_loss: 0.284137\tval_acc: 0.975490 val_loss: 0.0796068\n",
      "9_178 train_acc: 0.9176 train_loss: 0.219160\tval_acc: 0.975490 val_loss: 0.0774581\n",
      "9_183 train_acc: 0.8989 train_loss: 0.262466\tval_acc: 0.975490 val_loss: 0.0757168\n",
      "9_185 train_acc: 0.9176 train_loss: 0.206985\tval_acc: 0.975490 val_loss: 0.0717706\n",
      "9_189 train_acc: 0.8914 train_loss: 0.260687\tval_acc: 0.965686 val_loss: 0.0647855\n",
      "9_198 train_acc: 0.9288 train_loss: 0.204141\tval_acc: 0.985294 val_loss: 0.0499412\n",
      "9_218 train_acc: 0.8914 train_loss: 0.237329\tval_acc: 0.985294 val_loss: 0.0485298\n",
      "9_223 train_acc: 0.9101 train_loss: 0.217523\tval_acc: 0.980392 val_loss: 0.0472950\n",
      "9_225 train_acc: 0.8989 train_loss: 0.257359\tval_acc: 0.990196 val_loss: 0.0432757\n",
      "9_239 train_acc: 0.9251 train_loss: 0.192802\tval_acc: 0.980392 val_loss: 0.0424890\n",
      "9_245 train_acc: 0.9288 train_loss: 0.203386\tval_acc: 0.995098 val_loss: 0.0333959\n",
      "9_263 train_acc: 0.9401 train_loss: 0.182344\tval_acc: 0.990196 val_loss: 0.0306852\n",
      "9_265 train_acc: 0.9401 train_loss: 0.137068\tval_acc: 0.990196 val_loss: 0.0300157\n",
      "9_276 train_acc: 0.9251 train_loss: 0.204258\tval_acc: 0.995098 val_loss: 0.0264529\n",
      "9_288 train_acc: 0.9588 train_loss: 0.146267\tval_acc: 0.995098 val_loss: 0.0250430\n",
      "9_301 train_acc: 0.9401 train_loss: 0.162712\tval_acc: 1.000000 val_loss: 0.0206633\n",
      "9_338 train_acc: 0.9438 train_loss: 0.193731\tval_acc: 1.000000 val_loss: 0.0189735\n",
      "9_352 train_acc: 0.9438 train_loss: 0.152287\tval_acc: 1.000000 val_loss: 0.0179254\n",
      "9_368 train_acc: 0.9326 train_loss: 0.207581\tval_acc: 1.000000 val_loss: 0.0136858\n",
      "9_394 train_acc: 0.9663 train_loss: 0.094259\tval_acc: 1.000000 val_loss: 0.0107414\n",
      "9_418 train_acc: 0.9438 train_loss: 0.149483\tval_acc: 1.000000 val_loss: 0.0077912\n",
      "9_449 train_acc: 0.9738 train_loss: 0.100320\tval_acc: 1.000000 val_loss: 0.0066766\n",
      "9_490 train_acc: 0.9588 train_loss: 0.115309\tval_acc: 1.000000 val_loss: 0.0057936\n",
      "9_502 train_acc: 0.9700 train_loss: 0.088535\tval_acc: 1.000000 val_loss: 0.0054674\n",
      "9_521 train_acc: 0.9363 train_loss: 0.152894\tval_acc: 1.000000 val_loss: 0.0047721\n",
      "9_572 train_acc: 0.9625 train_loss: 0.101588\tval_acc: 1.000000 val_loss: 0.0047327\n",
      "9_583 train_acc: 0.9476 train_loss: 0.171963\tval_acc: 1.000000 val_loss: 0.0040793\n",
      "9_587 train_acc: 0.9700 train_loss: 0.079284\tval_acc: 1.000000 val_loss: 0.0035981\n",
      "9_588 train_acc: 0.9625 train_loss: 0.086549\tval_acc: 1.000000 val_loss: 0.0035122\n",
      "9_636 train_acc: 0.9700 train_loss: 0.111373\tval_acc: 1.000000 val_loss: 0.0025222\n",
      "9_692 train_acc: 0.9663 train_loss: 0.067338\tval_acc: 1.000000 val_loss: 0.0024719\n",
      "9_708 train_acc: 0.9625 train_loss: 0.084726\tval_acc: 1.000000 val_loss: 0.0024091\n",
      "9_725 train_acc: 0.9738 train_loss: 0.056113\tval_acc: 1.000000 val_loss: 0.0019169\n",
      "9_754 train_acc: 0.9738 train_loss: 0.050564\tval_acc: 1.000000 val_loss: 0.0018434\n",
      "9_791 train_acc: 0.9513 train_loss: 0.119097\tval_acc: 1.000000 val_loss: 0.0017620\n",
      "9_817 train_acc: 0.9588 train_loss: 0.118178\tval_acc: 1.000000 val_loss: 0.0016492\n",
      "9_860 train_acc: 0.9588 train_loss: 0.112471\tval_acc: 1.000000 val_loss: 0.0012418\n",
      "9_910 train_acc: 0.9513 train_loss: 0.122103\tval_acc: 1.000000 val_loss: 0.0011097\n",
      "9_926 train_acc: 0.9588 train_loss: 0.117114\tval_acc: 1.000000 val_loss: 0.0009379\n",
      "epoch:  926 \tThe test accuracy is: 0.8993055555555556\n",
      " THE BEST ACCURACY IS 0.8993055555555556\tkappa is 0.8657407407407407\n",
      "subject 9 duration: 0:09:32.419292\n",
      "**The average Best accuracy is: 86.57407407407408kappa is: 82.09876543209877\n",
      "\n",
      "best epochs:  [964, 959, 939, 989, 981, 991, 945, 945, 926]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     92.361111  92.649648  92.361111  92.344609  89.814815\n",
      "1     78.125000  78.890484  78.125000  78.092546  70.833333\n",
      "2     95.833333  95.886765  95.833333  95.837023  94.444444\n",
      "3     86.458333  86.828617  86.458333  86.372812  81.944444\n",
      "4     78.819444  78.768784  78.819444  78.593725  71.759259\n",
      "5     72.222222  72.798908  72.222222  72.273264  62.962963\n",
      "6     94.791667  94.857961  94.791667  94.761099  93.055556\n",
      "7     90.625000  90.671109  90.625000  90.565719  87.500000\n",
      "8     89.930556  89.929272  89.930556  89.889096  86.574074\n",
      "mean  86.574074  86.809061  86.574074  86.525544  82.098765\n",
      "std    8.306162   8.136244   8.306162   8.315693  11.074883\n",
      "****************************************\n",
      "Mon Feb 24 12:48:21 2025\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# The code is based on this github project \n",
    "#    https://github.com/snailpt/CTNet/tree/main\n",
    "########################################################################################\n",
    "\n",
    "import os\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # temporal conv kernel size 64=0.25fs\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 22, 1000] \n",
    "            nn.BatchNorm2d(f1),\n",
    "            # channel depth-wise conv\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), # \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # average pooling 1\n",
    "            nn.AvgPool2d((1, pooling_size1)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # spatial conv\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "\n",
    "            # average pooling 2 to adjust the length of feature into transformer encoder\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),  \n",
    "                    \n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "########################################################################################\n",
    "# The Transformer code is based on this github project and has been fine-tuned: \n",
    "#    https://github.com/eeyhsong/EEG-Conformer\n",
    "########################################################################################\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        \n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, dim, cf: float = 1.0, num_experts: int = 1, top_k: int = 1, epsilon: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.epsilon = epsilon\n",
    "        self.top_k = top_k\n",
    "        self.w_gate = nn.Linear(dim, num_experts)\n",
    "        # print(\"GatingNetwork initialized with:\", cf, num_experts, top_k)\n",
    "\n",
    "    def forward(self, x: Tensor, use_aux_loss=False):\n",
    "        gate_scores = F.softmax(self.w_gate(x), dim=-1)  # [batch, num_experts]\n",
    "        capacity = int(self.cf * x.size(0))\n",
    "        # select top_k experts\n",
    "        top_k_scores, top_k_indices = gate_scores.topk(self.top_k, dim=-1)\n",
    "        mask = torch.zeros_like(gate_scores).scatter_(-1, top_k_indices, 1)\n",
    "        masked_gate_scores = gate_scores * mask\n",
    "        denominators = masked_gate_scores.sum(0, keepdim=True) + self.epsilon  # [1, num_experts]\n",
    "        gate_scores_norm = (masked_gate_scores / denominators) * capacity  # [batch, num_experts]\n",
    "\n",
    "        if use_aux_loss:\n",
    "            # caculate load per expert in a batch\n",
    "            load = gate_scores_norm.sum(0)  # [num_experts]\n",
    "            # load balancing loss loss: CV²\n",
    "            aux_loss = (torch.std(load) / (torch.mean(load) + self.epsilon)) ** 2\n",
    "            return gate_scores_norm, aux_loss, top_k_indices\n",
    "        return gate_scores_norm, top_k_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MoE_Layer(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, output_dim: int, cf: float = 1.0, num_experts: int = 1, top_k: int = 1, mult: int = 4, use_aux_loss: bool = False, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.mult = mult\n",
    "        self.use_aux_loss = use_aux_loss\n",
    "        self.top_k = top_k\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim * mult),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(dim * mult, hidden_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = GatingNetwork(dim, cf, num_experts, top_k)\n",
    "        self.projection = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if self.use_aux_loss:\n",
    "            gate_scores, aux_loss, top_k_indices = self.gate(x, use_aux_loss=True)\n",
    "        else:\n",
    "            gate_scores, top_k_indices = self.gate(x, use_aux_loss=False)\n",
    "            aux_loss = 0.0\n",
    "\n",
    "        expert_outputs = [expert(x) for expert in self.experts] \n",
    "        stacked_expert_outputs = torch.stack(expert_outputs, dim=-1)\n",
    "        moe_output = torch.sum(gate_scores.unsqueeze(-2) * stacked_expert_outputs, dim=-1)\n",
    "        moe_output = self.projection(moe_output)\n",
    "        return moe_output, aux_loss, top_k_indices\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads=4, cf=1.0, num_experts=4, top_k=1):\n",
    "        super().__init__()\n",
    "        self.drop_p = 0.5\n",
    "        self.forward_expansion = 4\n",
    "        self.forward_drop_p = 0.5\n",
    "        self.attention = ResidualAdd(nn.Sequential(\n",
    "            MultiHeadAttention(emb_size, num_heads, self.drop_p)\n",
    "        ), emb_size, self.drop_p)\n",
    "        self.moe = MoE_Layer(emb_size, emb_size * num_heads, emb_size, cf, num_experts, top_k, use_aux_loss=True)\n",
    "        self.ffn = ResidualAdd(nn.Sequential(\n",
    "            FeedForwardBlock(emb_size, expansion=self.forward_expansion, drop_p=self.forward_drop_p)\n",
    "        ), emb_size, self.drop_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        moe_out, aux_loss, top_k_indices = self.moe(x)\n",
    "        x = x + moe_out  # residual connection\n",
    "        x = self.ffn(x)\n",
    "        return x, aux_loss, top_k_indices\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, heads, depth, emb_size, cf, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([TransformerEncoderBlock(emb_size, heads, cf, num_experts, top_k) for _ in range(depth)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        total_aux_loss = 0.0\n",
    "        expert_indices_per_block = []\n",
    "        for block in self.blocks:\n",
    "            x, aux_loss, top_k_indices = block(x)\n",
    "            total_aux_loss += aux_loss\n",
    "            expert_indices_per_block.append(top_k_indices.detach().cpu().numpy())\n",
    "        return x, total_aux_loss, expert_indices_per_block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "# learnable positional embedding module        \n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)        \n",
    "        \n",
    "   \n",
    "        \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1=20,\n",
    "                 eeg1_kernel_size=64,\n",
    "                 eeg1_D=2,\n",
    "                 eeg1_pooling_size1=8,\n",
    "                 eeg1_pooling_size2=8,\n",
    "                 eeg1_dropout_rate=0.3,\n",
    "                 eeg1_number_channel=22,\n",
    "                 flatten_eeg1=600,  \n",
    "                 cf=1.0,\n",
    "                 num_experts=4,\n",
    "                 top_k=1,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.cf = cf\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1=eeg1_f1,\n",
    "                                              kernel_size=eeg1_kernel_size,\n",
    "                                              D=eeg1_D,\n",
    "                                              pooling_size1=eeg1_pooling_size1,\n",
    "                                              pooling_size2=eeg1_pooling_size2,\n",
    "                                              dropout_rate=eeg1_dropout_rate)\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(heads, depth, emb_size, cf, num_experts, top_k)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1, self.number_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        trans, aux_loss, expert_indices = self.trans(cnn)\n",
    "        features = cnn + trans  # residual connection\n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out, aux_loss, expert_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads=heads\n",
    "        self.emb_size=emb_size\n",
    "        self.depth=depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "             heads=self.heads, \n",
    "             emb_size=self.emb_size,\n",
    "             depth=self.depth, \n",
    "            database_type=self.dataset_type, \n",
    "            eeg1_f1=eeg1_f1, \n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "            eeg1_number_channel = self.number_channel,\n",
    "            flatten_eeg1 = flatten_eeg1,  \n",
    "            cf = cf,\n",
    "            num_experts = num_experts,\n",
    "            top_k = top_k\n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "\n",
    "    # Segmentation and Reconstruction (S&R) data augmentation\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            \n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        aug_data = aug_data.float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label)  \n",
    "\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        # print(\"len(self.allData):\", len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]  # (288, 1, 22, 1000)\n",
    "        # print(\"shuffle_num\", shuffle_num)\n",
    "        # print(\"self.allLabel\", self.allLabel)\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "\n",
    "        print('-'*20, \"train size：\", self.train_data.shape, \"test size：\", self.test_data.shape)\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "\n",
    "\n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "\n",
    "        \n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        img, label, test_data, test_label = self.get_source_data()\n",
    "        # print(\"label size:\", label.shape)\n",
    "        # print(\"label size:\", label)\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the cnn model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # in_epoch = time.time()\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "            # 验证集\n",
    "            val_data_list = []\n",
    "            val_label_list = []\n",
    "            for i, (img, label) in enumerate(self.dataloader):\n",
    "                number_sample = img.shape[0]\n",
    "                number_validate = int(self.validate_ratio * number_sample)\n",
    "                \n",
    "                # split raw train dataset into real train dataset and validate dataset\n",
    "                train_data = img[:-number_validate]\n",
    "                train_label = label[:-number_validate]\n",
    "                \n",
    "                val_data_list.append(img[number_validate:])\n",
    "                val_label_list.append(label[number_validate:])\n",
    "                \n",
    "                # real train dataset\n",
    "                img = Variable(train_data.type(self.Tensor))\n",
    "                label = Variable(train_label.type(self.LongTensor))\n",
    "                \n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat real train dataset and generate aritifical train dataset\n",
    "                img = torch.cat((img, aug_data))\n",
    "                label = torch.cat((label, aug_label))\n",
    "\n",
    "                # training model\n",
    "                features, outputs, aux_loss, expert_indices = self.model(img)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label)\n",
    "                # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "                # print(features.size())\n",
    "                loss_cls = self.criterion_cls(outputs, label)\n",
    "                lambda_aux = 0.01  # aux loss에 대한 가중치 (실험에 따라 조정)\n",
    "                loss = loss_cls + lambda_aux * aux_loss\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            del img\n",
    "            torch.cuda.empty_cache()\n",
    "            # out_epoch = time.time()\n",
    "            # test process\n",
    "            if (e + 1) % 1 == 0:\n",
    "                self.model.eval()\n",
    "                # validate model\n",
    "                val_data = torch.cat(val_data_list).cuda()\n",
    "                val_label = torch.cat(val_label_list).cuda()\n",
    "                val_data = val_data.type(self.Tensor)\n",
    "                val_label = val_label.type(self.LongTensor)            \n",
    "                \n",
    "                val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "                self.val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "                outputs_list = []\n",
    "                with torch.no_grad():\n",
    "                    for i, (img, _) in enumerate(self.val_dataloader):\n",
    "                        # val model\n",
    "                        img = img.type(self.Tensor).cuda()\n",
    "                        _, Cls, aux_loss, _ = self.model(img)\n",
    "                        outputs_list.append(Cls)\n",
    "                        del img, Cls\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                Cls = torch.cat(outputs_list)\n",
    "                \n",
    "                val_loss = self.criterion_cls(Cls, val_label)\n",
    "                val_loss = val_loss+lambda_aux*aux_loss\n",
    "                val_pred = torch.max(Cls, 1)[1]\n",
    "                val_acc = float((val_pred == val_label).cpu().numpy().astype(int).sum()) / float(val_label.size(0))\n",
    "                \n",
    "                epoch_process['val_acc'] = val_acc                \n",
    "                epoch_process['val_loss'] = val_loss.detach().cpu().numpy()  \n",
    "                \n",
    "                train_pred = torch.max(outputs, 1)[1]\n",
    "\n",
    "\n",
    "                train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "                epoch_process['train_acc'] = train_acc\n",
    "                epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "                num = num + 1\n",
    "\n",
    "                # if min_loss>val_loss:                \n",
    "                if min_loss>val_loss:\n",
    "                    min_loss = val_loss\n",
    "                    best_epoch = e\n",
    "                    epoch_process['epoch'] = e\n",
    "                    torch.save(self.model, self.model_filename)\n",
    "                    print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f}\".format(self.nSub,\n",
    "                                                                                           epoch_process['epoch'],\n",
    "                                                                                           epoch_process['train_acc'],\n",
    "                                                                                           epoch_process['train_loss'],\n",
    "                                                                                           epoch_process['val_acc'],\n",
    "                                                                                           epoch_process['val_loss'],\n",
    "                                                                                        ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "\n",
    "        \n",
    "            del label, val_data, val_label\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename, weights_only=False).cuda()\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img, label) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img.type(self.Tensor)).cuda()\n",
    "                # label_test = Variable(label.type(self.LongTensor))\n",
    "\n",
    "                # test model\n",
    "                features, outputs,_,_ = self.model(img_test)\n",
    "                val_pred = torch.max(outputs, 1)[1]\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list) \n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "        \n",
    "        \n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().astype(int).sum()) / float(test_label.size(0))\n",
    "        \n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent', # \"LOSO\" or other\n",
    "         heads=8,             # heads of MHA\n",
    "         emb_size=48,         # token embding dim\n",
    "         depth=3,             # Transformer encoder depth\n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         eeg1_f1=20,          # features of temporal conv\n",
    "         eeg1_kernel_size=64, # kernel size of temporal conv\n",
    "         eeg1_D=2,            # depth-wise conv \n",
    "         eeg1_pooling_size1=8,# p1\n",
    "         eeg1_pooling_size2=8,# p2\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio = 0.2,\n",
    "         cf = 1,\n",
    "         num_experts=4,\n",
    "         top_k=1\n",
    "         ):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    \n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "    \n",
    "    for i in range(N_SUBJECT):      \n",
    "        \n",
    "        seed_n = np.random.randint(2024)\n",
    "\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus, \n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads, \n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth, \n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1 = eeg1_f1,\n",
    "                  eeg1_kernel_size = eeg1_kernel_size,\n",
    "                  eeg1_D = eeg1_D,\n",
    "                  eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "                  flatten_eeg1 = flatten_eeg1,  \n",
    "                  validate_ratio = validate_ratio\n",
    "                  )\n",
    "        starttime = datetime.datetime.now()\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {'accuray': accuracy*100,\n",
    "                          'precision': precison*100,\n",
    "                          'recall': recall*100,\n",
    "                          'f1': f1*100, \n",
    "                          'kappa': kappa*100\n",
    "                          }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "    \n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "   \n",
    "        df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GatingNetworkwrapper(nn.Module):\n",
    "    def __init__(self, dim, cf: float = 1.0, num_experts: int = 1,top_k:int = 1, epsilon: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.epsilon = epsilon\n",
    "        self.w_gate = nn.Linear(dim, num_experts)\n",
    "\n",
    "        # print(cf, num_experts, top_k)\n",
    "    def forward(self, x: Tensor, use_aux_loss=False):\n",
    "        gate_scores = F.softmax(self.w_gate(x), dim=-1)\n",
    "        capacity = int(self.cf * x.size(0))\n",
    "        top_k_scores, top_k_indices = gate_scores.topk(top_k, dim=-1)\n",
    "        mask = torch.zeros_like(gate_scores).scatter_(-1, top_k_indices, 1)\n",
    "        masked_gate_scores = gate_scores * mask\n",
    "        denominators = masked_gate_scores.sum(0, keepdim=True) + self.epsilon\n",
    "        gate_scores = (masked_gate_scores / denominators) * capacity\n",
    "\n",
    "        if use_aux_loss:\n",
    "            load = gate_scores.sum(0)\n",
    "            importance = gate_scores.sum(1)\n",
    "            loss = ((load - importance) ** 2).mean()\n",
    "            return gate_scores, loss\n",
    "\n",
    "        return gate_scores\n",
    "\n",
    "class MoE_Layerwrapper(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, output_dim: int, cf: float = 1.0, num_experts: int = 1, top_k:int = 1, mult: int = 4, use_aux_loss: bool = False, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.cf = cf\n",
    "        self.mult = mult\n",
    "        self.use_aux_loss = use_aux_loss\n",
    "        self.top_k = top_k\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim*mult),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(dim*mult, hidden_dim)\n",
    "\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = GatingNetworkwrapper(dim, cf, num_experts, top_k)\n",
    "        self.projection = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        gate_scores = self.gate(x, use_aux_loss=self.use_aux_loss)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "\n",
    "        if torch.isnan(gate_scores).any():\n",
    "            print(\"NaN in gate scores\")\n",
    "            gate_scores[torch.isnan(gate_scores)] = 0\n",
    "\n",
    "        stacked_expert_outputs = torch.stack(expert_outputs, dim=-1)\n",
    "        if torch.isnan(stacked_expert_outputs).any():\n",
    "            stacked_expert_outputs[torch.isnan(stacked_expert_outputs)] = 0\n",
    "\n",
    "        moe_output = torch.sum(gate_scores.unsqueeze(-2) * stacked_expert_outputs, dim=-1)\n",
    "        moe_output = self.projection(moe_output)\n",
    "        return moe_output\n",
    "\n",
    "class TransformerEncoderBlockwrapper(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 cf=1.0, num_experts=4, top_k=1):\n",
    "        self.drop_p=0.5\n",
    "        self.forward_expansion=4\n",
    "        self.forward_drop_p=0.5\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, self.drop_p),\n",
    "                ), emb_size, self.drop_p),\n",
    "            MoE_Layerwrapper(emb_size, emb_size * num_heads, emb_size, cf, num_experts, top_k),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=self.forward_expansion, drop_p=self.forward_drop_p),\n",
    "                ), emb_size, self.drop_p)\n",
    "            \n",
    "            )    \n",
    "        \n",
    "        \n",
    "class TransformerEncoderwrapper(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size, cf, num_experts, top_k):\n",
    "        super().__init__(*[TransformerEncoderBlockwrapper(emb_size, heads, cf, num_experts, top_k) for _ in range(depth)])\n",
    "      \n",
    "        \n",
    "   \n",
    "        \n",
    "# CTNet       \n",
    "class EEGTransformerwrapper(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 600,\n",
    "                 cf = 1.0,\n",
    "                 num_experts=4,\n",
    "                 top_k=1,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.cf = cf\n",
    "        self.num_experts=num_experts\n",
    "        self.top_k=top_k\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.flatten = nn.Flatten()\n",
    "        # print('self.number_channel', self.number_channel)\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1 = eeg1_f1,\n",
    "                                              kernel_size = eeg1_kernel_size,\n",
    "                                              D = eeg1_D,\n",
    "                                              pooling_size1 = eeg1_pooling_size1,\n",
    "                                              pooling_size2 = eeg1_pooling_size2,\n",
    "                                              dropout_rate = eeg1_dropout_rate,\n",
    "                                              )\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoderwrapper(heads, depth, emb_size, cf, num_experts, top_k)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class) # FLATTEN_EEGNet + FLATTEN_cnn_module\n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "\n",
    "        #  positional embedding\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        \n",
    "        trans = self.trans(cnn)\n",
    "        # residual connect\n",
    "        features = cnn+trans\n",
    "        \n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO-No' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    N_SUBJECT = 9       # BCI \n",
    "    N_AUG = 3           # data augmentation times for benerating artificial training data set\n",
    "    N_SEG = 8           # segmentation times for S&R\n",
    "\n",
    "    EPOCHS = 1000\n",
    "    EMB_DIM = 16\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "    TYPE = 'A'\n",
    "    validate_ratio = 0.3 # split raw train dataset into real train dataset and validate dataset\n",
    "\n",
    "    EEGNet1_F1 = 8\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "    FLATTEN_EEGNet1 = 240\n",
    "\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25    \n",
    "\n",
    "    # for cf in [1, 1.2, 1.5]:\n",
    "    for cf in [1.2]: # best cf\n",
    "        # for num_experts in [2, 4, 8]:\n",
    "        for num_experts in [4]: # best num_experts\n",
    "            if num_experts==2:\n",
    "                k = [1, 2]\n",
    "            elif num_experts==4:\n",
    "                # k = [1,2,4]\n",
    "                k = [2] # best top_k\n",
    "            else:\n",
    "                k = [1,2,4,8]\n",
    "            \n",
    "            for top_k in k:\n",
    "            \n",
    "                number_class, number_channel = numberClassChannel(TYPE)\n",
    "                RESULT_NAME = \"{}_heads_{}_depth_{}_cf_{}_num_exerts_{}_top_k_{}\".format(TYPE, HEADS, DEPTH, cf, num_experts, top_k)\n",
    "\n",
    "                sModel = EEGTransformer(\n",
    "                    heads=HEADS, \n",
    "                    emb_size=EMB_DIM,\n",
    "                    depth=DEPTH, \n",
    "                    database_type=TYPE,\n",
    "                    eeg1_f1=EEGNet1_F1, \n",
    "                    eeg1_D=EEGNet1_D,\n",
    "                    eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "                    eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                    eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                    eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                    eeg1_number_channel = number_channel,\n",
    "                    flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "                    cf = cf,\n",
    "                    num_experts=num_experts,\n",
    "                    top_k=top_k\n",
    "                    ).cuda()\n",
    "                summaryModel = EEGTransformerwrapper(\n",
    "                    heads=HEADS, \n",
    "                    emb_size=EMB_DIM,\n",
    "                    depth=DEPTH, \n",
    "                    database_type=TYPE,\n",
    "                    eeg1_f1=EEGNet1_F1, \n",
    "                    eeg1_D=EEGNet1_D,\n",
    "                    eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "                    eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                    eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                    eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                    eeg1_number_channel = number_channel,\n",
    "                    flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "                    cf = cf,\n",
    "                    num_experts=num_experts,\n",
    "                    top_k=top_k\n",
    "                    ).cuda()\n",
    "                # wrapped_model = EEGTransformerWrapper(sModel)\n",
    "                summary(summaryModel, (1, number_channel, 1000))\n",
    "\n",
    "                print(time.asctime(time.localtime(time.time())))\n",
    "\n",
    "\n",
    "                result = main(RESULT_NAME,\n",
    "                                evaluate_mode = EVALUATE_MODE,\n",
    "                                heads=HEADS, \n",
    "                                emb_size=EMB_DIM,\n",
    "                                depth=DEPTH, \n",
    "                                dataset_type=TYPE,\n",
    "                                eeg1_f1 = EEGNet1_F1,\n",
    "                                eeg1_kernel_size = EEGNet1_KERNEL_SIZE,\n",
    "                                eeg1_D = EEGNet1_D,\n",
    "                                eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                                eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                                eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                                flatten_eeg1 = FLATTEN_EEGNet1,\n",
    "                                validate_ratio = validate_ratio,\n",
    "                                cf = cf,\n",
    "                                num_experts=num_experts,\n",
    "                                top_k=top_k\n",
    "                              )\n",
    "                print(time.asctime(time.localtime(time.time())))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
