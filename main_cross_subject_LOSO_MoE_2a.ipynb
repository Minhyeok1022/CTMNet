{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cad76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 22, 1000]             512\n",
      "       BatchNorm2d-2          [-1, 8, 22, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]             352\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "          Dropout-15               [-1, 15, 16]               0\n",
      "PositioinalEncoding-16               [-1, 15, 16]               0\n",
      "           Linear-17               [-1, 15, 16]             272\n",
      "           Linear-18               [-1, 15, 16]             272\n",
      "           Linear-19               [-1, 15, 16]             272\n",
      "          Dropout-20            [-1, 2, 15, 15]               0\n",
      "           Linear-21               [-1, 15, 16]             272\n",
      "MultiHeadAttention-22               [-1, 15, 16]               0\n",
      "          Dropout-23               [-1, 15, 16]               0\n",
      "        LayerNorm-24               [-1, 15, 16]              32\n",
      "      ResidualAdd-25               [-1, 15, 16]               0\n",
      "           Linear-26                [-1, 15, 4]              68\n",
      "       SwitchGate-27                [-1, 15, 4]               0\n",
      "           Linear-28               [-1, 15, 64]           1,088\n",
      "             GELU-29               [-1, 15, 64]               0\n",
      "           Linear-30               [-1, 15, 32]           2,080\n",
      "           Linear-31               [-1, 15, 64]           1,088\n",
      "             GELU-32               [-1, 15, 64]               0\n",
      "           Linear-33               [-1, 15, 32]           2,080\n",
      "           Linear-34               [-1, 15, 64]           1,088\n",
      "             GELU-35               [-1, 15, 64]               0\n",
      "           Linear-36               [-1, 15, 32]           2,080\n",
      "           Linear-37               [-1, 15, 64]           1,088\n",
      "             GELU-38               [-1, 15, 64]               0\n",
      "           Linear-39               [-1, 15, 32]           2,080\n",
      "           Linear-40               [-1, 15, 16]             528\n",
      "        SwitchMoE-41               [-1, 15, 16]               0\n",
      "           Linear-42               [-1, 15, 64]           1,088\n",
      "             GELU-43               [-1, 15, 64]               0\n",
      "          Dropout-44               [-1, 15, 64]               0\n",
      "           Linear-45               [-1, 15, 16]           1,040\n",
      "          Dropout-46               [-1, 15, 16]               0\n",
      "        LayerNorm-47               [-1, 15, 16]              32\n",
      "      ResidualAdd-48               [-1, 15, 16]               0\n",
      "           Linear-49               [-1, 15, 16]             272\n",
      "           Linear-50               [-1, 15, 16]             272\n",
      "           Linear-51               [-1, 15, 16]             272\n",
      "          Dropout-52            [-1, 2, 15, 15]               0\n",
      "           Linear-53               [-1, 15, 16]             272\n",
      "MultiHeadAttention-54               [-1, 15, 16]               0\n",
      "          Dropout-55               [-1, 15, 16]               0\n",
      "        LayerNorm-56               [-1, 15, 16]              32\n",
      "      ResidualAdd-57               [-1, 15, 16]               0\n",
      "           Linear-58                [-1, 15, 4]              68\n",
      "       SwitchGate-59                [-1, 15, 4]               0\n",
      "           Linear-60               [-1, 15, 64]           1,088\n",
      "             GELU-61               [-1, 15, 64]               0\n",
      "           Linear-62               [-1, 15, 32]           2,080\n",
      "           Linear-63               [-1, 15, 64]           1,088\n",
      "             GELU-64               [-1, 15, 64]               0\n",
      "           Linear-65               [-1, 15, 32]           2,080\n",
      "           Linear-66               [-1, 15, 64]           1,088\n",
      "             GELU-67               [-1, 15, 64]               0\n",
      "           Linear-68               [-1, 15, 32]           2,080\n",
      "           Linear-69               [-1, 15, 64]           1,088\n",
      "             GELU-70               [-1, 15, 64]               0\n",
      "           Linear-71               [-1, 15, 32]           2,080\n",
      "           Linear-72               [-1, 15, 16]             528\n",
      "        SwitchMoE-73               [-1, 15, 16]               0\n",
      "           Linear-74               [-1, 15, 64]           1,088\n",
      "             GELU-75               [-1, 15, 64]               0\n",
      "          Dropout-76               [-1, 15, 64]               0\n",
      "           Linear-77               [-1, 15, 16]           1,040\n",
      "          Dropout-78               [-1, 15, 16]               0\n",
      "        LayerNorm-79               [-1, 15, 16]              32\n",
      "      ResidualAdd-80               [-1, 15, 16]               0\n",
      "           Linear-81               [-1, 15, 16]             272\n",
      "           Linear-82               [-1, 15, 16]             272\n",
      "           Linear-83               [-1, 15, 16]             272\n",
      "          Dropout-84            [-1, 2, 15, 15]               0\n",
      "           Linear-85               [-1, 15, 16]             272\n",
      "MultiHeadAttention-86               [-1, 15, 16]               0\n",
      "          Dropout-87               [-1, 15, 16]               0\n",
      "        LayerNorm-88               [-1, 15, 16]              32\n",
      "      ResidualAdd-89               [-1, 15, 16]               0\n",
      "           Linear-90                [-1, 15, 4]              68\n",
      "       SwitchGate-91                [-1, 15, 4]               0\n",
      "           Linear-92               [-1, 15, 64]           1,088\n",
      "             GELU-93               [-1, 15, 64]               0\n",
      "           Linear-94               [-1, 15, 32]           2,080\n",
      "           Linear-95               [-1, 15, 64]           1,088\n",
      "             GELU-96               [-1, 15, 64]               0\n",
      "           Linear-97               [-1, 15, 32]           2,080\n",
      "           Linear-98               [-1, 15, 64]           1,088\n",
      "             GELU-99               [-1, 15, 64]               0\n",
      "          Linear-100               [-1, 15, 32]           2,080\n",
      "          Linear-101               [-1, 15, 64]           1,088\n",
      "            GELU-102               [-1, 15, 64]               0\n",
      "          Linear-103               [-1, 15, 32]           2,080\n",
      "          Linear-104               [-1, 15, 16]             528\n",
      "       SwitchMoE-105               [-1, 15, 16]               0\n",
      "          Linear-106               [-1, 15, 64]           1,088\n",
      "            GELU-107               [-1, 15, 64]               0\n",
      "         Dropout-108               [-1, 15, 64]               0\n",
      "          Linear-109               [-1, 15, 16]           1,040\n",
      "         Dropout-110               [-1, 15, 16]               0\n",
      "       LayerNorm-111               [-1, 15, 16]              32\n",
      "     ResidualAdd-112               [-1, 15, 16]               0\n",
      "          Linear-113               [-1, 15, 16]             272\n",
      "          Linear-114               [-1, 15, 16]             272\n",
      "          Linear-115               [-1, 15, 16]             272\n",
      "         Dropout-116            [-1, 2, 15, 15]               0\n",
      "          Linear-117               [-1, 15, 16]             272\n",
      "MultiHeadAttention-118               [-1, 15, 16]               0\n",
      "         Dropout-119               [-1, 15, 16]               0\n",
      "       LayerNorm-120               [-1, 15, 16]              32\n",
      "     ResidualAdd-121               [-1, 15, 16]               0\n",
      "          Linear-122                [-1, 15, 4]              68\n",
      "      SwitchGate-123                [-1, 15, 4]               0\n",
      "          Linear-124               [-1, 15, 64]           1,088\n",
      "            GELU-125               [-1, 15, 64]               0\n",
      "          Linear-126               [-1, 15, 32]           2,080\n",
      "          Linear-127               [-1, 15, 64]           1,088\n",
      "            GELU-128               [-1, 15, 64]               0\n",
      "          Linear-129               [-1, 15, 32]           2,080\n",
      "          Linear-130               [-1, 15, 64]           1,088\n",
      "            GELU-131               [-1, 15, 64]               0\n",
      "          Linear-132               [-1, 15, 32]           2,080\n",
      "          Linear-133               [-1, 15, 64]           1,088\n",
      "            GELU-134               [-1, 15, 64]               0\n",
      "          Linear-135               [-1, 15, 32]           2,080\n",
      "          Linear-136               [-1, 15, 16]             528\n",
      "       SwitchMoE-137               [-1, 15, 16]               0\n",
      "          Linear-138               [-1, 15, 64]           1,088\n",
      "            GELU-139               [-1, 15, 64]               0\n",
      "         Dropout-140               [-1, 15, 64]               0\n",
      "          Linear-141               [-1, 15, 16]           1,040\n",
      "         Dropout-142               [-1, 15, 16]               0\n",
      "       LayerNorm-143               [-1, 15, 16]              32\n",
      "     ResidualAdd-144               [-1, 15, 16]               0\n",
      "          Linear-145               [-1, 15, 16]             272\n",
      "          Linear-146               [-1, 15, 16]             272\n",
      "          Linear-147               [-1, 15, 16]             272\n",
      "         Dropout-148            [-1, 2, 15, 15]               0\n",
      "          Linear-149               [-1, 15, 16]             272\n",
      "MultiHeadAttention-150               [-1, 15, 16]               0\n",
      "         Dropout-151               [-1, 15, 16]               0\n",
      "       LayerNorm-152               [-1, 15, 16]              32\n",
      "     ResidualAdd-153               [-1, 15, 16]               0\n",
      "          Linear-154                [-1, 15, 4]              68\n",
      "      SwitchGate-155                [-1, 15, 4]               0\n",
      "          Linear-156               [-1, 15, 64]           1,088\n",
      "            GELU-157               [-1, 15, 64]               0\n",
      "          Linear-158               [-1, 15, 32]           2,080\n",
      "          Linear-159               [-1, 15, 64]           1,088\n",
      "            GELU-160               [-1, 15, 64]               0\n",
      "          Linear-161               [-1, 15, 32]           2,080\n",
      "          Linear-162               [-1, 15, 64]           1,088\n",
      "            GELU-163               [-1, 15, 64]               0\n",
      "          Linear-164               [-1, 15, 32]           2,080\n",
      "          Linear-165               [-1, 15, 64]           1,088\n",
      "            GELU-166               [-1, 15, 64]               0\n",
      "          Linear-167               [-1, 15, 32]           2,080\n",
      "          Linear-168               [-1, 15, 16]             528\n",
      "       SwitchMoE-169               [-1, 15, 16]               0\n",
      "          Linear-170               [-1, 15, 64]           1,088\n",
      "            GELU-171               [-1, 15, 64]               0\n",
      "         Dropout-172               [-1, 15, 64]               0\n",
      "          Linear-173               [-1, 15, 16]           1,040\n",
      "         Dropout-174               [-1, 15, 16]               0\n",
      "       LayerNorm-175               [-1, 15, 16]              32\n",
      "     ResidualAdd-176               [-1, 15, 16]               0\n",
      "          Linear-177               [-1, 15, 16]             272\n",
      "          Linear-178               [-1, 15, 16]             272\n",
      "          Linear-179               [-1, 15, 16]             272\n",
      "         Dropout-180            [-1, 2, 15, 15]               0\n",
      "          Linear-181               [-1, 15, 16]             272\n",
      "MultiHeadAttention-182               [-1, 15, 16]               0\n",
      "         Dropout-183               [-1, 15, 16]               0\n",
      "       LayerNorm-184               [-1, 15, 16]              32\n",
      "     ResidualAdd-185               [-1, 15, 16]               0\n",
      "          Linear-186                [-1, 15, 4]              68\n",
      "      SwitchGate-187                [-1, 15, 4]               0\n",
      "          Linear-188               [-1, 15, 64]           1,088\n",
      "            GELU-189               [-1, 15, 64]               0\n",
      "          Linear-190               [-1, 15, 32]           2,080\n",
      "          Linear-191               [-1, 15, 64]           1,088\n",
      "            GELU-192               [-1, 15, 64]               0\n",
      "          Linear-193               [-1, 15, 32]           2,080\n",
      "          Linear-194               [-1, 15, 64]           1,088\n",
      "            GELU-195               [-1, 15, 64]               0\n",
      "          Linear-196               [-1, 15, 32]           2,080\n",
      "          Linear-197               [-1, 15, 64]           1,088\n",
      "            GELU-198               [-1, 15, 64]               0\n",
      "          Linear-199               [-1, 15, 32]           2,080\n",
      "          Linear-200               [-1, 15, 16]             528\n",
      "       SwitchMoE-201               [-1, 15, 16]               0\n",
      "          Linear-202               [-1, 15, 64]           1,088\n",
      "            GELU-203               [-1, 15, 64]               0\n",
      "         Dropout-204               [-1, 15, 64]               0\n",
      "          Linear-205               [-1, 15, 16]           1,040\n",
      "         Dropout-206               [-1, 15, 16]               0\n",
      "       LayerNorm-207               [-1, 15, 16]              32\n",
      "     ResidualAdd-208               [-1, 15, 16]               0\n",
      "         Flatten-209                  [-1, 240]               0\n",
      "         Dropout-210                  [-1, 240]               0\n",
      "          Linear-211                    [-1, 4]             964\n",
      "================================================================\n",
      "Total params: 105,292\n",
      "Trainable params: 105,292\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 3.89\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 4.38\n",
      "----------------------------------------------------------------\n",
      "Sun Mar  9 03:38:21 2025\n",
      "seed is 1855\n",
      "Subject 1\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "1_0 train_acc: 0.2814 train_loss: 0.003543\tval_acc: 0.365942 val_loss: 0.004824873, acc:0.307292\n",
      "1_1 train_acc: 0.3534 train_loss: 0.002825\tval_acc: 0.490580 val_loss: 0.004329144, acc:0.489583\n",
      "1_2 train_acc: 0.4511 train_loss: 0.002443\tval_acc: 0.544203 val_loss: 0.003911868, acc:0.534722\n",
      "1_3 train_acc: 0.5316 train_loss: 0.002197\tval_acc: 0.567391 val_loss: 0.003763121, acc:0.524306\n",
      "1_4 train_acc: 0.5646 train_loss: 0.002075\tval_acc: 0.588406 val_loss: 0.003538129, acc:0.559028\n",
      "1_5 train_acc: 0.5827 train_loss: 0.002000\tval_acc: 0.588406 val_loss: 0.003502102, acc:0.555556\n",
      "1_6 train_acc: 0.5948 train_loss: 0.001950\tval_acc: 0.613768 val_loss: 0.003415623, acc:0.517361\n",
      "1_7 train_acc: 0.6124 train_loss: 0.001901\tval_acc: 0.632609 val_loss: 0.003267872, acc:0.557292\n",
      "1_8 train_acc: 0.6196 train_loss: 0.001864\tval_acc: 0.627536 val_loss: 0.003192321, acc:0.574653\n",
      "1_11 train_acc: 0.6385 train_loss: 0.001784\tval_acc: 0.670290 val_loss: 0.003066454, acc:0.607639\n",
      "1_12 train_acc: 0.6464 train_loss: 0.001737\tval_acc: 0.679710 val_loss: 0.002995279, acc:0.578125\n",
      "1_14 train_acc: 0.6576 train_loss: 0.001701\tval_acc: 0.680435 val_loss: 0.002968177, acc:0.602431\n",
      "1_16 train_acc: 0.6629 train_loss: 0.001667\tval_acc: 0.684783 val_loss: 0.002869864, acc:0.626736\n",
      "1_18 train_acc: 0.6700 train_loss: 0.001654\tval_acc: 0.683333 val_loss: 0.002868260, acc:0.630208\n",
      "1_19 train_acc: 0.6748 train_loss: 0.001632\tval_acc: 0.705072 val_loss: 0.002828328, acc:0.649306\n",
      "1_20 train_acc: 0.6725 train_loss: 0.001642\tval_acc: 0.704348 val_loss: 0.002731065, acc:0.645833\n",
      "1_25 train_acc: 0.6869 train_loss: 0.001585\tval_acc: 0.720290 val_loss: 0.002702310, acc:0.666667\n",
      "1_26 train_acc: 0.6870 train_loss: 0.001565\tval_acc: 0.710145 val_loss: 0.002700025, acc:0.666667\n",
      "1_27 train_acc: 0.6942 train_loss: 0.001542\tval_acc: 0.720290 val_loss: 0.002666810, acc:0.661458\n",
      "1_28 train_acc: 0.6894 train_loss: 0.001565\tval_acc: 0.726087 val_loss: 0.002633723, acc:0.670139\n",
      "1_32 train_acc: 0.7013 train_loss: 0.001517\tval_acc: 0.728261 val_loss: 0.002594794, acc:0.687500\n",
      "1_33 train_acc: 0.6994 train_loss: 0.001518\tval_acc: 0.728986 val_loss: 0.002586274, acc:0.680556\n",
      "1_34 train_acc: 0.7023 train_loss: 0.001507\tval_acc: 0.744203 val_loss: 0.002504494, acc:0.689236\n",
      "1_41 train_acc: 0.7150 train_loss: 0.001475\tval_acc: 0.739130 val_loss: 0.002488847, acc:0.696181\n",
      "1_44 train_acc: 0.7168 train_loss: 0.001442\tval_acc: 0.743478 val_loss: 0.002416325, acc:0.697917\n",
      "1_45 train_acc: 0.7149 train_loss: 0.001453\tval_acc: 0.744203 val_loss: 0.002384603, acc:0.711806\n",
      "1_53 train_acc: 0.7267 train_loss: 0.001396\tval_acc: 0.755797 val_loss: 0.002366963, acc:0.701389\n",
      "1_57 train_acc: 0.7248 train_loss: 0.001387\tval_acc: 0.752899 val_loss: 0.002352909, acc:0.704861\n",
      "1_63 train_acc: 0.7348 train_loss: 0.001370\tval_acc: 0.764493 val_loss: 0.002321920, acc:0.703125\n",
      "1_64 train_acc: 0.7342 train_loss: 0.001360\tval_acc: 0.762319 val_loss: 0.002217754, acc:0.710069\n",
      "1_75 train_acc: 0.7378 train_loss: 0.001336\tval_acc: 0.773913 val_loss: 0.002179417, acc:0.720486\n",
      "1_90 train_acc: 0.7515 train_loss: 0.001296\tval_acc: 0.779710 val_loss: 0.002171456, acc:0.680556\n",
      "1_91 train_acc: 0.7524 train_loss: 0.001280\tval_acc: 0.786957 val_loss: 0.002119165, acc:0.697917\n",
      "1_101 train_acc: 0.7535 train_loss: 0.001272\tval_acc: 0.786957 val_loss: 0.002063413, acc:0.720486\n",
      "1_106 train_acc: 0.7574 train_loss: 0.001261\tval_acc: 0.802174 val_loss: 0.002038811, acc:0.704861\n",
      "1_113 train_acc: 0.7622 train_loss: 0.001241\tval_acc: 0.792754 val_loss: 0.001997783, acc:0.711806\n",
      "1_117 train_acc: 0.7614 train_loss: 0.001251\tval_acc: 0.792029 val_loss: 0.001996354, acc:0.703125\n",
      "1_127 train_acc: 0.7662 train_loss: 0.001200\tval_acc: 0.805072 val_loss: 0.001973206, acc:0.715278\n",
      "1_141 train_acc: 0.7741 train_loss: 0.001189\tval_acc: 0.800000 val_loss: 0.001939438, acc:0.725694\n",
      "1_150 train_acc: 0.7745 train_loss: 0.001177\tval_acc: 0.813043 val_loss: 0.001916830, acc:0.723958\n",
      "1_176 train_acc: 0.7761 train_loss: 0.001172\tval_acc: 0.816667 val_loss: 0.001896090, acc:0.725694\n",
      "1_197 train_acc: 0.7813 train_loss: 0.001160\tval_acc: 0.813768 val_loss: 0.001842092, acc:0.730903\n",
      "1_248 train_acc: 0.7824 train_loss: 0.001126\tval_acc: 0.820290 val_loss: 0.001790452, acc:0.729167\n",
      "1_330 train_acc: 0.7913 train_loss: 0.001109\tval_acc: 0.822464 val_loss: 0.001771047, acc:0.708333\n",
      "1_351 train_acc: 0.7959 train_loss: 0.001063\tval_acc: 0.821014 val_loss: 0.001758912, acc:0.710069\n",
      "1_481 train_acc: 0.8028 train_loss: 0.001043\tval_acc: 0.824638 val_loss: 0.001749570, acc:0.692708\n",
      "1_516 train_acc: 0.8020 train_loss: 0.001032\tval_acc: 0.817391 val_loss: 0.001719756, acc:0.704861\n",
      "epoch:  516 \tThe test accuracy is: 0.7048611111111112\n",
      " THE BEST ACCURACY IS 0.7048611111111112\tkappa is 0.6064814814814814\n",
      "subject 1 duration: 0:47:52.551102\n",
      "seed is 1943\n",
      "Subject 2\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "2_0 train_acc: 0.3078 train_loss: 0.003389\tval_acc: 0.420290 val_loss: 0.004563199, acc:0.309028\n",
      "2_1 train_acc: 0.4237 train_loss: 0.002607\tval_acc: 0.542029 val_loss: 0.003996846, acc:0.414931\n",
      "2_2 train_acc: 0.5129 train_loss: 0.002267\tval_acc: 0.547101 val_loss: 0.003874417, acc:0.376736\n",
      "2_3 train_acc: 0.5517 train_loss: 0.002119\tval_acc: 0.561594 val_loss: 0.003765595, acc:0.401042\n",
      "2_4 train_acc: 0.5717 train_loss: 0.002037\tval_acc: 0.580435 val_loss: 0.003665983, acc:0.397569\n",
      "2_6 train_acc: 0.6069 train_loss: 0.001895\tval_acc: 0.606522 val_loss: 0.003508045, acc:0.329861\n",
      "2_7 train_acc: 0.6270 train_loss: 0.001831\tval_acc: 0.632609 val_loss: 0.003344962, acc:0.414931\n",
      "2_8 train_acc: 0.6414 train_loss: 0.001765\tval_acc: 0.671014 val_loss: 0.003220998, acc:0.385417\n",
      "2_9 train_acc: 0.6524 train_loss: 0.001723\tval_acc: 0.674638 val_loss: 0.003153963, acc:0.390625\n",
      "2_10 train_acc: 0.6643 train_loss: 0.001685\tval_acc: 0.684058 val_loss: 0.003060939, acc:0.366319\n",
      "2_12 train_acc: 0.6792 train_loss: 0.001623\tval_acc: 0.689130 val_loss: 0.003027517, acc:0.380208\n",
      "2_13 train_acc: 0.6922 train_loss: 0.001573\tval_acc: 0.690580 val_loss: 0.003003474, acc:0.416667\n",
      "2_15 train_acc: 0.6983 train_loss: 0.001553\tval_acc: 0.702899 val_loss: 0.002925699, acc:0.366319\n",
      "2_17 train_acc: 0.7029 train_loss: 0.001519\tval_acc: 0.699275 val_loss: 0.002885791, acc:0.388889\n",
      "2_18 train_acc: 0.7058 train_loss: 0.001505\tval_acc: 0.712319 val_loss: 0.002840094, acc:0.399306\n",
      "2_21 train_acc: 0.7099 train_loss: 0.001479\tval_acc: 0.717391 val_loss: 0.002775459, acc:0.388889\n",
      "2_23 train_acc: 0.7150 train_loss: 0.001471\tval_acc: 0.713043 val_loss: 0.002765902, acc:0.387153\n",
      "2_26 train_acc: 0.7220 train_loss: 0.001418\tval_acc: 0.728261 val_loss: 0.002732060, acc:0.413194\n",
      "2_27 train_acc: 0.7223 train_loss: 0.001427\tval_acc: 0.735507 val_loss: 0.002659708, acc:0.446181\n",
      "2_34 train_acc: 0.7387 train_loss: 0.001370\tval_acc: 0.742754 val_loss: 0.002570600, acc:0.442708\n",
      "2_38 train_acc: 0.7323 train_loss: 0.001350\tval_acc: 0.754348 val_loss: 0.002524155, acc:0.446181\n",
      "2_42 train_acc: 0.7434 train_loss: 0.001327\tval_acc: 0.757971 val_loss: 0.002465196, acc:0.473958\n",
      "2_48 train_acc: 0.7464 train_loss: 0.001305\tval_acc: 0.754348 val_loss: 0.002460679, acc:0.454861\n",
      "2_52 train_acc: 0.7501 train_loss: 0.001297\tval_acc: 0.755072 val_loss: 0.002384293, acc:0.467014\n",
      "2_54 train_acc: 0.7515 train_loss: 0.001284\tval_acc: 0.770290 val_loss: 0.002367684, acc:0.472222\n",
      "2_61 train_acc: 0.7607 train_loss: 0.001254\tval_acc: 0.765942 val_loss: 0.002304525, acc:0.467014\n",
      "2_63 train_acc: 0.7618 train_loss: 0.001241\tval_acc: 0.769565 val_loss: 0.002293013, acc:0.446181\n",
      "2_68 train_acc: 0.7622 train_loss: 0.001234\tval_acc: 0.782609 val_loss: 0.002259684, acc:0.505208\n",
      "2_70 train_acc: 0.7631 train_loss: 0.001237\tval_acc: 0.773188 val_loss: 0.002254070, acc:0.519097\n",
      "2_74 train_acc: 0.7679 train_loss: 0.001214\tval_acc: 0.776087 val_loss: 0.002199543, acc:0.477431\n",
      "2_86 train_acc: 0.7743 train_loss: 0.001171\tval_acc: 0.778261 val_loss: 0.002178537, acc:0.512153\n",
      "2_92 train_acc: 0.7774 train_loss: 0.001178\tval_acc: 0.781884 val_loss: 0.002136215, acc:0.461806\n",
      "2_95 train_acc: 0.7748 train_loss: 0.001184\tval_acc: 0.781884 val_loss: 0.002126044, acc:0.498264\n",
      "2_96 train_acc: 0.7763 train_loss: 0.001167\tval_acc: 0.783333 val_loss: 0.002098082, acc:0.482639\n",
      "2_100 train_acc: 0.7779 train_loss: 0.001156\tval_acc: 0.783333 val_loss: 0.002097423, acc:0.498264\n",
      "2_105 train_acc: 0.7820 train_loss: 0.001157\tval_acc: 0.784058 val_loss: 0.002084771, acc:0.484375\n",
      "2_114 train_acc: 0.7844 train_loss: 0.001122\tval_acc: 0.788406 val_loss: 0.002047594, acc:0.496528\n",
      "2_115 train_acc: 0.7876 train_loss: 0.001124\tval_acc: 0.784783 val_loss: 0.002034154, acc:0.526042\n",
      "2_116 train_acc: 0.7854 train_loss: 0.001132\tval_acc: 0.791304 val_loss: 0.002033775, acc:0.482639\n",
      "2_118 train_acc: 0.7887 train_loss: 0.001122\tval_acc: 0.788406 val_loss: 0.002032132, acc:0.500000\n",
      "2_121 train_acc: 0.7901 train_loss: 0.001108\tval_acc: 0.794928 val_loss: 0.001997896, acc:0.522569\n",
      "2_124 train_acc: 0.7894 train_loss: 0.001117\tval_acc: 0.790580 val_loss: 0.001974998, acc:0.506944\n",
      "2_133 train_acc: 0.7911 train_loss: 0.001099\tval_acc: 0.800725 val_loss: 0.001955237, acc:0.484375\n",
      "2_143 train_acc: 0.7938 train_loss: 0.001080\tval_acc: 0.797826 val_loss: 0.001952954, acc:0.494792\n",
      "2_146 train_acc: 0.7969 train_loss: 0.001065\tval_acc: 0.803623 val_loss: 0.001937963, acc:0.486111\n",
      "2_148 train_acc: 0.7949 train_loss: 0.001085\tval_acc: 0.805797 val_loss: 0.001926313, acc:0.468750\n",
      "2_157 train_acc: 0.8023 train_loss: 0.001061\tval_acc: 0.808696 val_loss: 0.001877733, acc:0.472222\n",
      "2_168 train_acc: 0.8021 train_loss: 0.001042\tval_acc: 0.807246 val_loss: 0.001841689, acc:0.527778\n",
      "2_178 train_acc: 0.8032 train_loss: 0.001055\tval_acc: 0.802174 val_loss: 0.001835670, acc:0.496528\n",
      "2_186 train_acc: 0.8034 train_loss: 0.001039\tval_acc: 0.815217 val_loss: 0.001811891, acc:0.515625\n",
      "2_196 train_acc: 0.8036 train_loss: 0.001035\tval_acc: 0.819565 val_loss: 0.001757725, acc:0.510417\n",
      "2_221 train_acc: 0.8153 train_loss: 0.000990\tval_acc: 0.826087 val_loss: 0.001741495, acc:0.513889\n",
      "2_239 train_acc: 0.8137 train_loss: 0.001000\tval_acc: 0.821739 val_loss: 0.001733704, acc:0.487847\n",
      "2_264 train_acc: 0.8171 train_loss: 0.000992\tval_acc: 0.822464 val_loss: 0.001703992, acc:0.487847\n",
      "2_274 train_acc: 0.8133 train_loss: 0.000981\tval_acc: 0.829710 val_loss: 0.001703026, acc:0.515625\n",
      "2_276 train_acc: 0.8189 train_loss: 0.000965\tval_acc: 0.827536 val_loss: 0.001684056, acc:0.529514\n",
      "2_346 train_acc: 0.8255 train_loss: 0.000933\tval_acc: 0.827536 val_loss: 0.001671510, acc:0.482639\n",
      "2_369 train_acc: 0.8255 train_loss: 0.000945\tval_acc: 0.827536 val_loss: 0.001660301, acc:0.512153\n",
      "2_391 train_acc: 0.8252 train_loss: 0.000937\tval_acc: 0.827536 val_loss: 0.001655426, acc:0.510417\n",
      "2_392 train_acc: 0.8230 train_loss: 0.000938\tval_acc: 0.830435 val_loss: 0.001623190, acc:0.500000\n",
      "2_479 train_acc: 0.8279 train_loss: 0.000926\tval_acc: 0.838406 val_loss: 0.001598170, acc:0.506944\n",
      "2_485 train_acc: 0.8331 train_loss: 0.000911\tval_acc: 0.827536 val_loss: 0.001595058, acc:0.517361\n",
      "epoch:  485 \tThe test accuracy is: 0.5173611111111112\n",
      " THE BEST ACCURACY IS 0.5173611111111112\tkappa is 0.3564814814814815\n",
      "subject 2 duration: 0:47:47.166522\n",
      "seed is 1726\n",
      "Subject 3\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "3_0 train_acc: 0.2976 train_loss: 0.003567\tval_acc: 0.462319 val_loss: 0.004452237, acc:0.475694\n",
      "3_1 train_acc: 0.4105 train_loss: 0.002673\tval_acc: 0.517391 val_loss: 0.004046270, acc:0.590278\n",
      "3_2 train_acc: 0.4796 train_loss: 0.002371\tval_acc: 0.545652 val_loss: 0.003894435, acc:0.614583\n",
      "3_3 train_acc: 0.5269 train_loss: 0.002199\tval_acc: 0.566667 val_loss: 0.003879769, acc:0.654514\n",
      "3_4 train_acc: 0.5511 train_loss: 0.002105\tval_acc: 0.589855 val_loss: 0.003708989, acc:0.682292\n",
      "3_5 train_acc: 0.5789 train_loss: 0.002017\tval_acc: 0.618841 val_loss: 0.003497204, acc:0.720486\n",
      "3_6 train_acc: 0.5918 train_loss: 0.001958\tval_acc: 0.645652 val_loss: 0.003372415, acc:0.739583\n",
      "3_7 train_acc: 0.6077 train_loss: 0.001895\tval_acc: 0.653623 val_loss: 0.003285440, acc:0.727431\n",
      "3_8 train_acc: 0.6228 train_loss: 0.001854\tval_acc: 0.652899 val_loss: 0.003215203, acc:0.762153\n",
      "3_9 train_acc: 0.6280 train_loss: 0.001824\tval_acc: 0.666667 val_loss: 0.003213847, acc:0.774306\n",
      "3_10 train_acc: 0.6362 train_loss: 0.001785\tval_acc: 0.669565 val_loss: 0.003091178, acc:0.763889\n",
      "3_11 train_acc: 0.6466 train_loss: 0.001752\tval_acc: 0.680435 val_loss: 0.003084304, acc:0.751736\n",
      "3_13 train_acc: 0.6498 train_loss: 0.001725\tval_acc: 0.676087 val_loss: 0.003035190, acc:0.729167\n",
      "3_14 train_acc: 0.6594 train_loss: 0.001697\tval_acc: 0.689855 val_loss: 0.002987278, acc:0.798611\n",
      "3_16 train_acc: 0.6590 train_loss: 0.001706\tval_acc: 0.693478 val_loss: 0.002933386, acc:0.809028\n",
      "3_18 train_acc: 0.6644 train_loss: 0.001672\tval_acc: 0.701449 val_loss: 0.002904349, acc:0.784722\n",
      "3_19 train_acc: 0.6707 train_loss: 0.001647\tval_acc: 0.705797 val_loss: 0.002849650, acc:0.788194\n",
      "3_21 train_acc: 0.6727 train_loss: 0.001635\tval_acc: 0.700725 val_loss: 0.002823518, acc:0.756944\n",
      "3_22 train_acc: 0.6682 train_loss: 0.001647\tval_acc: 0.714493 val_loss: 0.002821267, acc:0.795139\n",
      "3_23 train_acc: 0.6768 train_loss: 0.001609\tval_acc: 0.706522 val_loss: 0.002782365, acc:0.779514\n",
      "3_24 train_acc: 0.6815 train_loss: 0.001600\tval_acc: 0.721014 val_loss: 0.002775798, acc:0.814236\n",
      "3_28 train_acc: 0.6875 train_loss: 0.001582\tval_acc: 0.732609 val_loss: 0.002746167, acc:0.812500\n",
      "3_32 train_acc: 0.6884 train_loss: 0.001563\tval_acc: 0.740580 val_loss: 0.002665781, acc:0.809028\n",
      "3_39 train_acc: 0.7046 train_loss: 0.001503\tval_acc: 0.742754 val_loss: 0.002586365, acc:0.789931\n",
      "3_42 train_acc: 0.7060 train_loss: 0.001482\tval_acc: 0.736957 val_loss: 0.002573113, acc:0.828125\n",
      "3_44 train_acc: 0.7058 train_loss: 0.001489\tval_acc: 0.744928 val_loss: 0.002546398, acc:0.810764\n",
      "3_52 train_acc: 0.7230 train_loss: 0.001424\tval_acc: 0.744928 val_loss: 0.002526153, acc:0.819444\n",
      "3_54 train_acc: 0.7233 train_loss: 0.001428\tval_acc: 0.746377 val_loss: 0.002510717, acc:0.793403\n",
      "3_79 train_acc: 0.7233 train_loss: 0.001423\tval_acc: 0.760145 val_loss: 0.002486447, acc:0.821181\n",
      "3_80 train_acc: 0.7299 train_loss: 0.001385\tval_acc: 0.752899 val_loss: 0.002449101, acc:0.803819\n",
      "3_81 train_acc: 0.7274 train_loss: 0.001384\tval_acc: 0.767391 val_loss: 0.002426243, acc:0.831597\n",
      "3_85 train_acc: 0.7319 train_loss: 0.001382\tval_acc: 0.765942 val_loss: 0.002420206, acc:0.828125\n",
      "3_86 train_acc: 0.7357 train_loss: 0.001368\tval_acc: 0.755797 val_loss: 0.002384334, acc:0.809028\n",
      "3_101 train_acc: 0.7351 train_loss: 0.001361\tval_acc: 0.761594 val_loss: 0.002369014, acc:0.833333\n",
      "3_105 train_acc: 0.7380 train_loss: 0.001352\tval_acc: 0.770290 val_loss: 0.002360831, acc:0.845486\n",
      "3_106 train_acc: 0.7427 train_loss: 0.001332\tval_acc: 0.763768 val_loss: 0.002360193, acc:0.836806\n",
      "3_107 train_acc: 0.7437 train_loss: 0.001325\tval_acc: 0.771739 val_loss: 0.002337221, acc:0.821181\n",
      "3_108 train_acc: 0.7378 train_loss: 0.001344\tval_acc: 0.765942 val_loss: 0.002319895, acc:0.826389\n",
      "3_113 train_acc: 0.7450 train_loss: 0.001324\tval_acc: 0.769565 val_loss: 0.002318978, acc:0.829861\n",
      "3_114 train_acc: 0.7471 train_loss: 0.001308\tval_acc: 0.769565 val_loss: 0.002308951, acc:0.819444\n",
      "3_121 train_acc: 0.7518 train_loss: 0.001288\tval_acc: 0.773913 val_loss: 0.002301431, acc:0.819444\n",
      "3_126 train_acc: 0.7496 train_loss: 0.001302\tval_acc: 0.773913 val_loss: 0.002258228, acc:0.758681\n",
      "3_134 train_acc: 0.7492 train_loss: 0.001307\tval_acc: 0.771739 val_loss: 0.002243362, acc:0.805556\n",
      "3_143 train_acc: 0.7491 train_loss: 0.001289\tval_acc: 0.776812 val_loss: 0.002237176, acc:0.779514\n",
      "3_144 train_acc: 0.7518 train_loss: 0.001288\tval_acc: 0.782609 val_loss: 0.002219251, acc:0.809028\n",
      "3_148 train_acc: 0.7544 train_loss: 0.001265\tval_acc: 0.775362 val_loss: 0.002217045, acc:0.805556\n",
      "3_156 train_acc: 0.7535 train_loss: 0.001284\tval_acc: 0.781159 val_loss: 0.002178515, acc:0.784722\n",
      "3_167 train_acc: 0.7618 train_loss: 0.001249\tval_acc: 0.789855 val_loss: 0.002163811, acc:0.802083\n",
      "3_191 train_acc: 0.7628 train_loss: 0.001232\tval_acc: 0.786232 val_loss: 0.002134247, acc:0.798611\n",
      "3_196 train_acc: 0.7691 train_loss: 0.001214\tval_acc: 0.786957 val_loss: 0.002117066, acc:0.807292\n",
      "3_203 train_acc: 0.7675 train_loss: 0.001206\tval_acc: 0.783333 val_loss: 0.002091555, acc:0.805556\n",
      "3_211 train_acc: 0.7707 train_loss: 0.001199\tval_acc: 0.782609 val_loss: 0.002076600, acc:0.819444\n",
      "3_212 train_acc: 0.7628 train_loss: 0.001201\tval_acc: 0.789855 val_loss: 0.002069730, acc:0.798611\n",
      "3_213 train_acc: 0.7698 train_loss: 0.001199\tval_acc: 0.788406 val_loss: 0.002065187, acc:0.815972\n",
      "3_215 train_acc: 0.7693 train_loss: 0.001191\tval_acc: 0.796377 val_loss: 0.002009172, acc:0.828125\n",
      "3_217 train_acc: 0.7701 train_loss: 0.001184\tval_acc: 0.802174 val_loss: 0.001984048, acc:0.821181\n",
      "3_246 train_acc: 0.7719 train_loss: 0.001171\tval_acc: 0.792754 val_loss: 0.001970117, acc:0.822917\n",
      "3_255 train_acc: 0.7765 train_loss: 0.001151\tval_acc: 0.798551 val_loss: 0.001951396, acc:0.817708\n",
      "3_256 train_acc: 0.7784 train_loss: 0.001141\tval_acc: 0.806522 val_loss: 0.001936833, acc:0.833333\n",
      "3_272 train_acc: 0.7792 train_loss: 0.001137\tval_acc: 0.798551 val_loss: 0.001934079, acc:0.812500\n",
      "3_299 train_acc: 0.7827 train_loss: 0.001119\tval_acc: 0.802174 val_loss: 0.001927710, acc:0.840278\n",
      "3_306 train_acc: 0.7798 train_loss: 0.001135\tval_acc: 0.800000 val_loss: 0.001904155, acc:0.822917\n",
      "3_347 train_acc: 0.7872 train_loss: 0.001106\tval_acc: 0.804348 val_loss: 0.001903181, acc:0.819444\n",
      "3_360 train_acc: 0.7915 train_loss: 0.001089\tval_acc: 0.809420 val_loss: 0.001884021, acc:0.809028\n",
      "3_382 train_acc: 0.7898 train_loss: 0.001099\tval_acc: 0.805072 val_loss: 0.001883493, acc:0.833333\n",
      "3_389 train_acc: 0.7916 train_loss: 0.001083\tval_acc: 0.802174 val_loss: 0.001882346, acc:0.831597\n",
      "3_392 train_acc: 0.7929 train_loss: 0.001074\tval_acc: 0.810870 val_loss: 0.001862546, acc:0.842014\n",
      "3_396 train_acc: 0.7920 train_loss: 0.001080\tval_acc: 0.806522 val_loss: 0.001860577, acc:0.833333\n",
      "3_407 train_acc: 0.7919 train_loss: 0.001076\tval_acc: 0.807971 val_loss: 0.001842482, acc:0.828125\n",
      "3_462 train_acc: 0.7983 train_loss: 0.001056\tval_acc: 0.818841 val_loss: 0.001830513, acc:0.836806\n",
      "3_483 train_acc: 0.7976 train_loss: 0.001056\tval_acc: 0.813768 val_loss: 0.001808445, acc:0.815972\n",
      "3_594 train_acc: 0.7981 train_loss: 0.001045\tval_acc: 0.819565 val_loss: 0.001794055, acc:0.848958\n",
      "epoch:  594 \tThe test accuracy is: 0.8489583333333334\n",
      " THE BEST ACCURACY IS 0.8489583333333334\tkappa is 0.7986111111111112\n",
      "subject 3 duration: 0:47:47.074829\n",
      "seed is 1159\n",
      "Subject 4\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "4_0 train_acc: 0.2914 train_loss: 0.003449\tval_acc: 0.387681 val_loss: 0.004738312, acc:0.279514\n",
      "4_1 train_acc: 0.3781 train_loss: 0.002747\tval_acc: 0.526087 val_loss: 0.004132582, acc:0.411458\n",
      "4_2 train_acc: 0.4712 train_loss: 0.002404\tval_acc: 0.552174 val_loss: 0.003880353, acc:0.413194\n",
      "4_3 train_acc: 0.5214 train_loss: 0.002216\tval_acc: 0.578261 val_loss: 0.003781393, acc:0.434028\n",
      "4_4 train_acc: 0.5534 train_loss: 0.002107\tval_acc: 0.588406 val_loss: 0.003664937, acc:0.434028\n",
      "4_5 train_acc: 0.5769 train_loss: 0.002029\tval_acc: 0.618841 val_loss: 0.003477913, acc:0.506944\n",
      "4_6 train_acc: 0.5902 train_loss: 0.001963\tval_acc: 0.615217 val_loss: 0.003400835, acc:0.434028\n",
      "4_7 train_acc: 0.6027 train_loss: 0.001915\tval_acc: 0.638406 val_loss: 0.003363929, acc:0.454861\n",
      "4_8 train_acc: 0.6268 train_loss: 0.001843\tval_acc: 0.652174 val_loss: 0.003308020, acc:0.413194\n",
      "4_9 train_acc: 0.6345 train_loss: 0.001800\tval_acc: 0.667391 val_loss: 0.003176530, acc:0.446181\n",
      "4_10 train_acc: 0.6519 train_loss: 0.001743\tval_acc: 0.688406 val_loss: 0.003051580, acc:0.467014\n",
      "4_11 train_acc: 0.6538 train_loss: 0.001727\tval_acc: 0.678986 val_loss: 0.003018637, acc:0.453125\n",
      "4_14 train_acc: 0.6746 train_loss: 0.001629\tval_acc: 0.720290 val_loss: 0.002856399, acc:0.484375\n",
      "4_16 train_acc: 0.6868 train_loss: 0.001592\tval_acc: 0.712319 val_loss: 0.002824784, acc:0.467014\n",
      "4_20 train_acc: 0.7023 train_loss: 0.001513\tval_acc: 0.728986 val_loss: 0.002669169, acc:0.527778\n",
      "4_22 train_acc: 0.7078 train_loss: 0.001484\tval_acc: 0.734783 val_loss: 0.002642524, acc:0.543403\n",
      "4_23 train_acc: 0.7160 train_loss: 0.001465\tval_acc: 0.751449 val_loss: 0.002536182, acc:0.545139\n",
      "4_28 train_acc: 0.7210 train_loss: 0.001423\tval_acc: 0.741304 val_loss: 0.002463457, acc:0.532986\n",
      "4_35 train_acc: 0.7358 train_loss: 0.001371\tval_acc: 0.759420 val_loss: 0.002434206, acc:0.545139\n",
      "4_41 train_acc: 0.7457 train_loss: 0.001335\tval_acc: 0.766667 val_loss: 0.002406460, acc:0.515625\n",
      "4_42 train_acc: 0.7462 train_loss: 0.001360\tval_acc: 0.763043 val_loss: 0.002383229, acc:0.555556\n",
      "4_45 train_acc: 0.7515 train_loss: 0.001306\tval_acc: 0.773188 val_loss: 0.002331380, acc:0.576389\n",
      "4_52 train_acc: 0.7507 train_loss: 0.001286\tval_acc: 0.775362 val_loss: 0.002271610, acc:0.548611\n",
      "4_53 train_acc: 0.7529 train_loss: 0.001285\tval_acc: 0.773913 val_loss: 0.002269593, acc:0.527778\n",
      "4_59 train_acc: 0.7540 train_loss: 0.001272\tval_acc: 0.766667 val_loss: 0.002246241, acc:0.534722\n",
      "4_61 train_acc: 0.7563 train_loss: 0.001277\tval_acc: 0.781884 val_loss: 0.002221016, acc:0.555556\n",
      "4_62 train_acc: 0.7604 train_loss: 0.001242\tval_acc: 0.778261 val_loss: 0.002208091, acc:0.555556\n",
      "4_63 train_acc: 0.7602 train_loss: 0.001257\tval_acc: 0.784783 val_loss: 0.002184511, acc:0.576389\n",
      "4_71 train_acc: 0.7680 train_loss: 0.001229\tval_acc: 0.797101 val_loss: 0.002128054, acc:0.520833\n",
      "4_78 train_acc: 0.7686 train_loss: 0.001210\tval_acc: 0.794203 val_loss: 0.002116843, acc:0.539931\n",
      "4_85 train_acc: 0.7725 train_loss: 0.001194\tval_acc: 0.782609 val_loss: 0.002107022, acc:0.555556\n",
      "4_88 train_acc: 0.7766 train_loss: 0.001195\tval_acc: 0.790580 val_loss: 0.002102482, acc:0.545139\n",
      "4_95 train_acc: 0.7736 train_loss: 0.001182\tval_acc: 0.787681 val_loss: 0.002101424, acc:0.529514\n",
      "4_99 train_acc: 0.7764 train_loss: 0.001158\tval_acc: 0.796377 val_loss: 0.002076472, acc:0.529514\n",
      "4_100 train_acc: 0.7811 train_loss: 0.001157\tval_acc: 0.792029 val_loss: 0.002063198, acc:0.510417\n",
      "4_107 train_acc: 0.7800 train_loss: 0.001157\tval_acc: 0.781159 val_loss: 0.002042813, acc:0.534722\n",
      "4_110 train_acc: 0.7775 train_loss: 0.001179\tval_acc: 0.807246 val_loss: 0.002009437, acc:0.569444\n",
      "4_114 train_acc: 0.7827 train_loss: 0.001151\tval_acc: 0.802174 val_loss: 0.001987504, acc:0.559028\n",
      "4_125 train_acc: 0.7789 train_loss: 0.001149\tval_acc: 0.803623 val_loss: 0.001970997, acc:0.592014\n",
      "4_128 train_acc: 0.7840 train_loss: 0.001130\tval_acc: 0.813768 val_loss: 0.001898632, acc:0.588542\n",
      "4_147 train_acc: 0.7866 train_loss: 0.001130\tval_acc: 0.813768 val_loss: 0.001860326, acc:0.600694\n",
      "4_159 train_acc: 0.7927 train_loss: 0.001096\tval_acc: 0.818116 val_loss: 0.001841216, acc:0.597222\n",
      "4_172 train_acc: 0.7976 train_loss: 0.001074\tval_acc: 0.815217 val_loss: 0.001827628, acc:0.583333\n",
      "4_184 train_acc: 0.7978 train_loss: 0.001068\tval_acc: 0.817391 val_loss: 0.001825160, acc:0.536458\n",
      "4_186 train_acc: 0.7992 train_loss: 0.001057\tval_acc: 0.831884 val_loss: 0.001792932, acc:0.576389\n",
      "4_189 train_acc: 0.8011 train_loss: 0.001077\tval_acc: 0.824638 val_loss: 0.001786976, acc:0.578125\n",
      "4_191 train_acc: 0.8026 train_loss: 0.001058\tval_acc: 0.821739 val_loss: 0.001772391, acc:0.579861\n",
      "4_199 train_acc: 0.8020 train_loss: 0.001060\tval_acc: 0.821014 val_loss: 0.001770107, acc:0.581597\n",
      "4_203 train_acc: 0.8023 train_loss: 0.001052\tval_acc: 0.821014 val_loss: 0.001752350, acc:0.574653\n",
      "4_206 train_acc: 0.8033 train_loss: 0.001047\tval_acc: 0.826812 val_loss: 0.001734829, acc:0.576389\n",
      "4_237 train_acc: 0.8084 train_loss: 0.001014\tval_acc: 0.826087 val_loss: 0.001705837, acc:0.588542\n",
      "4_254 train_acc: 0.8102 train_loss: 0.000999\tval_acc: 0.828261 val_loss: 0.001681551, acc:0.519097\n",
      "4_261 train_acc: 0.8116 train_loss: 0.001008\tval_acc: 0.826812 val_loss: 0.001657043, acc:0.574653\n",
      "4_338 train_acc: 0.8128 train_loss: 0.000998\tval_acc: 0.834058 val_loss: 0.001645242, acc:0.536458\n",
      "4_384 train_acc: 0.8179 train_loss: 0.000978\tval_acc: 0.828261 val_loss: 0.001638305, acc:0.600694\n",
      "4_388 train_acc: 0.8137 train_loss: 0.000977\tval_acc: 0.836232 val_loss: 0.001638061, acc:0.579861\n",
      "4_394 train_acc: 0.8174 train_loss: 0.000969\tval_acc: 0.834058 val_loss: 0.001627161, acc:0.557292\n",
      "4_406 train_acc: 0.8209 train_loss: 0.000956\tval_acc: 0.839855 val_loss: 0.001612582, acc:0.552083\n",
      "4_414 train_acc: 0.8234 train_loss: 0.000953\tval_acc: 0.839855 val_loss: 0.001598822, acc:0.567708\n",
      "4_420 train_acc: 0.8231 train_loss: 0.000959\tval_acc: 0.841304 val_loss: 0.001595772, acc:0.574653\n",
      "4_539 train_acc: 0.8248 train_loss: 0.000921\tval_acc: 0.841304 val_loss: 0.001573929, acc:0.602431\n",
      "epoch:  539 \tThe test accuracy is: 0.6024305555555556\n",
      " THE BEST ACCURACY IS 0.6024305555555556\tkappa is 0.46990740740740744\n",
      "subject 4 duration: 0:47:41.154658\n",
      "seed is 540\n",
      "Subject 5\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "5_0 train_acc: 0.2956 train_loss: 0.003410\tval_acc: 0.342754 val_loss: 0.004849016, acc:0.263889\n",
      "5_1 train_acc: 0.3964 train_loss: 0.002654\tval_acc: 0.500725 val_loss: 0.004227149, acc:0.369792\n",
      "5_2 train_acc: 0.4862 train_loss: 0.002306\tval_acc: 0.585507 val_loss: 0.003828602, acc:0.439236\n",
      "5_3 train_acc: 0.5457 train_loss: 0.002120\tval_acc: 0.591304 val_loss: 0.003588786, acc:0.406250\n",
      "5_4 train_acc: 0.5802 train_loss: 0.001992\tval_acc: 0.622464 val_loss: 0.003563902, acc:0.456597\n",
      "5_5 train_acc: 0.6057 train_loss: 0.001902\tval_acc: 0.642029 val_loss: 0.003371772, acc:0.449653\n",
      "5_6 train_acc: 0.6234 train_loss: 0.001830\tval_acc: 0.657971 val_loss: 0.003192375, acc:0.432292\n",
      "5_7 train_acc: 0.6432 train_loss: 0.001773\tval_acc: 0.670290 val_loss: 0.003115219, acc:0.414931\n",
      "5_8 train_acc: 0.6534 train_loss: 0.001729\tval_acc: 0.678261 val_loss: 0.003106514, acc:0.420139\n",
      "5_10 train_acc: 0.6731 train_loss: 0.001661\tval_acc: 0.689855 val_loss: 0.002813880, acc:0.430556\n",
      "5_13 train_acc: 0.6820 train_loss: 0.001582\tval_acc: 0.697826 val_loss: 0.002788738, acc:0.381944\n",
      "5_14 train_acc: 0.6885 train_loss: 0.001564\tval_acc: 0.707246 val_loss: 0.002777425, acc:0.375000\n",
      "5_17 train_acc: 0.7053 train_loss: 0.001495\tval_acc: 0.718116 val_loss: 0.002726899, acc:0.440972\n",
      "5_18 train_acc: 0.7090 train_loss: 0.001485\tval_acc: 0.729710 val_loss: 0.002623745, acc:0.487847\n",
      "5_19 train_acc: 0.7119 train_loss: 0.001465\tval_acc: 0.755797 val_loss: 0.002426036, acc:0.510417\n",
      "5_22 train_acc: 0.7176 train_loss: 0.001455\tval_acc: 0.744203 val_loss: 0.002398437, acc:0.435764\n",
      "5_27 train_acc: 0.7314 train_loss: 0.001374\tval_acc: 0.768116 val_loss: 0.002285216, acc:0.444444\n",
      "5_29 train_acc: 0.7289 train_loss: 0.001376\tval_acc: 0.773913 val_loss: 0.002246313, acc:0.430556\n",
      "5_38 train_acc: 0.7464 train_loss: 0.001298\tval_acc: 0.770290 val_loss: 0.002198146, acc:0.435764\n",
      "5_44 train_acc: 0.7502 train_loss: 0.001283\tval_acc: 0.774638 val_loss: 0.002168887, acc:0.425347\n",
      "5_53 train_acc: 0.7587 train_loss: 0.001256\tval_acc: 0.772464 val_loss: 0.002140648, acc:0.432292\n",
      "5_56 train_acc: 0.7651 train_loss: 0.001214\tval_acc: 0.781159 val_loss: 0.002128005, acc:0.432292\n",
      "5_57 train_acc: 0.7648 train_loss: 0.001226\tval_acc: 0.784783 val_loss: 0.002043703, acc:0.407986\n",
      "5_72 train_acc: 0.7735 train_loss: 0.001184\tval_acc: 0.803623 val_loss: 0.001975316, acc:0.444444\n",
      "5_78 train_acc: 0.7784 train_loss: 0.001170\tval_acc: 0.795652 val_loss: 0.001954930, acc:0.427083\n",
      "5_87 train_acc: 0.7837 train_loss: 0.001138\tval_acc: 0.798551 val_loss: 0.001926764, acc:0.437500\n",
      "5_95 train_acc: 0.7875 train_loss: 0.001125\tval_acc: 0.793478 val_loss: 0.001922103, acc:0.434028\n",
      "5_121 train_acc: 0.7847 train_loss: 0.001124\tval_acc: 0.801449 val_loss: 0.001921554, acc:0.437500\n",
      "5_132 train_acc: 0.7862 train_loss: 0.001118\tval_acc: 0.807971 val_loss: 0.001917377, acc:0.442708\n",
      "5_136 train_acc: 0.7912 train_loss: 0.001099\tval_acc: 0.803623 val_loss: 0.001856334, acc:0.428819\n",
      "5_141 train_acc: 0.7946 train_loss: 0.001076\tval_acc: 0.811594 val_loss: 0.001855864, acc:0.454861\n",
      "5_147 train_acc: 0.7911 train_loss: 0.001088\tval_acc: 0.809420 val_loss: 0.001838677, acc:0.414931\n",
      "5_172 train_acc: 0.7955 train_loss: 0.001071\tval_acc: 0.813768 val_loss: 0.001836272, acc:0.376736\n",
      "5_174 train_acc: 0.7997 train_loss: 0.001054\tval_acc: 0.810145 val_loss: 0.001832284, acc:0.366319\n",
      "5_185 train_acc: 0.8049 train_loss: 0.001030\tval_acc: 0.821739 val_loss: 0.001812313, acc:0.373264\n",
      "5_188 train_acc: 0.8065 train_loss: 0.001019\tval_acc: 0.812319 val_loss: 0.001803364, acc:0.371528\n",
      "5_199 train_acc: 0.8048 train_loss: 0.001020\tval_acc: 0.813043 val_loss: 0.001786586, acc:0.354167\n",
      "5_203 train_acc: 0.8035 train_loss: 0.001029\tval_acc: 0.820290 val_loss: 0.001785531, acc:0.368056\n",
      "5_222 train_acc: 0.8110 train_loss: 0.001012\tval_acc: 0.819565 val_loss: 0.001765040, acc:0.350694\n",
      "5_248 train_acc: 0.8104 train_loss: 0.000995\tval_acc: 0.828986 val_loss: 0.001763645, acc:0.347222\n",
      "5_253 train_acc: 0.8160 train_loss: 0.000977\tval_acc: 0.826812 val_loss: 0.001762844, acc:0.354167\n",
      "5_286 train_acc: 0.8204 train_loss: 0.000965\tval_acc: 0.828261 val_loss: 0.001734948, acc:0.355903\n",
      "5_301 train_acc: 0.8229 train_loss: 0.000945\tval_acc: 0.832609 val_loss: 0.001706809, acc:0.359375\n",
      "5_343 train_acc: 0.8272 train_loss: 0.000919\tval_acc: 0.831884 val_loss: 0.001688335, acc:0.348958\n",
      "5_351 train_acc: 0.8221 train_loss: 0.000929\tval_acc: 0.836957 val_loss: 0.001649832, acc:0.395833\n",
      "5_445 train_acc: 0.8288 train_loss: 0.000903\tval_acc: 0.834058 val_loss: 0.001642546, acc:0.449653\n",
      "5_476 train_acc: 0.8322 train_loss: 0.000883\tval_acc: 0.842754 val_loss: 0.001637651, acc:0.435764\n",
      "5_520 train_acc: 0.8294 train_loss: 0.000893\tval_acc: 0.831884 val_loss: 0.001599012, acc:0.442708\n",
      "epoch:  520 \tThe test accuracy is: 0.4427083333333333\n",
      " THE BEST ACCURACY IS 0.4427083333333333\tkappa is 0.2569444444444444\n",
      "subject 5 duration: 0:47:36.986800\n",
      "seed is 1216\n",
      "Subject 6\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "6_0 train_acc: 0.2936 train_loss: 0.003465\tval_acc: 0.444928 val_loss: 0.004462424, acc:0.402778\n",
      "6_1 train_acc: 0.3950 train_loss: 0.002680\tval_acc: 0.536232 val_loss: 0.004005600, acc:0.409722\n",
      "6_2 train_acc: 0.4725 train_loss: 0.002386\tval_acc: 0.551449 val_loss: 0.003807974, acc:0.421875\n",
      "6_3 train_acc: 0.5178 train_loss: 0.002221\tval_acc: 0.584783 val_loss: 0.003594209, acc:0.446181\n",
      "6_4 train_acc: 0.5493 train_loss: 0.002120\tval_acc: 0.595652 val_loss: 0.003492239, acc:0.451389\n",
      "6_5 train_acc: 0.5755 train_loss: 0.002019\tval_acc: 0.616667 val_loss: 0.003349417, acc:0.421875\n",
      "6_6 train_acc: 0.5971 train_loss: 0.001932\tval_acc: 0.648551 val_loss: 0.003170705, acc:0.432292\n",
      "6_7 train_acc: 0.6208 train_loss: 0.001858\tval_acc: 0.667391 val_loss: 0.003030500, acc:0.440972\n",
      "6_8 train_acc: 0.6408 train_loss: 0.001773\tval_acc: 0.670290 val_loss: 0.002981700, acc:0.456597\n",
      "6_9 train_acc: 0.6498 train_loss: 0.001735\tval_acc: 0.688406 val_loss: 0.002894520, acc:0.453125\n",
      "6_11 train_acc: 0.6623 train_loss: 0.001665\tval_acc: 0.688406 val_loss: 0.002874835, acc:0.454861\n",
      "6_12 train_acc: 0.6736 train_loss: 0.001626\tval_acc: 0.694928 val_loss: 0.002786724, acc:0.440972\n",
      "6_13 train_acc: 0.6786 train_loss: 0.001614\tval_acc: 0.702174 val_loss: 0.002741506, acc:0.414931\n",
      "6_16 train_acc: 0.6908 train_loss: 0.001536\tval_acc: 0.709420 val_loss: 0.002715364, acc:0.428819\n",
      "6_19 train_acc: 0.7008 train_loss: 0.001507\tval_acc: 0.719565 val_loss: 0.002607357, acc:0.428819\n",
      "6_22 train_acc: 0.7149 train_loss: 0.001455\tval_acc: 0.734058 val_loss: 0.002516333, acc:0.427083\n",
      "6_29 train_acc: 0.7251 train_loss: 0.001393\tval_acc: 0.747101 val_loss: 0.002485126, acc:0.385417\n",
      "6_31 train_acc: 0.7329 train_loss: 0.001376\tval_acc: 0.734783 val_loss: 0.002456291, acc:0.435764\n",
      "6_32 train_acc: 0.7332 train_loss: 0.001376\tval_acc: 0.750000 val_loss: 0.002434517, acc:0.434028\n",
      "6_34 train_acc: 0.7324 train_loss: 0.001365\tval_acc: 0.749275 val_loss: 0.002403395, acc:0.402778\n",
      "6_35 train_acc: 0.7401 train_loss: 0.001339\tval_acc: 0.751449 val_loss: 0.002366631, acc:0.416667\n",
      "6_38 train_acc: 0.7372 train_loss: 0.001344\tval_acc: 0.766667 val_loss: 0.002282562, acc:0.406250\n",
      "6_46 train_acc: 0.7491 train_loss: 0.001293\tval_acc: 0.765217 val_loss: 0.002265601, acc:0.401042\n",
      "6_48 train_acc: 0.7524 train_loss: 0.001286\tval_acc: 0.771014 val_loss: 0.002210981, acc:0.399306\n",
      "6_57 train_acc: 0.7589 train_loss: 0.001235\tval_acc: 0.773913 val_loss: 0.002158878, acc:0.401042\n",
      "6_58 train_acc: 0.7602 train_loss: 0.001241\tval_acc: 0.778986 val_loss: 0.002158281, acc:0.388889\n",
      "6_67 train_acc: 0.7677 train_loss: 0.001200\tval_acc: 0.778261 val_loss: 0.002154349, acc:0.414931\n",
      "6_68 train_acc: 0.7666 train_loss: 0.001189\tval_acc: 0.783333 val_loss: 0.002087678, acc:0.421875\n",
      "6_77 train_acc: 0.7694 train_loss: 0.001173\tval_acc: 0.783333 val_loss: 0.002085514, acc:0.421875\n",
      "6_81 train_acc: 0.7837 train_loss: 0.001139\tval_acc: 0.797101 val_loss: 0.001999365, acc:0.413194\n",
      "6_88 train_acc: 0.7860 train_loss: 0.001130\tval_acc: 0.794203 val_loss: 0.001997101, acc:0.439236\n",
      "6_97 train_acc: 0.7842 train_loss: 0.001122\tval_acc: 0.797826 val_loss: 0.001995936, acc:0.428819\n",
      "6_99 train_acc: 0.7865 train_loss: 0.001110\tval_acc: 0.794203 val_loss: 0.001973847, acc:0.420139\n",
      "6_103 train_acc: 0.7869 train_loss: 0.001102\tval_acc: 0.796377 val_loss: 0.001938732, acc:0.439236\n",
      "6_107 train_acc: 0.7871 train_loss: 0.001100\tval_acc: 0.792754 val_loss: 0.001919103, acc:0.416667\n",
      "6_122 train_acc: 0.7933 train_loss: 0.001076\tval_acc: 0.808696 val_loss: 0.001889024, acc:0.413194\n",
      "6_159 train_acc: 0.7994 train_loss: 0.001046\tval_acc: 0.810145 val_loss: 0.001871656, acc:0.446181\n",
      "6_161 train_acc: 0.8073 train_loss: 0.001030\tval_acc: 0.807971 val_loss: 0.001868634, acc:0.432292\n",
      "6_165 train_acc: 0.8038 train_loss: 0.001026\tval_acc: 0.810145 val_loss: 0.001819985, acc:0.428819\n",
      "6_192 train_acc: 0.8121 train_loss: 0.000992\tval_acc: 0.813768 val_loss: 0.001810695, acc:0.427083\n",
      "6_207 train_acc: 0.8110 train_loss: 0.000997\tval_acc: 0.813043 val_loss: 0.001795727, acc:0.409722\n",
      "6_209 train_acc: 0.8091 train_loss: 0.001005\tval_acc: 0.817391 val_loss: 0.001792481, acc:0.414931\n",
      "6_216 train_acc: 0.8125 train_loss: 0.000997\tval_acc: 0.812319 val_loss: 0.001780643, acc:0.420139\n",
      "6_232 train_acc: 0.8118 train_loss: 0.000972\tval_acc: 0.823188 val_loss: 0.001778779, acc:0.425347\n",
      "6_233 train_acc: 0.8157 train_loss: 0.000967\tval_acc: 0.826812 val_loss: 0.001777450, acc:0.423611\n",
      "6_243 train_acc: 0.8160 train_loss: 0.000974\tval_acc: 0.826812 val_loss: 0.001751326, acc:0.409722\n",
      "6_244 train_acc: 0.8207 train_loss: 0.000960\tval_acc: 0.823188 val_loss: 0.001742798, acc:0.388889\n",
      "6_251 train_acc: 0.8196 train_loss: 0.000951\tval_acc: 0.817391 val_loss: 0.001738398, acc:0.401042\n",
      "6_289 train_acc: 0.8221 train_loss: 0.000955\tval_acc: 0.825362 val_loss: 0.001737444, acc:0.416667\n",
      "6_302 train_acc: 0.8234 train_loss: 0.000939\tval_acc: 0.828986 val_loss: 0.001726062, acc:0.430556\n",
      "6_315 train_acc: 0.8249 train_loss: 0.000925\tval_acc: 0.827536 val_loss: 0.001708733, acc:0.428819\n",
      "6_335 train_acc: 0.8234 train_loss: 0.000924\tval_acc: 0.833333 val_loss: 0.001706099, acc:0.411458\n",
      "6_407 train_acc: 0.8275 train_loss: 0.000907\tval_acc: 0.834783 val_loss: 0.001695902, acc:0.388889\n",
      "6_410 train_acc: 0.8298 train_loss: 0.000901\tval_acc: 0.829710 val_loss: 0.001684711, acc:0.407986\n",
      "6_477 train_acc: 0.8296 train_loss: 0.000904\tval_acc: 0.829710 val_loss: 0.001666080, acc:0.416667\n",
      "6_504 train_acc: 0.8334 train_loss: 0.000887\tval_acc: 0.827536 val_loss: 0.001661346, acc:0.383681\n",
      "6_528 train_acc: 0.8314 train_loss: 0.000896\tval_acc: 0.834058 val_loss: 0.001647115, acc:0.397569\n",
      "6_538 train_acc: 0.8363 train_loss: 0.000872\tval_acc: 0.833333 val_loss: 0.001630464, acc:0.411458\n",
      "epoch:  538 \tThe test accuracy is: 0.4114583333333333\n",
      " THE BEST ACCURACY IS 0.4114583333333333\tkappa is 0.2152777777777778\n",
      "subject 6 duration: 0:47:38.606781\n",
      "seed is 1334\n",
      "Subject 7\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "7_0 train_acc: 0.2850 train_loss: 0.003449\tval_acc: 0.344928 val_loss: 0.004756112, acc:0.302083\n",
      "7_1 train_acc: 0.3734 train_loss: 0.002740\tval_acc: 0.469565 val_loss: 0.004191993, acc:0.470486\n",
      "7_2 train_acc: 0.4585 train_loss: 0.002414\tval_acc: 0.502899 val_loss: 0.003948646, acc:0.552083\n",
      "7_3 train_acc: 0.5028 train_loss: 0.002272\tval_acc: 0.523913 val_loss: 0.003875641, acc:0.557292\n",
      "7_4 train_acc: 0.5269 train_loss: 0.002176\tval_acc: 0.561594 val_loss: 0.003854396, acc:0.562500\n",
      "7_5 train_acc: 0.5612 train_loss: 0.002078\tval_acc: 0.593478 val_loss: 0.003605203, acc:0.581597\n",
      "7_6 train_acc: 0.5955 train_loss: 0.001950\tval_acc: 0.618116 val_loss: 0.003447929, acc:0.607639\n",
      "7_7 train_acc: 0.6122 train_loss: 0.001888\tval_acc: 0.615942 val_loss: 0.003425987, acc:0.647569\n",
      "7_8 train_acc: 0.6236 train_loss: 0.001834\tval_acc: 0.650000 val_loss: 0.003231259, acc:0.635417\n",
      "7_11 train_acc: 0.6555 train_loss: 0.001721\tval_acc: 0.650725 val_loss: 0.003197752, acc:0.623264\n",
      "7_13 train_acc: 0.6654 train_loss: 0.001678\tval_acc: 0.686232 val_loss: 0.003054140, acc:0.677083\n",
      "7_16 train_acc: 0.6822 train_loss: 0.001611\tval_acc: 0.684058 val_loss: 0.003020825, acc:0.644097\n",
      "7_17 train_acc: 0.6843 train_loss: 0.001584\tval_acc: 0.699275 val_loss: 0.002913449, acc:0.692708\n",
      "7_19 train_acc: 0.6936 train_loss: 0.001566\tval_acc: 0.714493 val_loss: 0.002849339, acc:0.710069\n",
      "7_26 train_acc: 0.7051 train_loss: 0.001509\tval_acc: 0.707246 val_loss: 0.002772681, acc:0.696181\n",
      "7_28 train_acc: 0.7055 train_loss: 0.001501\tval_acc: 0.715942 val_loss: 0.002728735, acc:0.711806\n",
      "7_32 train_acc: 0.7142 train_loss: 0.001457\tval_acc: 0.735507 val_loss: 0.002718814, acc:0.699653\n",
      "7_35 train_acc: 0.7243 train_loss: 0.001432\tval_acc: 0.737681 val_loss: 0.002620523, acc:0.722222\n",
      "7_47 train_acc: 0.7294 train_loss: 0.001384\tval_acc: 0.742754 val_loss: 0.002594133, acc:0.690972\n",
      "7_48 train_acc: 0.7297 train_loss: 0.001384\tval_acc: 0.744928 val_loss: 0.002548105, acc:0.729167\n",
      "7_54 train_acc: 0.7407 train_loss: 0.001348\tval_acc: 0.748551 val_loss: 0.002546323, acc:0.722222\n",
      "7_58 train_acc: 0.7375 train_loss: 0.001346\tval_acc: 0.736232 val_loss: 0.002537765, acc:0.763889\n",
      "7_59 train_acc: 0.7401 train_loss: 0.001332\tval_acc: 0.737681 val_loss: 0.002488505, acc:0.694444\n",
      "7_62 train_acc: 0.7407 train_loss: 0.001336\tval_acc: 0.740580 val_loss: 0.002459376, acc:0.734375\n",
      "7_67 train_acc: 0.7451 train_loss: 0.001322\tval_acc: 0.746377 val_loss: 0.002412516, acc:0.753472\n",
      "7_76 train_acc: 0.7487 train_loss: 0.001296\tval_acc: 0.765942 val_loss: 0.002390053, acc:0.734375\n",
      "7_78 train_acc: 0.7478 train_loss: 0.001317\tval_acc: 0.752899 val_loss: 0.002385912, acc:0.715278\n",
      "7_82 train_acc: 0.7540 train_loss: 0.001274\tval_acc: 0.758696 val_loss: 0.002354646, acc:0.736111\n",
      "7_88 train_acc: 0.7543 train_loss: 0.001264\tval_acc: 0.757971 val_loss: 0.002353076, acc:0.730903\n",
      "7_92 train_acc: 0.7534 train_loss: 0.001264\tval_acc: 0.755072 val_loss: 0.002349780, acc:0.751736\n",
      "7_95 train_acc: 0.7572 train_loss: 0.001263\tval_acc: 0.748551 val_loss: 0.002302797, acc:0.755208\n",
      "7_100 train_acc: 0.7577 train_loss: 0.001241\tval_acc: 0.766667 val_loss: 0.002240105, acc:0.751736\n",
      "7_128 train_acc: 0.7695 train_loss: 0.001208\tval_acc: 0.766667 val_loss: 0.002208496, acc:0.769097\n",
      "7_137 train_acc: 0.7674 train_loss: 0.001221\tval_acc: 0.772464 val_loss: 0.002198792, acc:0.729167\n",
      "7_160 train_acc: 0.7757 train_loss: 0.001179\tval_acc: 0.765942 val_loss: 0.002188522, acc:0.772569\n",
      "7_164 train_acc: 0.7736 train_loss: 0.001188\tval_acc: 0.784058 val_loss: 0.002149475, acc:0.734375\n",
      "7_166 train_acc: 0.7774 train_loss: 0.001168\tval_acc: 0.789130 val_loss: 0.002129693, acc:0.713542\n",
      "7_175 train_acc: 0.7815 train_loss: 0.001154\tval_acc: 0.772464 val_loss: 0.002110377, acc:0.748264\n",
      "7_219 train_acc: 0.7883 train_loss: 0.001116\tval_acc: 0.772464 val_loss: 0.002096686, acc:0.729167\n",
      "7_224 train_acc: 0.7874 train_loss: 0.001115\tval_acc: 0.784058 val_loss: 0.002095749, acc:0.722222\n",
      "7_225 train_acc: 0.7891 train_loss: 0.001105\tval_acc: 0.784058 val_loss: 0.002065340, acc:0.743056\n",
      "7_229 train_acc: 0.7894 train_loss: 0.001100\tval_acc: 0.787681 val_loss: 0.002045283, acc:0.736111\n",
      "7_252 train_acc: 0.7973 train_loss: 0.001072\tval_acc: 0.797101 val_loss: 0.002029145, acc:0.694444\n",
      "7_264 train_acc: 0.7948 train_loss: 0.001089\tval_acc: 0.799275 val_loss: 0.002020298, acc:0.710069\n",
      "7_266 train_acc: 0.7926 train_loss: 0.001082\tval_acc: 0.788406 val_loss: 0.002016192, acc:0.769097\n",
      "7_271 train_acc: 0.7993 train_loss: 0.001062\tval_acc: 0.802174 val_loss: 0.002011840, acc:0.713542\n",
      "7_279 train_acc: 0.7933 train_loss: 0.001081\tval_acc: 0.810145 val_loss: 0.001983072, acc:0.718750\n",
      "7_289 train_acc: 0.7987 train_loss: 0.001065\tval_acc: 0.792754 val_loss: 0.001969422, acc:0.730903\n",
      "7_307 train_acc: 0.8014 train_loss: 0.001052\tval_acc: 0.799275 val_loss: 0.001957667, acc:0.736111\n",
      "7_321 train_acc: 0.8037 train_loss: 0.001054\tval_acc: 0.807971 val_loss: 0.001925938, acc:0.744792\n",
      "7_334 train_acc: 0.8047 train_loss: 0.001033\tval_acc: 0.819565 val_loss: 0.001904275, acc:0.706597\n",
      "7_335 train_acc: 0.8053 train_loss: 0.001032\tval_acc: 0.807971 val_loss: 0.001892576, acc:0.720486\n",
      "7_427 train_acc: 0.8116 train_loss: 0.001006\tval_acc: 0.797826 val_loss: 0.001890160, acc:0.727431\n",
      "7_428 train_acc: 0.8084 train_loss: 0.001010\tval_acc: 0.800000 val_loss: 0.001879344, acc:0.723958\n",
      "7_451 train_acc: 0.8150 train_loss: 0.000981\tval_acc: 0.804348 val_loss: 0.001857957, acc:0.689236\n",
      "7_457 train_acc: 0.8143 train_loss: 0.000986\tval_acc: 0.810870 val_loss: 0.001832789, acc:0.708333\n",
      "7_522 train_acc: 0.8127 train_loss: 0.000992\tval_acc: 0.805797 val_loss: 0.001819352, acc:0.704861\n",
      "7_543 train_acc: 0.8155 train_loss: 0.000984\tval_acc: 0.809420 val_loss: 0.001791039, acc:0.723958\n",
      "epoch:  543 \tThe test accuracy is: 0.7239583333333334\n",
      " THE BEST ACCURACY IS 0.7239583333333334\tkappa is 0.6319444444444444\n",
      "subject 7 duration: 0:47:37.562407\n",
      "seed is 264\n",
      "Subject 8\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "8_0 train_acc: 0.2969 train_loss: 0.003435\tval_acc: 0.444203 val_loss: 0.004478922, acc:0.423611\n",
      "8_1 train_acc: 0.3940 train_loss: 0.002684\tval_acc: 0.526087 val_loss: 0.004070043, acc:0.453125\n",
      "8_2 train_acc: 0.4748 train_loss: 0.002379\tval_acc: 0.548551 val_loss: 0.003855681, acc:0.545139\n",
      "8_3 train_acc: 0.5203 train_loss: 0.002239\tval_acc: 0.573188 val_loss: 0.003747554, acc:0.538194\n",
      "8_4 train_acc: 0.5544 train_loss: 0.002078\tval_acc: 0.584783 val_loss: 0.003642323, acc:0.562500\n",
      "8_5 train_acc: 0.5748 train_loss: 0.002019\tval_acc: 0.594928 val_loss: 0.003486879, acc:0.470486\n",
      "8_6 train_acc: 0.5991 train_loss: 0.001934\tval_acc: 0.615217 val_loss: 0.003434457, acc:0.506944\n",
      "8_7 train_acc: 0.6091 train_loss: 0.001893\tval_acc: 0.628261 val_loss: 0.003296536, acc:0.534722\n",
      "8_8 train_acc: 0.6224 train_loss: 0.001854\tval_acc: 0.636232 val_loss: 0.003266209, acc:0.595486\n",
      "8_9 train_acc: 0.6302 train_loss: 0.001809\tval_acc: 0.649275 val_loss: 0.003211566, acc:0.565972\n",
      "8_10 train_acc: 0.6336 train_loss: 0.001799\tval_acc: 0.650725 val_loss: 0.003086423, acc:0.571181\n",
      "8_11 train_acc: 0.6430 train_loss: 0.001765\tval_acc: 0.674638 val_loss: 0.003002660, acc:0.567708\n",
      "8_14 train_acc: 0.6617 train_loss: 0.001680\tval_acc: 0.668116 val_loss: 0.002956401, acc:0.611111\n",
      "8_15 train_acc: 0.6607 train_loss: 0.001680\tval_acc: 0.678261 val_loss: 0.002923389, acc:0.557292\n",
      "8_16 train_acc: 0.6660 train_loss: 0.001660\tval_acc: 0.686957 val_loss: 0.002833270, acc:0.567708\n",
      "8_19 train_acc: 0.6732 train_loss: 0.001618\tval_acc: 0.700725 val_loss: 0.002818119, acc:0.576389\n",
      "8_21 train_acc: 0.6817 train_loss: 0.001612\tval_acc: 0.707246 val_loss: 0.002778933, acc:0.600694\n",
      "8_24 train_acc: 0.6870 train_loss: 0.001595\tval_acc: 0.706522 val_loss: 0.002708815, acc:0.590278\n",
      "8_25 train_acc: 0.6920 train_loss: 0.001550\tval_acc: 0.714493 val_loss: 0.002680316, acc:0.576389\n",
      "8_33 train_acc: 0.7026 train_loss: 0.001506\tval_acc: 0.713043 val_loss: 0.002643327, acc:0.626736\n",
      "8_34 train_acc: 0.7013 train_loss: 0.001500\tval_acc: 0.715942 val_loss: 0.002631775, acc:0.593750\n",
      "8_39 train_acc: 0.7086 train_loss: 0.001479\tval_acc: 0.729710 val_loss: 0.002589194, acc:0.638889\n",
      "8_40 train_acc: 0.7114 train_loss: 0.001468\tval_acc: 0.733333 val_loss: 0.002530782, acc:0.635417\n",
      "8_41 train_acc: 0.7124 train_loss: 0.001474\tval_acc: 0.737681 val_loss: 0.002515130, acc:0.625000\n",
      "8_48 train_acc: 0.7175 train_loss: 0.001439\tval_acc: 0.741304 val_loss: 0.002482637, acc:0.671875\n",
      "8_56 train_acc: 0.7229 train_loss: 0.001410\tval_acc: 0.734058 val_loss: 0.002457876, acc:0.611111\n",
      "8_58 train_acc: 0.7318 train_loss: 0.001391\tval_acc: 0.744203 val_loss: 0.002453448, acc:0.661458\n",
      "8_63 train_acc: 0.7345 train_loss: 0.001372\tval_acc: 0.728986 val_loss: 0.002430984, acc:0.616319\n",
      "8_64 train_acc: 0.7318 train_loss: 0.001374\tval_acc: 0.726087 val_loss: 0.002429855, acc:0.588542\n",
      "8_69 train_acc: 0.7338 train_loss: 0.001367\tval_acc: 0.740580 val_loss: 0.002421042, acc:0.649306\n",
      "8_70 train_acc: 0.7390 train_loss: 0.001347\tval_acc: 0.753623 val_loss: 0.002418831, acc:0.645833\n",
      "8_76 train_acc: 0.7406 train_loss: 0.001336\tval_acc: 0.748551 val_loss: 0.002417504, acc:0.633681\n",
      "8_77 train_acc: 0.7410 train_loss: 0.001349\tval_acc: 0.748551 val_loss: 0.002360338, acc:0.671875\n",
      "8_79 train_acc: 0.7423 train_loss: 0.001348\tval_acc: 0.759420 val_loss: 0.002313243, acc:0.654514\n",
      "8_94 train_acc: 0.7518 train_loss: 0.001296\tval_acc: 0.752174 val_loss: 0.002309158, acc:0.649306\n",
      "8_97 train_acc: 0.7503 train_loss: 0.001284\tval_acc: 0.757971 val_loss: 0.002286468, acc:0.649306\n",
      "8_103 train_acc: 0.7512 train_loss: 0.001292\tval_acc: 0.763768 val_loss: 0.002269687, acc:0.652778\n",
      "8_116 train_acc: 0.7554 train_loss: 0.001268\tval_acc: 0.770290 val_loss: 0.002215926, acc:0.654514\n",
      "8_126 train_acc: 0.7535 train_loss: 0.001260\tval_acc: 0.767391 val_loss: 0.002202801, acc:0.659722\n",
      "8_134 train_acc: 0.7596 train_loss: 0.001257\tval_acc: 0.771739 val_loss: 0.002168441, acc:0.675347\n",
      "8_157 train_acc: 0.7651 train_loss: 0.001217\tval_acc: 0.776087 val_loss: 0.002151623, acc:0.645833\n",
      "8_165 train_acc: 0.7650 train_loss: 0.001225\tval_acc: 0.774638 val_loss: 0.002133587, acc:0.637153\n",
      "8_168 train_acc: 0.7662 train_loss: 0.001220\tval_acc: 0.778261 val_loss: 0.002121449, acc:0.670139\n",
      "8_182 train_acc: 0.7713 train_loss: 0.001201\tval_acc: 0.785507 val_loss: 0.002115437, acc:0.659722\n",
      "8_198 train_acc: 0.7721 train_loss: 0.001171\tval_acc: 0.784783 val_loss: 0.002090653, acc:0.678819\n",
      "8_206 train_acc: 0.7752 train_loss: 0.001174\tval_acc: 0.786957 val_loss: 0.002062009, acc:0.682292\n",
      "8_214 train_acc: 0.7732 train_loss: 0.001180\tval_acc: 0.787681 val_loss: 0.002060815, acc:0.666667\n",
      "8_220 train_acc: 0.7728 train_loss: 0.001163\tval_acc: 0.794203 val_loss: 0.002057241, acc:0.692708\n",
      "8_247 train_acc: 0.7799 train_loss: 0.001152\tval_acc: 0.794203 val_loss: 0.002037676, acc:0.710069\n",
      "8_269 train_acc: 0.7828 train_loss: 0.001136\tval_acc: 0.795652 val_loss: 0.002015081, acc:0.715278\n",
      "8_281 train_acc: 0.7812 train_loss: 0.001130\tval_acc: 0.794928 val_loss: 0.002000087, acc:0.739583\n",
      "8_287 train_acc: 0.7858 train_loss: 0.001120\tval_acc: 0.804348 val_loss: 0.001977943, acc:0.744792\n",
      "8_299 train_acc: 0.7900 train_loss: 0.001120\tval_acc: 0.801449 val_loss: 0.001966202, acc:0.739583\n",
      "8_307 train_acc: 0.7849 train_loss: 0.001128\tval_acc: 0.810870 val_loss: 0.001933118, acc:0.750000\n",
      "8_322 train_acc: 0.7919 train_loss: 0.001092\tval_acc: 0.803623 val_loss: 0.001923342, acc:0.756944\n",
      "8_380 train_acc: 0.7950 train_loss: 0.001077\tval_acc: 0.795652 val_loss: 0.001921264, acc:0.753472\n",
      "8_399 train_acc: 0.7947 train_loss: 0.001070\tval_acc: 0.797826 val_loss: 0.001889640, acc:0.755208\n",
      "8_404 train_acc: 0.7976 train_loss: 0.001076\tval_acc: 0.807971 val_loss: 0.001886342, acc:0.762153\n",
      "8_444 train_acc: 0.7978 train_loss: 0.001066\tval_acc: 0.812319 val_loss: 0.001882660, acc:0.769097\n",
      "8_461 train_acc: 0.7945 train_loss: 0.001059\tval_acc: 0.807971 val_loss: 0.001880273, acc:0.751736\n",
      "8_473 train_acc: 0.7973 train_loss: 0.001067\tval_acc: 0.802174 val_loss: 0.001874213, acc:0.756944\n",
      "8_480 train_acc: 0.7973 train_loss: 0.001069\tval_acc: 0.797826 val_loss: 0.001872644, acc:0.753472\n",
      "8_481 train_acc: 0.8003 train_loss: 0.001041\tval_acc: 0.813768 val_loss: 0.001863535, acc:0.755208\n",
      "8_486 train_acc: 0.7974 train_loss: 0.001045\tval_acc: 0.811594 val_loss: 0.001835214, acc:0.767361\n",
      "8_522 train_acc: 0.7998 train_loss: 0.001035\tval_acc: 0.809420 val_loss: 0.001826082, acc:0.758681\n",
      "8_547 train_acc: 0.7956 train_loss: 0.001053\tval_acc: 0.811594 val_loss: 0.001823112, acc:0.730903\n",
      "8_556 train_acc: 0.7991 train_loss: 0.001045\tval_acc: 0.814493 val_loss: 0.001816654, acc:0.763889\n",
      "8_579 train_acc: 0.7989 train_loss: 0.001028\tval_acc: 0.810145 val_loss: 0.001816368, acc:0.760417\n",
      "epoch:  579 \tThe test accuracy is: 0.7604166666666666\n",
      " THE BEST ACCURACY IS 0.7604166666666666\tkappa is 0.6805555555555556\n",
      "subject 8 duration: 0:47:37.442204\n",
      "seed is 1694\n",
      "Subject 9\n",
      "(17052, 1, 22, 1000) (17052,) (1380, 1, 22, 1000) (1380,) (576, 1, 22, 1000) (576,)\n",
      "9_0 train_acc: 0.2994 train_loss: 0.003465\tval_acc: 0.412319 val_loss: 0.004706171, acc:0.362847\n",
      "9_1 train_acc: 0.4043 train_loss: 0.002678\tval_acc: 0.510870 val_loss: 0.004090001, acc:0.319444\n",
      "9_2 train_acc: 0.5110 train_loss: 0.002276\tval_acc: 0.594203 val_loss: 0.003642668, acc:0.369792\n",
      "9_3 train_acc: 0.5586 train_loss: 0.002100\tval_acc: 0.610145 val_loss: 0.003517466, acc:0.387153\n",
      "9_4 train_acc: 0.5907 train_loss: 0.001995\tval_acc: 0.631159 val_loss: 0.003426409, acc:0.427083\n",
      "9_5 train_acc: 0.6085 train_loss: 0.001908\tval_acc: 0.641304 val_loss: 0.003269212, acc:0.439236\n",
      "9_7 train_acc: 0.6334 train_loss: 0.001803\tval_acc: 0.655797 val_loss: 0.003254468, acc:0.470486\n",
      "9_8 train_acc: 0.6417 train_loss: 0.001778\tval_acc: 0.663043 val_loss: 0.003222911, acc:0.435764\n",
      "9_9 train_acc: 0.6451 train_loss: 0.001745\tval_acc: 0.665217 val_loss: 0.003192083, acc:0.486111\n",
      "9_10 train_acc: 0.6542 train_loss: 0.001735\tval_acc: 0.659420 val_loss: 0.003150965, acc:0.484375\n",
      "9_11 train_acc: 0.6567 train_loss: 0.001711\tval_acc: 0.674638 val_loss: 0.003099618, acc:0.493056\n",
      "9_12 train_acc: 0.6645 train_loss: 0.001680\tval_acc: 0.686232 val_loss: 0.003000543, acc:0.503472\n",
      "9_17 train_acc: 0.6844 train_loss: 0.001591\tval_acc: 0.701449 val_loss: 0.002931170, acc:0.526042\n",
      "9_19 train_acc: 0.6915 train_loss: 0.001557\tval_acc: 0.705797 val_loss: 0.002880046, acc:0.517361\n",
      "9_24 train_acc: 0.7021 train_loss: 0.001521\tval_acc: 0.717391 val_loss: 0.002776559, acc:0.501736\n",
      "9_28 train_acc: 0.7056 train_loss: 0.001487\tval_acc: 0.717391 val_loss: 0.002765289, acc:0.557292\n",
      "9_29 train_acc: 0.7092 train_loss: 0.001500\tval_acc: 0.715942 val_loss: 0.002752666, acc:0.536458\n",
      "9_32 train_acc: 0.7149 train_loss: 0.001467\tval_acc: 0.718841 val_loss: 0.002745384, acc:0.572917\n",
      "9_33 train_acc: 0.7155 train_loss: 0.001456\tval_acc: 0.721014 val_loss: 0.002713972, acc:0.579861\n",
      "9_37 train_acc: 0.7241 train_loss: 0.001421\tval_acc: 0.721739 val_loss: 0.002654389, acc:0.592014\n",
      "9_38 train_acc: 0.7277 train_loss: 0.001423\tval_acc: 0.726087 val_loss: 0.002607753, acc:0.602431\n",
      "9_39 train_acc: 0.7236 train_loss: 0.001435\tval_acc: 0.723188 val_loss: 0.002597238, acc:0.567708\n",
      "9_48 train_acc: 0.7326 train_loss: 0.001387\tval_acc: 0.727536 val_loss: 0.002548555, acc:0.602431\n",
      "9_50 train_acc: 0.7333 train_loss: 0.001370\tval_acc: 0.735507 val_loss: 0.002548387, acc:0.602431\n",
      "9_51 train_acc: 0.7363 train_loss: 0.001349\tval_acc: 0.732609 val_loss: 0.002500860, acc:0.604167\n",
      "9_56 train_acc: 0.7437 train_loss: 0.001345\tval_acc: 0.743478 val_loss: 0.002428938, acc:0.609375\n",
      "9_61 train_acc: 0.7461 train_loss: 0.001337\tval_acc: 0.749275 val_loss: 0.002428317, acc:0.593750\n",
      "9_64 train_acc: 0.7458 train_loss: 0.001327\tval_acc: 0.736232 val_loss: 0.002427561, acc:0.595486\n",
      "9_71 train_acc: 0.7532 train_loss: 0.001288\tval_acc: 0.746377 val_loss: 0.002329870, acc:0.586806\n",
      "9_85 train_acc: 0.7611 train_loss: 0.001242\tval_acc: 0.747826 val_loss: 0.002311902, acc:0.581597\n",
      "9_96 train_acc: 0.7651 train_loss: 0.001234\tval_acc: 0.763768 val_loss: 0.002293063, acc:0.602431\n",
      "9_104 train_acc: 0.7684 train_loss: 0.001214\tval_acc: 0.767391 val_loss: 0.002188484, acc:0.612847\n",
      "9_108 train_acc: 0.7695 train_loss: 0.001210\tval_acc: 0.773188 val_loss: 0.002158900, acc:0.604167\n",
      "9_121 train_acc: 0.7694 train_loss: 0.001205\tval_acc: 0.778986 val_loss: 0.002135544, acc:0.619792\n",
      "9_142 train_acc: 0.7875 train_loss: 0.001132\tval_acc: 0.780435 val_loss: 0.002132741, acc:0.602431\n",
      "9_153 train_acc: 0.7826 train_loss: 0.001139\tval_acc: 0.785507 val_loss: 0.002102702, acc:0.611111\n",
      "9_154 train_acc: 0.7823 train_loss: 0.001131\tval_acc: 0.784783 val_loss: 0.002035173, acc:0.619792\n",
      "9_170 train_acc: 0.7871 train_loss: 0.001114\tval_acc: 0.785507 val_loss: 0.002006590, acc:0.600694\n",
      "9_192 train_acc: 0.7902 train_loss: 0.001109\tval_acc: 0.792754 val_loss: 0.001940382, acc:0.631944\n",
      "9_237 train_acc: 0.7974 train_loss: 0.001074\tval_acc: 0.798551 val_loss: 0.001926435, acc:0.631944\n",
      "9_262 train_acc: 0.7982 train_loss: 0.001044\tval_acc: 0.797101 val_loss: 0.001910770, acc:0.630208\n",
      "9_263 train_acc: 0.7983 train_loss: 0.001050\tval_acc: 0.794928 val_loss: 0.001906335, acc:0.645833\n",
      "9_270 train_acc: 0.8008 train_loss: 0.001051\tval_acc: 0.805072 val_loss: 0.001896952, acc:0.645833\n",
      "9_281 train_acc: 0.7988 train_loss: 0.001057\tval_acc: 0.807971 val_loss: 0.001876915, acc:0.647569\n",
      "9_290 train_acc: 0.8035 train_loss: 0.001026\tval_acc: 0.810145 val_loss: 0.001865149, acc:0.664931\n",
      "9_320 train_acc: 0.8064 train_loss: 0.001028\tval_acc: 0.808696 val_loss: 0.001844354, acc:0.652778\n",
      "9_391 train_acc: 0.8095 train_loss: 0.001006\tval_acc: 0.807971 val_loss: 0.001836970, acc:0.654514\n",
      "9_392 train_acc: 0.8124 train_loss: 0.000991\tval_acc: 0.810145 val_loss: 0.001826653, acc:0.623264\n",
      "9_407 train_acc: 0.8112 train_loss: 0.000987\tval_acc: 0.815942 val_loss: 0.001795954, acc:0.645833\n",
      "9_459 train_acc: 0.8158 train_loss: 0.000978\tval_acc: 0.814493 val_loss: 0.001771107, acc:0.651042\n",
      "9_488 train_acc: 0.8194 train_loss: 0.000964\tval_acc: 0.819565 val_loss: 0.001766070, acc:0.661458\n",
      "9_490 train_acc: 0.8164 train_loss: 0.000965\tval_acc: 0.816667 val_loss: 0.001719818, acc:0.692708\n",
      "epoch:  490 \tThe test accuracy is: 0.6927083333333334\n",
      " THE BEST ACCURACY IS 0.6927083333333334\tkappa is 0.5902777777777778\n",
      "subject 9 duration: 0:47:39.657260\n",
      "**The average Best accuracy is: 63.387345679012356kappa is: 51.18312757201646\n",
      "\n",
      "best epochs:  [516, 485, 594, 539, 520, 538, 543, 579, 490]\n",
      "---------  all result  ---------\n",
      "        accuray  precision     recall         f1      kappa\n",
      "0     70.486111  73.339855  70.486111  69.884488  60.648148\n",
      "1     51.736111  57.932372  51.736111  48.648844  35.648148\n",
      "2     84.895833  85.986225  84.895833  84.721028  79.861111\n",
      "3     60.243056  63.332154  60.243056  60.504454  46.990741\n",
      "4     44.270833  44.385197  44.270833  44.200295  25.694444\n",
      "5     41.145833  53.320426  41.145833  31.688965  21.527778\n",
      "6     72.395833  76.502804  72.395833  71.034264  63.194444\n",
      "7     76.041667  80.486857  76.041667  75.929701  68.055556\n",
      "8     69.270833  69.904859  69.270833  68.677810  59.027778\n",
      "mean  63.387346  67.243417  63.387346  61.698872  51.183128\n",
      "std   14.977781  13.578517  14.977781  17.007058  19.970375\n",
      "****************************************\n",
      "Sun Mar  9 10:47:40 2025\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "# The code is based on this github project \n",
    "#    https://github.com/snailpt/CTNet/tree/main\n",
    "########################################################################################\n",
    "\n",
    "import os\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import recall_score  \n",
    "from sklearn.metrics import f1_score  \n",
    "from torch.autograd import Variable\n",
    "# from zeta.nn import FeedForward \n",
    "\n",
    "\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D*f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # temporal conv kernel size 64=0.25fs\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False), # [batch, 22, 1000] \n",
    "            nn.BatchNorm2d(f1),\n",
    "            # channel depth-wise conv\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False), # \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # average pooling 1\n",
    "            nn.AvgPool2d((1, pooling_size1)),  # pooling acts as slicing to obtain 'patch' along the time dimension as in ViT\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # spatial conv\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False), \n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "\n",
    "            # average pooling 2 to adjust the length of feature into transformer encoder\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),  \n",
    "                    \n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.cnn_module(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# PointWise FFN\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn, emb_size, drop_p):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "        self.layernorm = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x_input = x\n",
    "        res = self.fn(x, **kwargs)\n",
    "        \n",
    "        out = self.layernorm(self.drop(res)+x_input)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SwitchGate(nn.Module):\n",
    "    def __init__(self, dim, num_experts: int, capacity_factor: float = 1.0, epsilon: float = 1e-6, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.w_gate = nn.Linear(dim, num_experts)\n",
    "\n",
    "    def forward(self, x: Tensor, use_aux_loss=False):\n",
    "        gate_scores = F.softmax(self.w_gate(x), dim=-1)\n",
    "        capacity = int(self.capacity_factor * x.size(0))\n",
    "        top_k_scores, top_k_indices = gate_scores.topk(1, dim=-1)\n",
    "        mask = torch.zeros_like(gate_scores).scatter_(1, top_k_indices, 1)\n",
    "        masked_gate_scores = gate_scores * mask\n",
    "        denominators = masked_gate_scores.sum(0, keepdim=True) + self.epsilon\n",
    "        gate_scores = (masked_gate_scores / denominators) * capacity\n",
    "\n",
    "        if use_aux_loss:\n",
    "            load = gate_scores.sum(0)\n",
    "            importance = gate_scores.sum(1)\n",
    "            loss = ((load - importance) ** 2).mean()\n",
    "            return gate_scores, loss\n",
    "\n",
    "        return gate_scores\n",
    "\n",
    "class SwitchMoE(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, output_dim: int, num_experts: int, capacity_factor: float = 1.0, mult: int = 4, use_aux_loss: bool = False, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.mult = mult\n",
    "        self.use_aux_loss = use_aux_loss\n",
    "\n",
    "        # self.experts = nn.ModuleList([FeedForward(dim, hidden_dim, mult, *args, **kwargs) for _ in range(num_experts)])\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim*mult),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(dim*mult, hidden_dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gate = SwitchGate(dim, num_experts, capacity_factor)\n",
    "        self.projection = nn.Linear(hidden_dim, output_dim) \n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # gate_scores, loss = self.gate(x, use_aux_loss=self.use_aux_loss)\n",
    "        gate_scores = self.gate(x, use_aux_loss=self.use_aux_loss)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "\n",
    "        if torch.isnan(gate_scores).any():\n",
    "            print(\"NaN in gate scores\")\n",
    "            gate_scores[torch.isnan(gate_scores)] = 0\n",
    "\n",
    "        stacked_expert_outputs = torch.stack(expert_outputs, dim=-1)\n",
    "        if torch.isnan(stacked_expert_outputs).any():\n",
    "            stacked_expert_outputs[torch.isnan(stacked_expert_outputs)] = 0\n",
    "\n",
    "        moe_output = torch.sum(gate_scores.unsqueeze(-2) * stacked_expert_outputs, dim=-1)\n",
    "        moe_output = self.projection(moe_output) \n",
    "        return moe_output\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=4,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5, num_experts=4):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                ), emb_size, drop_p),\n",
    "            SwitchMoE(emb_size, emb_size * num_heads, emb_size, num_experts),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                ), emb_size, drop_p)\n",
    "            \n",
    "            )     \n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, heads, depth, emb_size):\n",
    "        super().__init__(*[TransformerEncoderBlock(emb_size, heads) for _ in range(depth)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BranchEEGNetTransformer(nn.Sequential):\n",
    "    def __init__(self, heads=4, \n",
    "                 depth=6, \n",
    "                 emb_size=40, \n",
    "                 number_channel=22,\n",
    "                 f1 = 20,\n",
    "                 kernel_size = 64,\n",
    "                 D = 2,\n",
    "                 pooling_size1 = 8,\n",
    "                 pooling_size2 = 8,\n",
    "                 dropout_rate = 0.3,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbeddingCNN(f1=f1, \n",
    "                                 kernel_size=kernel_size,\n",
    "                                 D=D, \n",
    "                                 pooling_size1=pooling_size1, \n",
    "                                 pooling_size2=pooling_size2, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 number_channel=number_channel,\n",
    "                                 emb_size=emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "class PositioinalEncoding(nn.Module):\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x): # x-> [batch, embedding, length]\n",
    "        x = x + self.encoding[:, :x.shape[1], :].cuda()\n",
    "        return self.dropout(x)      \n",
    "        \n",
    "   \n",
    "    \n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 database_type='A', \n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 eeg1_number_channel = 22,\n",
    "                 flatten_eeg1 = 600,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.number_class, self.number_channel = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "        self.flatten = nn.Flatten()\n",
    "        # print('self.number_channel', self.number_channel)\n",
    "        self.cnn = BranchEEGNetTransformer(heads, depth, emb_size, number_channel=self.number_channel,\n",
    "                                              f1 = eeg1_f1,\n",
    "                                              kernel_size = eeg1_kernel_size,\n",
    "                                              D = eeg1_D,\n",
    "                                              pooling_size1 = eeg1_pooling_size1,\n",
    "                                              pooling_size2 = eeg1_pooling_size2,\n",
    "                                              dropout_rate = eeg1_dropout_rate,\n",
    "                                              )\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "        self.trans = TransformerEncoder(heads, depth, emb_size)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1 , self.number_class) # FLATTEN_EEGNet + FLATTEN_cnn_module\n",
    "    def forward(self, x):\n",
    "        cnn = self.cnn(x)\n",
    "\n",
    "        #  positional embedding\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "        cnn = self.position(cnn)\n",
    "        \n",
    "        trans = self.trans(cnn)\n",
    "        # residual connect\n",
    "        features = cnn+trans\n",
    "        \n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads=heads\n",
    "        self.emb_size=emb_size\n",
    "        self.depth=depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        self.model = EEGTransformer(\n",
    "             heads=self.heads, \n",
    "             emb_size=self.emb_size,\n",
    "             depth=self.depth, \n",
    "            database_type=self.dataset_type, \n",
    "            eeg1_f1=eeg1_f1, \n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "            eeg1_number_channel = self.number_channel,\n",
    "            flatten_eeg1 = flatten_eeg1,  \n",
    "            ).cuda()\n",
    "        #self.model = nn.DataParallel(self.model, device_ids=gpus)\n",
    "        self.model = self.model.cuda()\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "        \n",
    "\n",
    "\n",
    "    # Segmentation and Reconstruction (S&R) data augmentation\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        \n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            number_records_by_augmentation = self.number_augmentation * tmp_data.shape[0]\n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append([clsAug + 1]*number_records_by_augmentation)\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        # aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        # aug_data = aug_data[aug_shuffle, :, :]\n",
    "        # aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        # aug_data = torch.from_numpy(aug_data).cuda()\n",
    "        # aug_data = aug_data.float()\n",
    "        # aug_label = torch.from_numpy(aug_label-1).cuda()\n",
    "        # aug_label = aug_label.long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,    # (batch, channel, length)\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)  # (288, 1, 22, 1000)\n",
    "        self.train_label = np.transpose(self.train_label) \n",
    "        \n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]  \n",
    "        # split original allData into training and validate datasets\n",
    "\n",
    "        train_data_list = []\n",
    "        train_label_list = []\n",
    "        validate_data_list = []\n",
    "        validate_label_list = []\n",
    "        for cls in range(self.number_class):\n",
    "            # filter by class \n",
    "            cls_idx = np.where(self.allLabel == cls + 1)\n",
    "            cat_data = self.allData[cls_idx]\n",
    "            cat_label = self.allLabel[cls_idx]\n",
    "\n",
    "            \n",
    "            # each category split\n",
    "            number_sample = cat_data.shape[0]\n",
    "            number_validate = int(self.validate_ratio * number_sample)\n",
    "            # shuffle index\n",
    "            index_shuffle = np.random.permutation(len(cat_data))\n",
    "            index_train = index_shuffle[:-number_validate]\n",
    "            index_validate = index_shuffle[-number_validate:]\n",
    "            \n",
    "            train_data_list.append(cat_data[index_train])\n",
    "            train_label_list.append(cat_label[index_train])\n",
    "            \n",
    "            validate_data_list.append(cat_data[index_validate])\n",
    "            validate_label_list.append(cat_label[index_validate])\n",
    "        \n",
    "        # data augmentation\n",
    "        aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "            \n",
    "        train_data_list.append(aug_data)\n",
    "        train_label_list.append(aug_label)\n",
    "            \n",
    "        self.trainData = np.concatenate(train_data_list)\n",
    "        self.trainLabel = np.concatenate(train_label_list)\n",
    "        self.validateData = np.concatenate(validate_data_list)\n",
    "        self.validateLabel = np.concatenate(validate_label_list)\n",
    "        \n",
    "        # shuffle in all category\n",
    "        shuffle_num = np.random.permutation(len(self.trainData))\n",
    "        self.trainData = self.trainData[shuffle_num, :, :, :]  # (number of training sample, 1, 22, 1000)\n",
    "        self.trainLabel = self.trainLabel[shuffle_num]\n",
    "\n",
    "        # self.test_data = np.transpose(self.test_data, (2, 1, 0))\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "\n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        # self.allData = (self.allData - target_mean) / target_std\n",
    "        self.trainData = (self.trainData - target_mean) / target_std\n",
    "        self.validateData = (self.validateData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        isSaveDataLabel = False #True\n",
    "        if isSaveDataLabel:\n",
    "            np.save(\"./gradm_data/train_data_{}.npy\".format(self.nSub), self.allData)\n",
    "            np.save(\"./gradm_data/train_lable_{}.npy\".format(self.nSub), self.allLabel)\n",
    "            np.save(\"./gradm_data/test_data_{}.npy\".format(self.nSub), self.testData)\n",
    "            np.save(\"./gradm_data/test_label_{}.npy\".format(self.nSub), self.testLabel)\n",
    "        print(self.trainData.shape, self.trainLabel.shape, self.validateData.shape, self.validateLabel.shape, self.testData.shape, self.testLabel.shape)\n",
    "        # data shape: (trial, conv channel, electrode channel, time samples)\n",
    "        return self.trainData, self.trainLabel, self.validateData, self.validateLabel, self.testData, self.testLabel\n",
    "\n",
    "\n",
    "\n",
    "    def fit_test(self, model, loss_fn, testloader):\n",
    "        y_list = []\n",
    "        y_pred_list = []\n",
    "\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_running_loss = 0\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            for x, y in testloader:\n",
    "                x = Variable(x.type(self.Tensor))\n",
    "                y = Variable(y.type(self.LongTensor))\n",
    "                \n",
    "                features, y_pred = model(x)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                y_pred = torch.argmax(y_pred, dim=1)\n",
    "                test_correct += (y_pred == y).sum().item()\n",
    "                test_total += y.size(0)\n",
    "                test_running_loss += loss.item()\n",
    "                y_pred = y_pred.cpu().numpy()\n",
    "                y = y.cpu().numpy()\n",
    "                y_list.extend(y)  \n",
    "                y_pred_list.extend(y_pred)  \n",
    "\n",
    "        acc_score = accuracy_score(y_list, y_pred_list)\n",
    "        epoch_test_loss = test_running_loss / len(testloader.dataset)\n",
    "\n",
    "        return epoch_test_loss, acc_score, y_list, y_pred_list\n",
    "    \n",
    "    \n",
    "    def fit_train(self, model, loss_fn, dataloader, optimizer, trainData, trainLabel):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in dataloader:\n",
    "            # real train dataset\n",
    "            img = Variable(train_data.type(self.Tensor))\n",
    "            label = Variable(train_label.type(self.LongTensor))\n",
    "\n",
    "\n",
    "            # training model\n",
    "            features, y_pred = model(img)\n",
    "            # print(\"train outputs: \", outputs.shape, type(outputs))\n",
    "            # print(features.size())\n",
    "            loss = loss_fn(y_pred, label) \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                y_pred = torch.argmax(y_pred, dim=1)\n",
    "                correct += (y_pred == label).sum().item()\n",
    "                total += label.size(0)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "\n",
    "        return epoch_train_loss, epoch_train_acc\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        img, label, validate_data, validate_label, test_data, test_label = self.get_source_data()\n",
    "        # train dataset\n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        # validate dataset\n",
    "        validate_data = torch.from_numpy(validate_data)\n",
    "        validate_label = torch.from_numpy(validate_label - 1)\n",
    "        validate_dataset = torch.utils.data.TensorDataset(validate_data, validate_label)\n",
    "\n",
    "        self.validate_dataloader = torch.utils.data.DataLoader(dataset=validate_dataset, batch_size=288, shuffle=False)\n",
    "        # test dataset\n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=288, shuffle=False)\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        # recording train_acc, train_loss, test_acc, test_loss\n",
    "        result_process = []\n",
    "        # Train the CTNet model\n",
    "        for e in range(self.n_epochs):\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            # train model\n",
    "            self.model.train()\n",
    "            train_loss, train_acc = self.fit_train(self.model, self.criterion_cls, self.dataloader, self.optimizer, self.allData, self.allLabel)\n",
    "            epoch_process['train_acc'] = train_acc\n",
    "            epoch_process['train_loss'] = train_loss\n",
    "            \n",
    "            # validate model\n",
    "            (validate_loss, \n",
    "             validate_acc, \n",
    "             y_list, \n",
    "             y_pred_list) = self.fit_test(self.model, self.criterion_cls, self.validate_dataloader)\n",
    "            epoch_process['val_acc'] = validate_acc                \n",
    "            epoch_process['val_loss'] = validate_loss\n",
    "\n",
    "#             train_pred = torch.max(outputs, 1)[1]\n",
    "#             train_acc = float((train_pred == label).cpu().numpy().astype(int).sum()) / float(label.size(0))\n",
    "            num = num + 1\n",
    "\n",
    "            if min_loss>validate_loss:\n",
    "                min_loss = validate_loss\n",
    "                best_epoch = e\n",
    "                epoch_process['epoch'] = e\n",
    "                torch.save(self.model, self.model_filename)\n",
    "\n",
    "                (test_loss, \n",
    "                 test_acc, \n",
    "                 y_list, \n",
    "                 y_pred_list) = self.fit_test(self.model, self.criterion_cls, self.test_dataloader)\n",
    "                epoch_process['test_acc'] = test_acc                \n",
    "                epoch_process['test_loss'] = test_loss\n",
    "                print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.9f}, acc:{:.6f}\".format(self.nSub,\n",
    "                                                                                       epoch_process['epoch'],\n",
    "                                                                                       epoch_process['train_acc'],\n",
    "                                                                                       epoch_process['train_loss'],\n",
    "                                                                                       epoch_process['val_acc'],\n",
    "                                                                                       epoch_process['val_loss'],\n",
    "                                                                                       epoch_process['test_acc']\n",
    "                                                                                    ))\n",
    "            \n",
    "                \n",
    "            result_process.append(epoch_process)  \n",
    "        \n",
    "        # load model for test\n",
    "        self.model.eval()\n",
    "        self.model = torch.load(self.model_filename).cuda()\n",
    "        # test model\n",
    "        (test_loss, \n",
    "         test_acc, \n",
    "         y_list, \n",
    "         y_pred_list) = self.fit_test(self.model, self.criterion_cls, self.test_dataloader)\n",
    "\n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "\n",
    "        return test_acc, torch.tensor(y_list), torch.tensor(y_pred_list), df_process, best_epoch\n",
    "        # writer.close()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent', # LOSO or not\n",
    "         heads=8,             # heads of MHA\n",
    "         emb_size=48,         # token embding dim\n",
    "         depth=3,             # Transformer encoder depth\n",
    "         dataset_type='A',    # A->'BCI IV2a', B->'BCI IV2b'\n",
    "         eeg1_f1=20,          # features of temporal conv\n",
    "         eeg1_kernel_size=64, # kernel size of temporal conv\n",
    "         eeg1_D=2,            # depth-wise conv \n",
    "         eeg1_pooling_size1=8,# p1\n",
    "         eeg1_pooling_size2=8,# p2\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio = 0.2,\n",
    "         batch_size = 72\n",
    "         ):\n",
    "         \n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    \n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = { }\n",
    "\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "\n",
    "    for i in range(N_SUBJECT):      \n",
    "\n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        \n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "        index_round =0\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus, \n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads, \n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth, \n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1 = eeg1_f1,\n",
    "                  eeg1_kernel_size = eeg1_kernel_size,\n",
    "                  eeg1_D = eeg1_D,\n",
    "                  eeg1_pooling_size1 = eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2 = eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate = eeg1_dropout_rate,\n",
    "                  flatten_eeg1 = flatten_eeg1,  \n",
    "                  validate_ratio = validate_ratio,\n",
    "                  batch_size = batch_size \n",
    "                  )\n",
    "\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {'accuray': accuracy*100,\n",
    "                          'precision': precison*100,\n",
    "                          'recall': recall*100,\n",
    "                          'f1': f1*100, \n",
    "                          'kappa': kappa*100\n",
    "                          }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "    \n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "    \n",
    "\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "                \n",
    "        df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "    #df_result.to_excel(result_write_metric, index=False)\n",
    "    result_metric_dict = df_result\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    \n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    \n",
    "    print(\"*\"*40)\n",
    "\n",
    "    result_write_metric.close()\n",
    "\n",
    "    \n",
    "    return result_metric_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #----------------------------------------\n",
    "    DATA_DIR = r'mymat_raw/'\n",
    "    EVALUATE_MODE = 'LOSO' # leaving one subject out subject-dependent  subject-indenpedent\n",
    "\n",
    "    N_SUBJECT = 9       # BCI \n",
    "    N_AUG = 3           # data augmentation times for benerating artificial training data set\n",
    "    N_SEG = 8           # segmentation times for S&R\n",
    "\n",
    "    EPOCHS = 600\n",
    "    EMB_DIM = 16\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "    TYPE = 'A'\n",
    "    validate_ratio = 0.3 # split raw train dataset into real train dataset and validate dataset\n",
    "    BATCH_SIZE = 512\n",
    "    EEGNet1_F1 = 8\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "    FLATTEN_EEGNet1 = 240\n",
    "\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25    \n",
    "\n",
    "    \n",
    "    parameters_list = [0]\n",
    "    for i in parameters_list:\n",
    "        number_class, number_channel = numberClassChannel(TYPE)\n",
    "        RESULT_NAME = \"Loso_{}_heads_{}_depth_{}_{}\".format(TYPE, HEADS, DEPTH, i)\n",
    "    \n",
    "        sModel = EEGTransformer(\n",
    "            heads=HEADS, \n",
    "            emb_size=EMB_DIM,\n",
    "            depth=DEPTH, \n",
    "            database_type=TYPE,\n",
    "            eeg1_f1=EEGNet1_F1, \n",
    "            eeg1_D=EEGNet1_D,\n",
    "            eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "            eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "            eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "            eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "            eeg1_number_channel = number_channel,\n",
    "            flatten_eeg1 = FLATTEN_EEGNet1,  \n",
    "            ).cuda()\n",
    "        summary(sModel, (1, number_channel, 1000)) \n",
    "    \n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        \n",
    "        result = main(RESULT_NAME,\n",
    "                        evaluate_mode = EVALUATE_MODE,\n",
    "                        heads=HEADS, \n",
    "                        emb_size=EMB_DIM,\n",
    "                        depth=DEPTH, \n",
    "                        dataset_type=TYPE,\n",
    "                        eeg1_f1 = EEGNet1_F1,\n",
    "                        eeg1_kernel_size = EEGNet1_KERNEL_SIZE,\n",
    "                        eeg1_D = EEGNet1_D,\n",
    "                        eeg1_pooling_size1 = EEGNet1_POOL_SIZE1,\n",
    "                        eeg1_pooling_size2 = EEGNet1_POOL_SIZE2,\n",
    "                        eeg1_dropout_rate = EEGNet1_DROPOUT_RATE,\n",
    "                        flatten_eeg1 = FLATTEN_EEGNet1,\n",
    "                        validate_ratio = validate_ratio,\n",
    "                        batch_size = BATCH_SIZE\n",
    "                      )\n",
    "        print(time.asctime(time.localtime(time.time())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
